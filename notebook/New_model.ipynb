{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuKJRQxHe5oa",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning to Predict auction volumes\n",
    "\n",
    "Edgar Jullien, Antoine Settelen, Simon Weiss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NIS3_ohwfgA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF9hc-4Ewkyl"
   },
   "source": [
    "### Challenge context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9fcC6BHwp4O"
   },
   "source": [
    "In many stock exchanges, at the end of a trading day, **auction takes place for each stock**. Each stock is then exchanged at a single price, based on the interest that market participants show in the auction.\n",
    "\n",
    "It is advantageous to do some trading during this auction instead of during the preceding continuous trading, as trading costs are usually lower.\n",
    "\n",
    "In addition, some market participants (day traders, market makers…) prefer to not hold stocks overnight, because events might affect the stock price between the close of the market and the price at the open on the next day (elections, company announcements, etc.), which may result in a loss. For these market participants, the auction is the last chance to limit such a risk, as it is an opportunity to get rid of their remaining stocks and not hold any overnight.\n",
    "\n",
    "For market participants that instead want to hold a specific number of stocks by the end of the day (asset managers…), the auction is their last chance to reach this target. This is in principle important, because they have optimized this number of stocks to hold. For example, if they predict that the price of a stock should rise, then it is in their interest to buy as much of the stock as possible, within the limits set by how much they can invest and by the financial risk that they are ready to take.\n",
    "\n",
    "Market participants may thus want to **estimate the expected number of stocks available during the auction**, as this allows them to gauge how many they can hope to buy or sell during this final, financially advantageous trading opportunity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSs7HPElw-wm"
   },
   "source": [
    "#### Challenge goals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVbKhQfYw7zw"
   },
   "source": [
    "The goal of this challenge is to predict the volume (total value of stock exchanged) available for auction, for **900 stocks** over about **350 days**.\n",
    "Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rkTqfJU0yQs"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9GJj8XixYvy"
   },
   "source": [
    "### Data descritpion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVtfmO5ywhoG"
   },
   "source": [
    "\n",
    "\n",
    "**Input data**\n",
    "\n",
    "The prediction of the auction volume for a given stock on a given day can be made based on the following 126 input columns:\n",
    "\n",
    "    pid: a Product ID, that represents a stock.\n",
    "    day: day of the data sample, as an integer. The ordering is chronological, with day 0 coming before day 1, etc.\n",
    "    abs_retn (n from 0 to 60): absolute values of stock returns (relative price change) between the last known price (typically the price at the beginning of period n) and the end of period n (as a percentage), where the periods cover a good part of the trading day, don't overlap, and have the same duration. Return n=0 comes before return n=1, etc.\n",
    "    rel_voln: like abs_retn, but represents the traded stock volume as a fraction of the volume traded during the period covered (thus, they sum to 1, over a day). The periods are the same as for the returns.\n",
    "    LS and NLV: two quantities associated with the trades of the day for the stock in question. Their nature is kept undisclosed for this challenge.\n",
    "\n",
    "**Output data**\n",
    "\n",
    "The output data contains, for a given stock and a given day, the natural logarithm of the auction volume (= total value of traded stocks), as a fraction of the total volume in the 61 given periods. Thus, if the auction volume represents 10 % of the volume traded over all the periods of a day, the target is log 0.10 = -2.30…\n",
    "\n",
    "Training and test data\n",
    "\n",
    "The 900 stocks found in the training and test data are the same: it is therefore in principle possible to devise predictions that are customized for each stock.\n",
    "\n",
    "The training data contains information on about 800 different days, while the test data requires auction volume predictions for about 350 days.\n",
    "\n",
    "Furthermore, the test inputs correspond to days that come after those of the training data. A challenge is that auction volumes can evolve over time (for instance by becoming relatively larger and larger over time), but we only see what the past (training) auction volumes looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8l-nL7bJzLpt"
   },
   "source": [
    "### Ideas from challenge prez and AMF report about fixing volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_haisTlOzQqa"
   },
   "source": [
    "*Presentation ideas :*\n",
    "- the evolution of the volume in the future is not necessarily linear.\n",
    "=> Feature: Evolution of the global volume per day\n",
    "\n",
    "- For a given day (see idea amf report, election), the market can behave in a particular way that can have csq on the volume. \n",
    "\n",
    "\n",
    "- For boosted trees, bad for extrapolations on future data (in a particular domain).\n",
    "=> They can be combined with other algoes\n",
    "\n",
    "\n",
    "*Ideas from AMF report (only for France):*\n",
    "\n",
    "\n",
    "\n",
    "- In a given quarter, the auction share for end-of-quarter months (March, June, September and December) is about 4% to 6% higher than that observed for the other months (bc of derivative product)\n",
    "=> Fature Need quarter feature encode / months\n",
    "\n",
    "- The days on which quarterly derivatives expire are not only amongst the most active days, they are also the days on which the share of the closing auction reaches its highest level cf graph amf\n",
    "\n",
    "=> Encode these specific days\n",
    "\n",
    "- last trading day of the months of February, May, August and November are also among the most active days in terms of volumes traded (to a lesser extent than for quarterly expiries) and those with the largest share of the fixing\n",
    "This comes from The MSCI indices rebalancing days\n",
    "\n",
    "=> Encode these specific days\n",
    "\n",
    "\n",
    "- In contrast, on days of high volatility and large volumes (e.g. start of February 2018 and around the first round of the French presidential elections), \n",
    "the share of the closing auction is generally lower than the average on the other days.\n",
    "\n",
    "Origin  : \n",
    "\n",
    "- RAPID DEVELOPMENT OF PASSIVE MANAGEMENT in Europe notably the ETF which have to increase their volume at the end of the day (vs US)\n",
    "=> If we have the days, we could add the share of passive funds in equity funds\n",
    "\n",
    "- Best Exectution obligation : since the closing auction offers a single simple reference price.\n",
    "\n",
    "- Avoiding HTF\n",
    "\n",
    "- THE AMPLIFYING EFFECT OF EXECUTION ALGORITHMS\n",
    "VWAP type, which adapt their execution volumes to that of the market in general\n",
    "\n",
    "=> Newcolumn : Abs_var_price* Abs_vol\n",
    "\n",
    "\n",
    "- Check TOP & TOV formation in doc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEQJaUM01izI"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG78cwRK1k5J"
   },
   "source": [
    "https://fingfx.thomsonreuters.com/gfx/mkt/12/4751/4713/closing%20auctions.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRT3abbJx2P0"
   },
   "source": [
    "### Description of the steps : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6cvcwbTxeCn"
   },
   "source": [
    "\n",
    "- Step 0 : load drive in google collab\n",
    "- Step 1 : Implement benchmark methodology used by challenge provider\n",
    "- Step 2 : Advanced feature engineering\n",
    "- Step 3 : Advanced Machine Learning modeling\n",
    "- Step 4 : Deep learning : LTSM with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgPOjq-5xbjP"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9e091g9x6Q0"
   },
   "source": [
    "- Benchmark : 15th in leaderboard (submission 01.02, 5:30pm) - MSE : 0.5723\n",
    "- Advanced Machine Learning : 10th in leaderboard (submission 12.02, 7:37pm) - MSE : 0.4909\n",
    "- Deep Learning : 21th in leaderboard (submission 14.02, 2.37pm, MSE : 0.631259918503299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKTNkDc6y7EV"
   },
   "source": [
    "### Set- up used : \n",
    "- Google collab pro GPU\n",
    "- a lot of coffee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymSG0JoOUDMV"
   },
   "source": [
    "## Step 0 : Load drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2092,
     "status": "ok",
     "timestamp": 1613316177519,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "QBe2le0CFK13",
    "outputId": "90fe4327-39cb-4ae5-f6df-9e13057327ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb2mUfK0kAgR"
   },
   "source": [
    "## Step 1. Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXrLUHY7kP8j"
   },
   "source": [
    "### Task 1 : Load libraries & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3930,
     "status": "ok",
     "timestamp": 1613316174743,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "u7ndzkx0dkWV",
    "outputId": "97aa3480-b6ce-4029-b87d-c0f2e41f9349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "#pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3851,
     "status": "ok",
     "timestamp": 1613316177517,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "aTh1ViVnm-S2",
    "outputId": "7ad7cc81-4841-4ec6-b488-ca75ef169234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delayed in /usr/local/lib/python3.6/dist-packages (0.11.0b1)\n",
      "Requirement already satisfied: redis in /usr/local/lib/python3.6/dist-packages (from delayed) (3.5.3)\n",
      "Requirement already satisfied: hiredis in /usr/local/lib/python3.6/dist-packages (from delayed) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "#pip install delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1613316185953,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "5OWhl6UAkobw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "import delayed\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score \n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbLN3ip2kbw9"
   },
   "source": [
    "Load Light GBM in Google collab for futur use of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 11349,
     "status": "ok",
     "timestamp": 1613166533207,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "JJVLh2KUejuS",
    "outputId": "29eb45d9-9a25-4d17-9d1e-e17187876dc1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'!rm -r /content/LightGBM\\n!git clone --recursive https://github.com/Microsoft/LightGBM\\n%cd /content/LightGBM\\n!mkdir build\\n!cmake -DUSE_GPU=1 #avoid ..\\n!make -j$(nproc)\\n!sudo apt-get -y install python-pip\\n!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\\n%cd /content/LightGBM/python-package\\n!sudo python setup.py install --precompile'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!rm -r /content/LightGBM\n",
    "!git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "%cd /content/LightGBM\n",
    "!mkdir build\n",
    "!cmake -DUSE_GPU=1 #avoid ..\n",
    "!make -j$(nproc)\n",
    "!sudo apt-get -y install python-pip\n",
    "!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
    "%cd /content/LightGBM/python-package\n",
    "!sudo python setup.py install --precompile\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1613316193035,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "O_rqoGBWDef2"
   },
   "outputs": [],
   "source": [
    "data_dir  = \"/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1613316193897,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "Pp-8rtyYDef2",
    "outputId": "ca2868c3-c0c6-4500-ff58-53d6ecaac856"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/dataset/x_train.csv',\n",
       " '/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/dataset/x_test.csv',\n",
       " '/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/dataset/submission_csv_file_random_example.csv',\n",
       " '/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/dataset/y_train.csv']"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = glob.glob(os.path.join(data_dir, '**.csv'))\n",
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "executionInfo": {
     "elapsed": 24724,
     "status": "ok",
     "timestamp": 1613316463554,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "x0kM1RZtDef4"
   },
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(\"%s/y_train.csv\" % data_dir, sep=\",\")\n",
    "x_train = pd.read_csv(\"%s/x_train.csv\" % data_dir, sep=\",\")\n",
    "x_test=pd.read_csv(\"%s/x_test.csv\" % data_dir, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svWFeAjylRFy"
   },
   "source": [
    "### Task 2  : feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fncxRb-lamq"
   },
   "source": [
    "#### 2.1 Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1613316466466,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "bKuJQEabDef9",
    "outputId": "805a8d1d-455c-4de5-97ce-1bb744ab0302"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1613316467794,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "CgobUOKeDef9"
   },
   "outputs": [],
   "source": [
    "x_train.fillna(0, inplace=True) \n",
    "x_test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49UHaA5Rlpbr"
   },
   "source": [
    "#### 2.2 Basic feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GefdoNp_lvRC"
   },
   "source": [
    "We create min, max, std and median for the volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 3144,
     "status": "ok",
     "timestamp": 1613316475847,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "SjnrFv44Def-"
   },
   "outputs": [],
   "source": [
    "x_train['min_ret']    = np.min(x_train.iloc[:,3:63], axis=1)\n",
    "x_train['max_ret']    = np.max(x_train.iloc[:,3:63], axis=1)\n",
    "x_train['std_ret']    = np.std(x_train.iloc[:,3:63], axis=1)\n",
    "x_train['median_ret'] = np.median(x_train.iloc[:,3:63], axis=1)\n",
    "x_train['sum_ret'] = np.median(x_train.iloc[:,3:63], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "executionInfo": {
     "elapsed": 3283,
     "status": "ok",
     "timestamp": 1613316476829,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "rXmr-t5qDef-"
   },
   "outputs": [],
   "source": [
    "x_test['min_ret']    = np.min(x_train.iloc[:,3:63], axis=1)\n",
    "x_test['max_ret']    = np.max(x_test.iloc[:,3:63], axis=1)\n",
    "x_test['std_ret']    = np.std(x_test.iloc[:,3:63], axis=1)\n",
    "x_test['median_ret'] = np.median(x_test.iloc[:,3:63], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "executionInfo": {
     "elapsed": 4310,
     "status": "ok",
     "timestamp": 1613316478642,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "tbwgg9zLDef-"
   },
   "outputs": [],
   "source": [
    "x_train['min_vol']    = np.min(x_train.iloc[:,64:125], axis=1)\n",
    "x_train['max_vol']    = np.max(x_train.iloc[:,64:125], axis=1)\n",
    "x_train['std_vol']    = np.std(x_train.iloc[:,64:125], axis=1)\n",
    "x_train['median_vol'] = np.median(x_train.iloc[:,64:125], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "executionInfo": {
     "elapsed": 4098,
     "status": "ok",
     "timestamp": 1613316479467,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "vEhyknCEvy5G"
   },
   "outputs": [],
   "source": [
    "x_test['min_vol']    = np.min(x_test.iloc[:,64:125], axis=1)\n",
    "x_test['max_vol']    = np.max(x_test.iloc[:,64:125], axis=1)\n",
    "x_test['std_vol']    = np.std(x_test.iloc[:,64:125], axis=1)\n",
    "x_test['median_vol'] = np.median(x_test.iloc[:,64:125], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdwHB_o14iM4"
   },
   "source": [
    "We Convert pid (categorical variable) to dummies for XGboost (doesn't take cat features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "executionInfo": {
     "elapsed": 1741,
     "status": "ok",
     "timestamp": 1613316481218,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "D_ZD7TDOJIyD"
   },
   "outputs": [],
   "source": [
    "train_dataset = y_train.merge(x_train, on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "error",
     "timestamp": 1613316511990,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "85aP4DNzJ9dP",
    "outputId": "69874cc6-262c-421c-f497-a60a685015a4"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-2fe8771d07eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sum_ret'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m         )\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3921\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3924\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['sum_ret'] not found in axis\""
     ]
    }
   ],
   "source": [
    "x_train.drop(['sum_ret'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "executionInfo": {
     "elapsed": 5544,
     "status": "ok",
     "timestamp": 1613316552992,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "Me7QBsR9JbJB"
   },
   "outputs": [],
   "source": [
    "x_train=pd.get_dummies(x_train,columns=['pid'])\n",
    "x_test=pd.get_dummies(x_test,columns=['pid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "executionInfo": {
     "elapsed": 89992,
     "status": "ok",
     "timestamp": 1613316644131,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "mfhNjSTEKcOy"
   },
   "outputs": [],
   "source": [
    "for f in x_train.columns[134:1034]:\n",
    "  x_train[f]=x_train[f].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "executionInfo": {
     "elapsed": 40098,
     "status": "ok",
     "timestamp": 1613316691050,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "Nv4AMwBfMO5a"
   },
   "outputs": [],
   "source": [
    "for f in x_test.columns[134:1034]:\n",
    "  x_test[f]=x_test[f].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 910,
     "status": "ok",
     "timestamp": 1613316648819,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "8nT7e05tKU0b",
    "outputId": "0febb3e4-3e2e-4a75-9610-a0a2475f9a3c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>day</th>\n",
       "      <th>abs_ret0</th>\n",
       "      <th>abs_ret1</th>\n",
       "      <th>abs_ret2</th>\n",
       "      <th>abs_ret3</th>\n",
       "      <th>abs_ret4</th>\n",
       "      <th>abs_ret5</th>\n",
       "      <th>abs_ret6</th>\n",
       "      <th>abs_ret7</th>\n",
       "      <th>abs_ret8</th>\n",
       "      <th>abs_ret9</th>\n",
       "      <th>abs_ret10</th>\n",
       "      <th>abs_ret11</th>\n",
       "      <th>abs_ret12</th>\n",
       "      <th>abs_ret13</th>\n",
       "      <th>abs_ret14</th>\n",
       "      <th>abs_ret15</th>\n",
       "      <th>abs_ret16</th>\n",
       "      <th>abs_ret17</th>\n",
       "      <th>abs_ret18</th>\n",
       "      <th>abs_ret19</th>\n",
       "      <th>abs_ret20</th>\n",
       "      <th>abs_ret21</th>\n",
       "      <th>abs_ret22</th>\n",
       "      <th>abs_ret23</th>\n",
       "      <th>abs_ret24</th>\n",
       "      <th>abs_ret25</th>\n",
       "      <th>abs_ret26</th>\n",
       "      <th>abs_ret27</th>\n",
       "      <th>abs_ret28</th>\n",
       "      <th>abs_ret29</th>\n",
       "      <th>abs_ret30</th>\n",
       "      <th>abs_ret31</th>\n",
       "      <th>abs_ret32</th>\n",
       "      <th>abs_ret33</th>\n",
       "      <th>abs_ret34</th>\n",
       "      <th>abs_ret35</th>\n",
       "      <th>abs_ret36</th>\n",
       "      <th>abs_ret37</th>\n",
       "      <th>...</th>\n",
       "      <th>pid_860</th>\n",
       "      <th>pid_861</th>\n",
       "      <th>pid_862</th>\n",
       "      <th>pid_863</th>\n",
       "      <th>pid_864</th>\n",
       "      <th>pid_865</th>\n",
       "      <th>pid_866</th>\n",
       "      <th>pid_867</th>\n",
       "      <th>pid_868</th>\n",
       "      <th>pid_869</th>\n",
       "      <th>pid_870</th>\n",
       "      <th>pid_871</th>\n",
       "      <th>pid_872</th>\n",
       "      <th>pid_873</th>\n",
       "      <th>pid_874</th>\n",
       "      <th>pid_875</th>\n",
       "      <th>pid_876</th>\n",
       "      <th>pid_877</th>\n",
       "      <th>pid_878</th>\n",
       "      <th>pid_879</th>\n",
       "      <th>pid_880</th>\n",
       "      <th>pid_881</th>\n",
       "      <th>pid_882</th>\n",
       "      <th>pid_883</th>\n",
       "      <th>pid_884</th>\n",
       "      <th>pid_885</th>\n",
       "      <th>pid_886</th>\n",
       "      <th>pid_887</th>\n",
       "      <th>pid_888</th>\n",
       "      <th>pid_889</th>\n",
       "      <th>pid_890</th>\n",
       "      <th>pid_891</th>\n",
       "      <th>pid_892</th>\n",
       "      <th>pid_893</th>\n",
       "      <th>pid_894</th>\n",
       "      <th>pid_895</th>\n",
       "      <th>pid_896</th>\n",
       "      <th>pid_897</th>\n",
       "      <th>pid_898</th>\n",
       "      <th>pid_899</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>0.073265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036601</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>0.029261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073206</td>\n",
       "      <td>0.032942</td>\n",
       "      <td>0.036609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>0.036643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036630</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.036627</td>\n",
       "      <td>0.018305</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014802</td>\n",
       "      <td>0.014818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029678</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.048146</td>\n",
       "      <td>0.092654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118457</td>\n",
       "      <td>0.059084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029588</td>\n",
       "      <td>0.044418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044557</td>\n",
       "      <td>0.052169</td>\n",
       "      <td>0.044613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>4</td>\n",
       "      <td>0.088086</td>\n",
       "      <td>0.109737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.036627</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>0.197325</td>\n",
       "      <td>0.094835</td>\n",
       "      <td>0.029218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>0.058245</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.018169</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.014541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076542</td>\n",
       "      <td>0.182548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051136</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051230</td>\n",
       "      <td>0.036611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684477</th>\n",
       "      <td>1536855</td>\n",
       "      <td>800</td>\n",
       "      <td>1.125704</td>\n",
       "      <td>0.865801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124611</td>\n",
       "      <td>0.124456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062344</td>\n",
       "      <td>0.124533</td>\n",
       "      <td>0.124688</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>0.876644</td>\n",
       "      <td>0.248293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435594</td>\n",
       "      <td>0.743494</td>\n",
       "      <td>0.187266</td>\n",
       "      <td>0.249221</td>\n",
       "      <td>0.062112</td>\n",
       "      <td>0.186451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.248139</td>\n",
       "      <td>0.123762</td>\n",
       "      <td>0.061958</td>\n",
       "      <td>0.124378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124301</td>\n",
       "      <td>0.061920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061920</td>\n",
       "      <td>0.061958</td>\n",
       "      <td>0.123839</td>\n",
       "      <td>0.494743</td>\n",
       "      <td>0.093226</td>\n",
       "      <td>0.093139</td>\n",
       "      <td>0.062035</td>\n",
       "      <td>0.123993</td>\n",
       "      <td>0.186220</td>\n",
       "      <td>0.186683</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684478</th>\n",
       "      <td>1536856</td>\n",
       "      <td>801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.502197</td>\n",
       "      <td>0.441640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125392</td>\n",
       "      <td>0.187852</td>\n",
       "      <td>0.062735</td>\n",
       "      <td>0.690521</td>\n",
       "      <td>0.124844</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310945</td>\n",
       "      <td>0.247219</td>\n",
       "      <td>0.185874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124069</td>\n",
       "      <td>0.186567</td>\n",
       "      <td>0.186220</td>\n",
       "      <td>0.062035</td>\n",
       "      <td>0.124146</td>\n",
       "      <td>0.186451</td>\n",
       "      <td>0.249066</td>\n",
       "      <td>0.312110</td>\n",
       "      <td>0.124456</td>\n",
       "      <td>0.062305</td>\n",
       "      <td>0.062305</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062539</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125235</td>\n",
       "      <td>0.250156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062578</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684479</th>\n",
       "      <td>1536857</td>\n",
       "      <td>802</td>\n",
       "      <td>1.086262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568900</td>\n",
       "      <td>0.471846</td>\n",
       "      <td>0.189753</td>\n",
       "      <td>0.475285</td>\n",
       "      <td>0.441362</td>\n",
       "      <td>0.251572</td>\n",
       "      <td>0.219642</td>\n",
       "      <td>0.031447</td>\n",
       "      <td>0.220057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438048</td>\n",
       "      <td>0.405869</td>\n",
       "      <td>0.434243</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.187617</td>\n",
       "      <td>0.249688</td>\n",
       "      <td>0.188088</td>\n",
       "      <td>0.125235</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.063052</td>\n",
       "      <td>0.063012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063131</td>\n",
       "      <td>0.378549</td>\n",
       "      <td>0.125707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094607</td>\n",
       "      <td>0.125549</td>\n",
       "      <td>0.062854</td>\n",
       "      <td>0.062814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313873</td>\n",
       "      <td>0.125945</td>\n",
       "      <td>0.062814</td>\n",
       "      <td>0.062854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684480</th>\n",
       "      <td>1536858</td>\n",
       "      <td>803</td>\n",
       "      <td>1.505646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.157360</td>\n",
       "      <td>1.556420</td>\n",
       "      <td>2.305665</td>\n",
       "      <td>0.515132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390371</td>\n",
       "      <td>0.522876</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.065660</td>\n",
       "      <td>0.788955</td>\n",
       "      <td>0.260756</td>\n",
       "      <td>0.195950</td>\n",
       "      <td>0.195567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.454841</td>\n",
       "      <td>0.587467</td>\n",
       "      <td>0.523560</td>\n",
       "      <td>0.688750</td>\n",
       "      <td>0.065147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.194553</td>\n",
       "      <td>0.454841</td>\n",
       "      <td>0.324886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.388853</td>\n",
       "      <td>0.065062</td>\n",
       "      <td>0.455729</td>\n",
       "      <td>0.130804</td>\n",
       "      <td>0.065274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326158</td>\n",
       "      <td>0.785340</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684481</th>\n",
       "      <td>1536859</td>\n",
       "      <td>804</td>\n",
       "      <td>0.262812</td>\n",
       "      <td>0.392413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393959</td>\n",
       "      <td>0.264201</td>\n",
       "      <td>0.131752</td>\n",
       "      <td>0.066094</td>\n",
       "      <td>0.264375</td>\n",
       "      <td>0.133156</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.533689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232404</td>\n",
       "      <td>0.166389</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200535</td>\n",
       "      <td>0.133422</td>\n",
       "      <td>0.467602</td>\n",
       "      <td>0.201342</td>\n",
       "      <td>0.134499</td>\n",
       "      <td>0.606878</td>\n",
       "      <td>0.815772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201613</td>\n",
       "      <td>0.201207</td>\n",
       "      <td>0.134409</td>\n",
       "      <td>0.268276</td>\n",
       "      <td>0.267738</td>\n",
       "      <td>0.134228</td>\n",
       "      <td>0.134499</td>\n",
       "      <td>0.201748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134771</td>\n",
       "      <td>0.201884</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>0.067204</td>\n",
       "      <td>0.402955</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>684482 rows × 1034 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  day  abs_ret0  abs_ret1  ...  pid_896  pid_897  pid_898  pid_899\n",
       "0           148    0  0.000000  0.000000  ...        0        0        0        0\n",
       "1           149    1  0.000000  0.000000  ...        0        0        0        0\n",
       "2           150    2  0.000000  0.000000  ...        0        0        0        0\n",
       "3           151    3  0.000000  0.007384  ...        0        0        0        0\n",
       "4           152    4  0.088086  0.109737  ...        0        0        0        0\n",
       "...         ...  ...       ...       ...  ...      ...      ...      ...      ...\n",
       "684477  1536855  800  1.125704  0.865801  ...        0        0        0        0\n",
       "684478  1536856  801  0.000000  0.746269  ...        0        0        0        0\n",
       "684479  1536857  802  1.086262  0.000000  ...        0        0        0        0\n",
       "684480  1536858  803  1.505646  0.000000  ...        0        0        0        0\n",
       "684481  1536859  804  0.262812  0.392413  ...        0        0        0        0\n",
       "\n",
       "[684482 rows x 1034 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqJaORGOmUud"
   },
   "source": [
    "#### 2.3 Create split for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zl0ii2B9mksr"
   },
   "source": [
    "First we merge x_train and y_train and drop ID columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "executionInfo": {
     "elapsed": 4082,
     "status": "ok",
     "timestamp": 1613316698434,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "DDsd42NZDef_"
   },
   "outputs": [],
   "source": [
    "train_df  = y_train.merge(x_train, on=\"ID\")\n",
    "train_df.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "executionInfo": {
     "elapsed": 3685,
     "status": "ok",
     "timestamp": 1613316699418,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "Vzfqx6daDef_"
   },
   "outputs": [],
   "source": [
    "train_X_, test_X_, train_y_, test_y_ = train_test_split(train_df.iloc[:,1:134], train_df['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "executionInfo": {
     "elapsed": 2781,
     "status": "ok",
     "timestamp": 1613316699419,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "omRLjZ0WDef_"
   },
   "outputs": [],
   "source": [
    "import delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z76QIbMDLvlo"
   },
   "source": [
    "### Task 3 : EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5hLg8E5V_yE"
   },
   "source": [
    "EDA inspired from https://www.kaggle.com/gopidurgaprasad/m5-forecasting-eda-lstm-pytorch-modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-AdHF10NyV6"
   },
   "source": [
    "For EDA we are going to use one of pid  \n",
    "We are going to use pid 10\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "executionInfo": {
     "elapsed": 1430,
     "status": "ok",
     "timestamp": 1613316703170,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "VYVeRC71L0cd"
   },
   "outputs": [],
   "source": [
    "pid_10=train_dataset[train_dataset['pid']==10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1613316704743,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "DhIiEVt1OBfb",
    "outputId": "d8fde7ce-8ef7-427f-af39-6cf74eb6fabe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>pid</th>\n",
       "      <th>day</th>\n",
       "      <th>abs_ret0</th>\n",
       "      <th>abs_ret1</th>\n",
       "      <th>abs_ret2</th>\n",
       "      <th>abs_ret3</th>\n",
       "      <th>abs_ret4</th>\n",
       "      <th>abs_ret5</th>\n",
       "      <th>abs_ret6</th>\n",
       "      <th>abs_ret7</th>\n",
       "      <th>abs_ret8</th>\n",
       "      <th>abs_ret9</th>\n",
       "      <th>abs_ret10</th>\n",
       "      <th>abs_ret11</th>\n",
       "      <th>abs_ret12</th>\n",
       "      <th>abs_ret13</th>\n",
       "      <th>abs_ret14</th>\n",
       "      <th>abs_ret15</th>\n",
       "      <th>abs_ret16</th>\n",
       "      <th>abs_ret17</th>\n",
       "      <th>abs_ret18</th>\n",
       "      <th>abs_ret19</th>\n",
       "      <th>abs_ret20</th>\n",
       "      <th>abs_ret21</th>\n",
       "      <th>abs_ret22</th>\n",
       "      <th>abs_ret23</th>\n",
       "      <th>abs_ret24</th>\n",
       "      <th>abs_ret25</th>\n",
       "      <th>abs_ret26</th>\n",
       "      <th>abs_ret27</th>\n",
       "      <th>abs_ret28</th>\n",
       "      <th>abs_ret29</th>\n",
       "      <th>abs_ret30</th>\n",
       "      <th>abs_ret31</th>\n",
       "      <th>abs_ret32</th>\n",
       "      <th>abs_ret33</th>\n",
       "      <th>abs_ret34</th>\n",
       "      <th>abs_ret35</th>\n",
       "      <th>...</th>\n",
       "      <th>rel_vol32</th>\n",
       "      <th>rel_vol33</th>\n",
       "      <th>rel_vol34</th>\n",
       "      <th>rel_vol35</th>\n",
       "      <th>rel_vol36</th>\n",
       "      <th>rel_vol37</th>\n",
       "      <th>rel_vol38</th>\n",
       "      <th>rel_vol39</th>\n",
       "      <th>rel_vol40</th>\n",
       "      <th>rel_vol41</th>\n",
       "      <th>rel_vol42</th>\n",
       "      <th>rel_vol43</th>\n",
       "      <th>rel_vol44</th>\n",
       "      <th>rel_vol45</th>\n",
       "      <th>rel_vol46</th>\n",
       "      <th>rel_vol47</th>\n",
       "      <th>rel_vol48</th>\n",
       "      <th>rel_vol49</th>\n",
       "      <th>rel_vol50</th>\n",
       "      <th>rel_vol51</th>\n",
       "      <th>rel_vol52</th>\n",
       "      <th>rel_vol53</th>\n",
       "      <th>rel_vol54</th>\n",
       "      <th>rel_vol55</th>\n",
       "      <th>rel_vol56</th>\n",
       "      <th>rel_vol57</th>\n",
       "      <th>rel_vol58</th>\n",
       "      <th>rel_vol59</th>\n",
       "      <th>rel_vol60</th>\n",
       "      <th>LS</th>\n",
       "      <th>NLV</th>\n",
       "      <th>min_ret</th>\n",
       "      <th>max_ret</th>\n",
       "      <th>std_ret</th>\n",
       "      <th>median_ret</th>\n",
       "      <th>sum_ret</th>\n",
       "      <th>min_vol</th>\n",
       "      <th>max_vol</th>\n",
       "      <th>std_vol</th>\n",
       "      <th>median_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63938</th>\n",
       "      <td>125243</td>\n",
       "      <td>-1.755755</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.305111</td>\n",
       "      <td>0.152323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153022</td>\n",
       "      <td>0.076570</td>\n",
       "      <td>0.076394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07622</td>\n",
       "      <td>0.076104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.018136</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.006987</td>\n",
       "      <td>0.011027</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.020922</td>\n",
       "      <td>0.010839</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.011312</td>\n",
       "      <td>0.010480</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.049389</td>\n",
       "      <td>0.016689</td>\n",
       "      <td>0.017533</td>\n",
       "      <td>0.030076</td>\n",
       "      <td>0.014692</td>\n",
       "      <td>0.011263</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>0.011122</td>\n",
       "      <td>-4.116767</td>\n",
       "      <td>-0.923613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.378501</td>\n",
       "      <td>0.072113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.094261</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.011701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63939</th>\n",
       "      <td>125244</td>\n",
       "      <td>-2.587678</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076453</td>\n",
       "      <td>0.152439</td>\n",
       "      <td>0.381971</td>\n",
       "      <td>0.530705</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.191424</td>\n",
       "      <td>0.038476</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076687</td>\n",
       "      <td>0.076982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009695</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.004650</td>\n",
       "      <td>0.009060</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.019403</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>0.007745</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>0.009828</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.007196</td>\n",
       "      <td>0.009422</td>\n",
       "      <td>0.011843</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.006461</td>\n",
       "      <td>0.018719</td>\n",
       "      <td>0.009347</td>\n",
       "      <td>0.010604</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>-4.046217</td>\n",
       "      <td>-0.730793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530705</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.180957</td>\n",
       "      <td>0.027686</td>\n",
       "      <td>0.009052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63940</th>\n",
       "      <td>125245</td>\n",
       "      <td>-2.504335</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.230415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076864</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045857</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.013380</td>\n",
       "      <td>0.007059</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.009312</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.004560</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.011068</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>0.008496</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.026246</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.003573</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>0.010302</td>\n",
       "      <td>0.038486</td>\n",
       "      <td>0.008936</td>\n",
       "      <td>0.015049</td>\n",
       "      <td>-4.355014</td>\n",
       "      <td>-1.086094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230415</td>\n",
       "      <td>0.049749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090617</td>\n",
       "      <td>0.017319</td>\n",
       "      <td>0.010956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63941</th>\n",
       "      <td>125246</td>\n",
       "      <td>-2.121251</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302343</td>\n",
       "      <td>0.152091</td>\n",
       "      <td>0.076220</td>\n",
       "      <td>0.380228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.038417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.303260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.075815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012009</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>0.010576</td>\n",
       "      <td>0.013792</td>\n",
       "      <td>0.048989</td>\n",
       "      <td>0.028612</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>0.013209</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.006744</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.010214</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>0.008528</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>0.012009</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.088387</td>\n",
       "      <td>0.018533</td>\n",
       "      <td>0.030084</td>\n",
       "      <td>-4.215094</td>\n",
       "      <td>-0.650578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.380228</td>\n",
       "      <td>0.077397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.088387</td>\n",
       "      <td>0.015839</td>\n",
       "      <td>0.012328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63942</th>\n",
       "      <td>125247</td>\n",
       "      <td>-1.833420</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009347</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.015303</td>\n",
       "      <td>0.012233</td>\n",
       "      <td>0.011776</td>\n",
       "      <td>0.004607</td>\n",
       "      <td>0.008698</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>0.008332</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0.007445</td>\n",
       "      <td>0.037189</td>\n",
       "      <td>0.085508</td>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.006204</td>\n",
       "      <td>0.015138</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>0.006120</td>\n",
       "      <td>0.004956</td>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.028867</td>\n",
       "      <td>0.010017</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.017945</td>\n",
       "      <td>-4.138836</td>\n",
       "      <td>-1.050539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148920</td>\n",
       "      <td>0.031751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.085508</td>\n",
       "      <td>0.016613</td>\n",
       "      <td>0.010099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID    target  pid  day  ...   min_vol   max_vol   std_vol  median_vol\n",
       "63938  125243 -1.755755   10    0  ...  0.000997  0.094261  0.014706    0.011701\n",
       "63939  125244 -2.587678   10    1  ...  0.001181  0.180957  0.027686    0.009052\n",
       "63940  125245 -2.504335   10    2  ...  0.000000  0.090617  0.017319    0.010956\n",
       "63941  125246 -2.121251   10    3  ...  0.001904  0.088387  0.015839    0.012328\n",
       "63942  125247 -1.833420   10    4  ...  0.000619  0.085508  0.016613    0.010099\n",
       "\n",
       "[5 rows x 137 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1613316707227,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "IQOMk7GfO0v3"
   },
   "outputs": [],
   "source": [
    "day_sum = pid_10.groupby(\"day\")[[\"sum_ret\", \"median_vol\"]].agg(\"sum\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1613316750896,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "kpAkH62pK-8r"
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1745,
     "status": "ok",
     "timestamp": 1613316756683,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "FcvyZS5qO3F8",
    "outputId": "847d85d6-513d-45b7-e260-c64481e4de17"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
       "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
       "            <div id=\"c78d2219-f4fd-4998-a2ed-76406a76c168\" class=\"plotly-graph-div\" style=\"height:1000px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                \n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"c78d2219-f4fd-4998-a2ed-76406a76c168\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'c78d2219-f4fd-4998-a2ed-76406a76c168',\n",
       "                        [{\"mode\": \"lines\", \"name\": \"median_vol\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804], \"xaxis\": \"x\", \"y\": [0.011700655932212384, 0.009052118807624769, 0.010956044100633212, 0.012327915412214036, 0.010098941199429194, 0.008126191717393627, 0.009734730288495164, 0.010748737057866328, 0.012405734208503187, 0.008616950606750072, 0.011849178625405986, 0.008476180475694015, 0.00993704818594225, 0.008213373363705059, 0.008524546053115863, 0.008519279112795323, 0.008289969928178877, 0.011451670378761205, 0.01090915594627611, 0.01126100358256412, 0.011359683816369651, 0.011987696043318612, 0.01043598965423444, 0.009139361287688295, 0.00950442307166176, 0.011463955534054372, 0.011226521795229174, 0.013104244263684823, 0.010925223986093722, 0.010092768597522042, 0.012587183407857949, 0.01005385239440374, 0.012974960684577085, 0.011923989999857302, 0.013703734119467568, 0.009189563159355554, 0.010863150771953822, 0.008791561570949697, 0.011327802851997023, 0.01096718184263407, 0.00903130457883042, 0.013343713998014956, 0.012742471742278385, 0.01075113429166594, 0.011902828139403677, 0.011505022959616444, 0.012996981569007448, 0.012306522899488777, 0.009671420462373504, 0.0072988877777401566, 0.006453269463036609, 0.011564195940578491, 0.009930216500303506, 0.01094928599881207, 0.011067436996564916, 0.00979880547391367, 0.009653197979125509, 0.011294275159660954, 0.009002841954614416, 0.011131454765029487, 0.010866451497589016, 0.010044417444930077, 0.01250984665405014, 0.010788928771929824, 0.009886819598348558, 0.007976301322656547, 0.01037764023780817, 0.01048110043230646, 0.00877391099115361, 0.00865231085592193, 0.010709227815723029, 0.0058799547688708435, 0.009675637329581351, 0.010993927108380649, 0.009503078619316551, 0.011081084934317722, 0.010436546049555613, 0.012859226878573757, 0.007207979295551605, 0.01091226049194647, 0.010548772628902836, 0.010577414812889076, 0.011090036173035948, 0.009960611116167086, 0.012300095370821428, 0.013734078590493665, 0.01099958585425963, 0.010072753525639714, 0.012957664417671051, 0.010066322972225854, 0.011444695405779949, 0.010373796149291086, 0.011917210747965794, 0.011119212812530088, 0.012782402373225471, 0.01339791232849358, 0.012281226133016666, 0.010954054797044614, 0.011569687672095085, 0.011229630527517144, 0.011596821818156895, 0.011261986400441343, 0.009847046980978704, 0.011136302343090458, 0.011560264569222871, 0.011365152904899695, 0.01038049244372527, 0.012944455843988277, 0.011353623257163388, 0.01047135189382497, 0.012595177475138629, 0.011421805326155134, 0.014137953450699894, 0.009466073103403316, 0.009454122026588359, 0.008489912209156913, 0.009795507647931186, 0.012816137258726773, 0.013334035349149544, 0.012627608152490227, 0.010847663342267257, 0.010812744728597971, 0.012398994168670038, 0.011593045668695429, 0.011553956116799022, 0.010115130831434949, 0.011606384885160229, 0.009005711039064999, 0.01053224674054704, 0.010424717535282506, 0.013281701166813192, 0.007300206873990024, 0.010927998218767184, 0.012300937134865207, 0.013761743027631777, 0.010390994777581056, 0.008301416517320099, 0.014254738284634031, 0.01280506864271152, 0.010201316928646462, 0.009669929064530385, 0.011669995989403286, 0.009975951795418832, 0.01082162818040912, 0.010262709253847896, 0.01186332915408565, 0.010489022116309485, 0.010434184763777036, 0.011219678740419415, 0.01493025324101401, 0.011065991045257276, 0.011240395298965158, 0.013049649338543535, 0.009525114745490292, 0.009570171596233091, 0.01450092956589941, 0.013093142033210074, 0.011990892944330556, 0.012712078060417556, 0.01243115313853993, 0.009605349465008937, 0.012522120093824828, 0.010564523615537495, 0.010519398415400857, 0.011462336816501238, 0.011185287709533082, 0.00950045691325212, 0.011244161598988231, 0.00973101696538995, 0.013672886354993388, 0.009861662831701166, 0.00943371549985048, 0.01110140238652615, 0.009374047185639, 0.012301668844557192, 0.012282473605833542, 0.012067920578238531, 0.011049721792250049, 0.012922306876124195, 0.011055095206317716, 0.011320753368596422, 0.011318846336285422, 0.011756897983799754, 0.010229450363864835, 0.011179892907334382, 0.009797518009570344, 0.010175348695023496, 0.010389424759061669, 0.013189616819587179, 0.012792273193560823, 0.013366731233782934, 0.013855901313990678, 0.012263355201179216, 0.011261485809683685, 0.0091026933877933, 0.013970995087613595, 0.01378848434036522, 0.010928800036886337, 0.011159798764335808, 0.010935782844606542, 0.01004360734770627, 0.010263401666094048, 0.013060763771014736, 0.009864067808152372, 0.013828411010215922, 0.012086166519057345, 0.011753693990183792, 0.011647767658578698, 0.013089488793453456, 0.01123816540617083, 0.01062411502984167, 0.007914717112596809, 0.007855500391627862, 0.008628623505359909, 0.00956622457637916, 0.009182738751169857, 0.008376164686204961, 0.008607956118952764, 0.009175039326678002, 0.007938215098598433, 0.008910021947696169, 0.00963746981096893, 0.008882676526535106, 0.01175646449773902, 0.00931101027350638, 0.010543890937848877, 0.007685760789905838, 0.012459658115687264, 0.012301640378928369, 0.010159179297052935, 0.013182448836465127, 0.01599436805721436, 0.010605557354227977, 0.009428314993706324, 0.01099866233670882, 0.01115816004466035, 0.011502767363151591, 0.01136221012826235, 0.010594089759555244, 0.009734131287871563, 0.011162765353520156, 0.01001044615169638, 0.013278653189898564, 0.011444024989696112, 0.010186563545419778, 0.010169374533517545, 0.009870322852621904, 0.0128159977913363, 0.011762279566685916, 0.010088809727787284, 0.011606644597943971, 0.011213366713336193, 0.012160792591950471, 0.012183564613742186, 0.007878524622000308, 0.01214390188586409, 0.012104722516352395, 0.010262157783479931, 0.01005948709987064, 0.013773484711952892, 0.011729447231998523, 0.010289668083787468, 0.008177622995191454, 0.013803595687939436, 0.0106457373554438, 0.010213051567925324, 0.008984409129354236, 0.010681804564947644, 0.009088463392471009, 0.011605963675569302, 0.009291983086066408, 0.013361237646691962, 0.009032263314714096, 0.011526531647351118, 0.011591729247780184, 0.013348646379735771, 0.01139695209437645, 0.010013862239909056, 0.011906932264255009, 0.010017546033896173, 0.009817409419165769, 0.0114329580796524, 0.01064140005443495, 0.01107875686076236, 0.011911847259274052, 0.011434549618186723, 0.009082309942387174, 0.009810319705236171, 0.008646501213859539, 0.008061832917314497, 0.011913477048558302, 0.01507630620308066, 0.011784700475579724, 0.010047606364672487, 0.014170682418899816, 0.014231050509524609, 0.013663811667316377, 0.011435474210732608, 0.013915916792547254, 0.012381116963633832, 0.013101876510063, 0.011649356653778828, 0.01134584567581068, 0.012276294194870387, 0.010980183717417557, 0.010226396787866878, 0.011301394609607971, 0.013464053089265005, 0.0148695949915269, 0.011941236384977488, 0.014349157797327372, 0.011864014106916114, 0.010472373211442414, 0.012496216234138505, 0.013604153761061454, 0.013699392323107287, 0.013093526863795352, 0.014955113703004815, 0.012084446984510278, 0.011618203159147062, 0.011145521934677056, 0.009497864838652531, 0.012880022590193758, 0.011398400493447713, 0.01097400253035241, 0.009798019028164571, 0.011165496951724476, 0.011639027045939412, 0.014261057366807847, 0.011606969138229593, 0.01205089377316972, 0.011792593738448674, 0.012230962675972795, 0.01347820723442414, 0.009712160960569404, 0.013384701128731672, 0.012971784425332646, 0.010317479348441476, 0.011716969572239113, 0.012965668626903754, 0.013293351557359487, 0.012824410164482856, 0.011952468912447004, 0.012219103851568913, 0.012252841257186871, 0.009132181983621002, 0.011868487963996477, 0.010082875892252856, 0.011932037833575408, 0.013030091109378404, 0.013922926099327637, 0.014520162048089725, 0.007916986537298136, 0.00995003128212087, 0.012862268778604169, 0.007922972320719776, 0.009207530623035785, 0.010376195501880778, 0.010372921857197126, 0.01163035902916348, 0.01203831674046965, 0.011048437949040093, 0.01225658711695953, 0.010040099519060285, 0.01032054798417328, 0.01115750342879838, 0.0126874965591849, 0.008890149860349382, 0.01209295023698342, 0.012160051942401051, 0.010954792107627212, 0.010303041237449968, 0.010302259437542882, 0.011026322723316386, 0.01143701510944508, 0.013897569717205613, 0.009678865060174405, 0.01128516795701579, 0.012917057341579044, 0.009812847992607905, 0.012415869479567791, 0.012992531698980318, 0.01191310480864937, 0.014490580795368581, 0.014250991784101687, 0.01133474121707967, 0.01428279302311852, 0.0115846077069928, 0.010537388604710448, 0.010760812639138756, 0.0119946601707121, 0.012778964709411748, 0.01195843897637387, 0.01297082111995427, 0.013840944161812935, 0.009477258024977226, 0.011618249200093037, 0.009153184623883405, 0.008138383959742401, 0.012186147088373116, 0.010659667493280513, 0.012557132499625236, 0.011222553857428486, 0.012837722404970196, 0.012527010613828491, 0.012260018568609287, 0.013828795205962335, 0.010855023925746856, 0.013082545401716588, 0.01252874251776916, 0.009811533879588488, 0.011829184152590542, 0.009779921684642862, 0.013270575508237759, 0.011750037785409164, 0.012790881627101859, 0.013185716917495451, 0.012518844904920914, 0.012865204233151677, 0.010495288165686224, 0.01123421846050537, 0.012607714123786584, 0.010571923768313445, 0.0123666800243801, 0.010924079649350993, 0.010177203216468292, 0.014145515425398882, 0.011550765084344132, 0.009285297970874266, 0.010748772768492823, 0.01409264890101032, 0.008577637990069461, 0.011163983456937246, 0.013199729140209024, 0.008460288315089019, 0.011895592850830469, 0.012602932068746587, 0.013140051751128149, 0.013242495665304948, 0.010928146269883329, 0.010411408097041757, 0.013714996234181714, 0.013638396343467559, 0.012546009761436648, 0.011386811210872318, 0.01318224378865618, 0.013347138438136166, 0.015299001742511554, 0.0113678036945184, 0.011957429266628641, 0.009508463573784218, 0.010147708144881695, 0.01116750734185781, 0.010865899414631123, 0.011566549491586356, 0.0116763101266688, 0.011523612710265672, 0.012409860700408995, 0.012615705813687664, 0.012589966583987923, 0.01452141299980293, 0.011648321805947059, 0.012455510437275562, 0.014475320758597638, 0.014208840489048274, 0.011642860890781308, 0.013205949786809671, 0.01223599230996511, 0.013712264248053621, 0.0125227137522062, 0.013886046776416036, 0.01222657773672768, 0.014010865381150471, 0.0120636446128115, 0.01430579323551695, 0.012797236823373087, 0.01394502049755916, 0.0129717589738566, 0.013207953653984776, 0.01147072978645153, 0.015533970214313348, 0.008534238367438989, 0.00986527132286883, 0.011445116387835623, 0.01190433218440376, 0.013356490587840551, 0.011826878952969, 0.012625462093691623, 0.013415105662893476, 0.0113534799758548, 0.010017470321087784, 0.009111081628421235, 0.011115380186261507, 0.012485080198891043, 0.011939766541633305, 0.011925637462554893, 0.010948736273391208, 0.01080650461564264, 0.00939893642160362, 0.010991437233653732, 0.00866343205331328, 0.010142707857948552, 0.012717621493770405, 0.01052657783088484, 0.012650750574051172, 0.011899967311890657, 0.0090818309462589, 0.011823026136198308, 0.011281858528795631, 0.007709822259231856, 0.00855488662314338, 0.00759344061380287, 0.007393511891983769, 0.013108793312204568, 0.012180763514033206, 0.0083338628103862, 0.010337820209756376, 0.008878132953577608, 0.011073637352181595, 0.01081487721205726, 0.007466083052096601, 0.014506633586537653, 0.01244838538416497, 0.009621714132736927, 0.01298550705044244, 0.011480706144166184, 0.012313319511235113, 0.013393613213602627, 0.009513093585005909, 0.009384303085054065, 0.01051505761352882, 0.011322791985546405, 0.01108303078399611, 0.013232113756081413, 0.010623857489151886, 0.011968332350511526, 0.01171961782206384, 0.012484878789208773, 0.011857237173822305, 0.00938334040759905, 0.012849710859034635, 0.01195643998172928, 0.009109411256011104, 0.013080331711644641, 0.014481630698683787, 0.009337018227188831, 0.013450505089757932, 0.010639563243550131, 0.010493368447927932, 0.011441554713496348, 0.011008855303687351, 0.013022319307671186, 0.009107077033116794, 0.012301177481292669, 0.009124215725724558, 0.009368739602000948, 0.010071769159253294, 0.013161957805154886, 0.011522519661103396, 0.011990770748087274, 0.013062536016745941, 0.010344034504372034, 0.009305811539918857, 0.011343881140967273, 0.011575673256259763, 0.011883336516186244, 0.011564847366495677, 0.011156693881671585, 0.012514656592081204, 0.01393023245182938, 0.011462444045656123, 0.01180168922647256, 0.011843665151746409, 0.008119955130395992, 0.009889302780250558, 0.012543189590367506, 0.013975096723788105, 0.01077195038974814, 0.01097787719356811, 0.009673321053782291, 0.010848975783186152, 0.011301755004238006, 0.012794777719168484, 0.01201910246481695, 0.008502382807957948, 0.01160863870145984, 0.009607178221402808, 0.008563996325156744, 0.010201569832142123, 0.012887431037436227, 0.010465854155358513, 0.010304889767685184, 0.006256948592815829, 0.009507760209906077, 0.01129645104412202, 0.01175520780778533, 0.015095720257121362, 0.013823039137212114, 0.01023353218018658, 0.01080398818797306, 0.010924039402467427, 0.011436324653113415, 0.013505687816186456, 0.012735438780054892, 0.012757546884188148, 0.011995455353139605, 0.012476604282351231, 0.012727643212872434, 0.011117065713601812, 0.012452223085716491, 0.01361150126229964, 0.011561451360258731, 0.00858289656615515, 0.013660090330909595, 0.012782596201837956, 0.014321491454695096, 0.010352077461009293, 0.01055415762966219, 0.013119950194008738, 0.012344314580577212, 0.013835523057598994, 0.012662306079792796, 0.009363451505556944, 0.010746844552787802, 0.00842481884536998, 0.010900936792596756, 0.012743870880344682, 0.013220758417563004, 0.011593935497093557, 0.012608457054408051, 0.012422902451363954, 0.011971494830049743, 0.012612732387289069, 0.012286743123764805, 0.014161433723035282, 0.013307988626398131, 0.010442665522630977, 0.00764767216663767, 0.008915956878036138, 0.011468934389375948, 0.003995007030690271, 0.013225077341162671, 0.011775234711193064, 0.012151357711047342, 0.008385592888418208, 0.008729601065293581, 0.011960257107992164, 0.0095524430864797, 0.014794528570383181, 0.011249186886192242, 0.012255846968182462, 0.012392657800520264, 0.007648059156194552, 0.014530253128501178, 0.011781672641799749, 0.011797849839796658, 0.01258953235480995, 0.012489097720877824, 0.012466563830536008, 0.010086599531484431, 0.009580540187834823, 0.009837002052689626, 0.007816143201454507, 0.010832470407702771, 0.008795574740714067, 0.011880841424904157, 0.012583614008211924, 0.01180493968366486, 0.0088171238890238, 0.010140989995893496, 0.010281232116279735, 0.009761245442742532, 0.010726720870683056, 0.012140056913674693, 0.006218641709666836, 0.010677854335123605, 0.011629425418535457, 0.008930423086768717, 0.012580148426675444, 0.010828620256687262, 0.013214570337767623, 0.00890672436482904, 0.01254232765828518, 0.012835072698354795, 0.010307097199737608, 0.013566727773497907, 0.009646009071646365, 0.009807966645658093, 0.01188134890951234, 0.0140146363513187, 0.008439436188239738, 0.013203779676313443, 0.010352580503475452, 0.01231169585795732, 0.011067924190186774, 0.01174268098982154, 0.014133531522147975, 0.011272886412057658, 0.007990541756621212, 0.014482325152497125, 0.010528740723075684, 0.01322141204749327, 0.01243901813593267, 0.010090754848536887, 0.012694024684260723, 0.008828911938215318, 0.014185573645132631, 0.010538401601802564, 0.011438531255393534, 0.011272198269630159, 0.013125875446377071, 0.00948259605397443, 0.013102686339467044, 0.012280526990007892, 0.012987267940496773, 0.014919888186972381, 0.01448598711232929, 0.01243155797648742, 0.013823260709214543, 0.012766362381151856, 0.013399675221803934, 0.014626029337482848, 0.01121436883698667, 0.014743626903490606, 0.014103070348172247, 0.013829038973810477, 0.013697682885101129, 0.011159286464806786, 0.012460430182862131, 0.013315666461597485, 0.013727209816695285, 0.011654260524439887, 0.016446156318348684, 0.012034395585115536, 0.011685145657509804, 0.009229574133422194, 0.012048117507403531, 0.0094520755898222, 0.012310948581505334, 0.012066622497727695, 0.0117350746665348, 0.008191740909203578, 0.013475491757902452, 0.010612942114604744, 0.01097058170705988, 0.013552327374822528, 0.009852313077772904, 0.011435489195558413, 0.01409489126617828, 0.01264083789555395, 0.01420067845914783, 0.014181932446899654, 0.01439977610782631, 0.011382120594562487, 0.010594475237987271, 0.011533663589116977, 0.013133959246406292, 0.014887704585666541, 0.01602351826247815, 0.01395123150061454, 0.01047384159678701, 0.011094744539860032, 0.013046534602839421, 0.009418213574260523, 0.011568892076071736, 0.010819509487712058, 0.01406138429917632, 0.011377668636209887, 0.013459209072274143, 0.01484379637154258, 0.013616605510831192, 0.013944623409100451, 0.012783542714044485, 0.01225142599577024, 0.01386083148738388, 0.009033495970072455, 0.011100759455782141, 0.0075839917168961934, 0.010449366970965705, 0.010541704331218005, 0.011406672853850459, 0.01225137903981625, 0.009556930174975572, 0.011587713996070646, 0.013449465903989608, 0.009314358527483108, 0.014073050093045705, 0.01050100281638146, 0.01392826977096904, 0.011177213565419165, 0.012702991395664987, 0.011811867715621241, 0.012902033238666715, 0.011614602292403296, 0.013644875350724334, 0.01164000866859262, 0.015540368670243731, 0.01460878857352556, 0.012341853508486995, 0.015160337805214232, 0.011333824328662196], \"yaxis\": \"y\"}, {\"mode\": \"lines\", \"name\": \"sum_ret\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804], \"xaxis\": \"x2\", \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034843205574913716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07572916200706548, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06673340749640544, 0.06527459642233224, 0.06470398606698291, 0.0, 0.06160023251384205, 0.06238325031133196, 0.0, 0.0, 0.0, 0.0, 0.060132378013172394, 0.05943541128244356, 0.11037530955492758, 0.05541705131537067, 0.0562587948875537, 0.0, 0.014068655036580944, 0.027685492801765577, 0.054377379010328575, 0.0, 0.05578800991953292, 0.0, 0.014819205690580484, 0.03086515015894853, 0.047385920202663856, 0.0, 0.0, 0.1227747084100672, 0.1280409731114096, 0.1303359601543097, 0.195376119706836, 0.20256620331299824, 0.262478492780982, 0.1334228159245665, 0.19986684420771894, 0.13736263736264798, 0.17540133890758813, 0.20435976785398435, 0.2114164904862492, 0.25173373086858075, 0.21254115804689167, 0.28368794326241176, 0.20359746425472336, 0.19601439530743672, 0.1366120218579292, 0.0677509573953905, 0.1337345518931221, 0.131319820239284, 0.13025085695817507, 0.13220493385093235, 0.12722651458835776, 0.18478611400929434, 0.45380708373943257, 0.2549116526144535, 0.20768571953100334, 0.21192232920322907, 0.10695190224253026, 0.09456539197221227, 0.16340354029137916, 0.15806151185530082, 0.1761106190584838, 0.1524005506084336, 0.17781501652467946, 0.2477701302180391, 0.20289131704404606, 0.15079166571925762, 0.1000750625563096, 0.1456312052325437, 0.27626158697313663, 0.154480081456021, 0.19783090447467178, 0.20393879596277142, 0.09434065118634205, 0.32299309989114455, 0.3320232230229314, 0.3408917837496084, 0.31291832137946285, 0.16187778227438332, 0.38553852082244267, 0.21819944276537662, 0.2181981961393742, 0.2567398325753034, 0.2564103032380527, 0.19548042455815873, 0.1962176583518882, 0.20563480814183577, 0.16484649370863136, 0.16771488469602416, 0.21178596585494924, 0.18941231099665057, 0.11325034768156494, 0.11265529763335726, 0.16771202617944514, 0.14950481245682434, 0.17263655322752314, 0.19436352575208926, 0.1961942095571267, 0.15518942291512516, 0.11556350116453395, 0.11398177937163025, 0.11627908723620406, 0.22603191723374885, 0.22992951882656598, 0.15568127598664105, 0.11935552291347795, 0.19396166781754376, 0.14978468976676318, 0.11061946902654052, 0.10750789469967104, 0.07208506271231263, 0.08093252728074707, 0.17064856337765666, 0.2201853839730039, 0.1092321916197403, 0.18864381997009128, 0.19228152884703809, 0.09456030284560879, 0.07501875468867159, 0.11119422515388777, 0.07527287479490163, 0.10416600866778314, 0.1126972837499185, 0.07534377994450336, 0.10995055549511412, 0.0743219058224609, 0.11280316247557765, 0.07358352725263528, 0.0906324822923632, 0.144482576502436, 0.14472052811474456, 0.1281049157547287, 0.37935545953451943, 0.183387885514269, 0.17710870914532384, 0.23030880899898576, 0.1760563721345032, 0.13230435778754934, 0.13504389610504708, 0.09084752924843899, 0.13908205841447474, 0.13565454005659006, 0.17068829956653486, 0.11484787619344684, 0.08501594432928083, 0.0859106529209619, 0.12690364413155963, 0.08244024484004853, 0.08274720728174145, 0.08238957251776036, 0.19615683435343367, 0.08179993308800038, 0.12547053637647076, 0.12432664985276842, 0.08148299380702495, 0.16467707293791878, 0.12528834545433853, 0.12279985684240935, 0.1216545012165504, 0.08012820512820484, 0.16028852576948083, 0.2572111749584216, 0.11137934228503954, 0.11074271063054031, 0.11169030764316967, 0.1471129091577872, 0.17658511882934214, 0.24171919198158198, 0.23907103825136497, 0.13675215273603047, 0.08519715061377497, 0.170735996033633, 0.0673752893267654, 0.10254789424240518, 0.13860095399814143, 0.21261583400292716, 0.35883225032583566, 0.27387637451019575, 0.23725326575039785, 0.17328528889222206, 0.20655089264063897, 0.18487189126990544, 0.12646223205239338, 0.1799100449775104, 0.1486825742431419, 0.11864155681369225, 0.08850863151738109, 0.09106086110489686, 0.15559359331093758, 0.091771183848266, 0.1899036987795033, 0.10741514439722155, 0.12498115812878964, 0.17806011804911148, 0.09514747859181716, 0.13517560157150776, 0.09474183087952448, 0.17000983149580984, 0.11227685549233257, 0.11020992366412297, 0.12552982825478898, 0.09496722793899126, 0.15444634693719814, 0.1569004390235329, 0.11565707918471668, 0.1578892458795278, 0.11766438531479184, 0.12934836909781477, 0.08685582662028013, 0.14357516296862327, 0.14068682882308337, 0.11575750987523058, 0.10248912452210934, 0.14467739160394788, 0.11675486896089438, 0.07320079175295868, 0.10330968012663422, 0.15439255765419269, 0.15290532747410657, 0.15101199931951692, 0.106941223564494, 0.12355215007455733, 0.1246882914176739, 0.15611101769469338, 0.06169986702599939, 0.09129641746301598, 0.12159916486750855, 0.09170107384017245, 0.15453565367506417, 0.14518398908944796, 0.08815751569373753, 0.17715187177141378, 0.12247398576230584, 0.12300127653538939, 0.09301001814927545, 0.15753007141859898, 0.16553572963085572, 0.13223153501360496, 0.1771569667807671, 0.43666329160279016, 0.31445493655398726, 0.1555212069219658, 0.11698422024508659, 0.15252635405306747, 0.26174816044125526, 0.25423747072498837, 0.16323205463228407, 0.20597344194137057, 0.17323519923174713, 0.2880066927043845, 0.12549676349070116, 0.12412089590945197, 0.1270379053351145, 0.1750547381227041, 0.18091598228234806, 0.350414612709693, 0.3274092615769719, 0.22624439021404763, 0.22962384924442578, 0.22041005889909537, 0.2102180929855979, 0.16902599529098894, 0.1774229406406258, 0.21264053984659803, 0.16909744995612375, 0.12776833663364906, 0.08235536688408485, 0.14507366808322875, 0.08458448253041984, 0.12790580577657917, 0.0836295541214227, 0.08319467554076533, 0.04151961980889407, 0.08245739701154275, 0.08295314575118562, 0.08361450984400998, 0.12847986946958323, 0.12695746235512617, 0.199450043790661, 0.12560184766443605, 0.21901024254210033, 0.22999084903015943, 0.13726871169447152, 0.13714286430786538, 0.17978345184227473, 0.08733706060448654, 0.1260504424211606, 0.15933086747581537, 0.22763556145024566, 0.20020521535586422, 0.0825082648671871, 0.08331606620442522, 0.1260239945874042, 0.0844773004944066, 0.09212515438037361, 0.08128432202003655, 0.611143217375415, 0.18488561190855846, 0.15926806020510886, 0.13474068545378248, 0.16852755835625732, 0.19586752715586297, 0.21482301905948464, 0.21303826882579371, 0.12998269337882729, 0.2527168498337007, 0.1477832870934226, 0.20763054373013068, 0.16034206306788645, 0.16021454345914177, 0.11025361674712464, 0.11504171213300715, 0.14384351017890928, 0.1913627409219243, 0.269905710024676, 0.16769191929291716, 0.11083403455539598, 0.10967939452883878, 0.20529597231112584, 0.10317261787545773, 0.05356186395287788, 0.10660983839460014, 0.16322094058953684, 0.2214227931262691, 0.11080346013730424, 0.22377638135751554, 0.18890281475074655, 0.17730496453901567, 0.18039687312086383, 0.22560631697687294, 0.11757793599226352, 0.18214942962639924, 0.17699121204695634, 0.17693909454769563, 0.1756440281030447, 0.17549244648480955, 0.1685401769482564, 0.2281814504621338, 0.11031442958982507, 0.056116722783389195, 0.17356090546880587, 0.18755564892903864, 0.16825659555473882, 0.17899786825927855, 0.17809440571762702, 0.2554580703080511, 0.18007202881153428, 0.18110475540744633, 0.19041576021270878, 0.1901143738353761, 0.18933570193816474, 0.26490112697009627, 0.278551532033422, 0.27652985145959486, 0.35989241819732176, 0.3160618138591753, 0.1936733376371791, 0.19795449201913118, 0.24029954515719099, 0.19430059963883584, 0.13409393318228324, 0.13249421791546867, 0.19646365422396614, 0.20491812839577572, 0.23859498681971236, 0.19065778855543725, 0.19286405078567934, 0.13427326118605754, 0.13364554504430304, 0.2536462655935323, 0.19556847446390302, 0.19361086236697744, 0.18820606793365502, 0.1776725059557338, 0.11876484560569556, 0.17585000180386223, 0.11621150493899003, 0.08738184366147152, 0.18159832911950335, 0.17720048355950313, 0.17236439116401492, 0.32938364262581343, 0.18109813397769736, 0.10248527447042632, 0.15552099533437946, 0.20444779762750498, 0.225448205016765, 0.16352855007682465, 0.10211897525420621, 0.1548389674738193, 0.1538468011962668, 0.10389621604583454, 0.16255758440575363, 0.1089622084273989, 0.1449494709438115, 0.10490428206458846, 0.10498687664042272, 0.052938062466917346, 0.10802053177723092, 0.10992031607754416, 0.054794536999663856, 0.10663829801750291, 0.10188487009679116, 0.10152284263959532, 0.10354647341565548, 0.10498690557020353, 0.10746910263299547, 0.10675207592473757, 0.1080497343996234, 0.10893246187363426, 0.10718125924768507, 0.15839497551734527, 0.1027749229188024, 0.05158628148198563, 0.05232862375721403, 0.16420361247948545, 0.133523757448345, 0.10669549067118811, 0.10675207592474867, 0.10709511594659271, 0.16042780748661833, 0.05494507153273043, 0.08060650699039384, 0.10767385632395876, 0.1094091903719896, 0.1088437388935104, 0.11226495411409965, 0.1566325789918388, 0.23049104460279857, 0.16652800887410613, 0.10860712177267473, 0.10545760481952549, 0.15052699554009008, 0.12713972639042392, 0.1531395164314331, 0.10057838260250085, 0.19413368608097725, 0.23998081535656635, 0.14245274404162678, 0.09261403599060136, 0.0929152198799299, 0.09151224455187235, 0.09053872385074714, 0.0455166135639562, 0.04570383912249065, 0.04509584927025467, 0.08994828428755541, 0.07689929669990048, 0.06606474665239448, 0.054705161712487176, 0.043177892918833116, 0.08552500687564746, 0.08576329331045908, 0.043658641837884815, 0.13129102844638418, 0.0887705281846396, 0.08735532192123974, 0.13321494633838915, 0.08699436182691977, 0.08613264427217437, 0.04345936549325913, 0.126236067022778, 0.0833854528627942, 0.12998305940021826, 0.13201320771085157, 0.0637216940100549, 0.08264464221102163, 0.12133468645841305, 0.10058884358824516, 0.08130081300812275, 0.07971304574366789, 0.07849294772594995, 0.1833180629890907, 0.14162122077041595, 0.20069933995781053, 0.11104942079460667, 0.09980387519646916, 0.07311277890033052, 0.08080377067913869, 0.09700423489611976, 0.07024938531787761, 0.08839072532235148, 0.07062154817545907, 0.07010165600112028, 0.10460251046024993, 0.10465725069863097, 0.06994229974293109, 0.10526379291176946, 0.10314646668235228, 0.10429344049310285, 0.06945650495927547, 0.10357552806891102, 0.19632721116271679, 0.10456606359249276, 0.10422102702516423, 0.06114976467698208, 0.06857534917820707, 0.03439381013149445, 0.06863417982154729, 0.09317100118987343, 0.06808510835556447, 0.06816632583503958, 0.06836438415386281, 0.06915629322268302, 0.06900120957441525, 0.0693601546778333, 0.08721378365614818, 0.03502018919119276, 0.03487358432098797, 0.1019541235895316, 0.15905802505731814, 0.11144575195868867, 0.05634372848671299, 0.07608902691151886, 0.057885813386338025, 0.07875566361609798, 0.11787819253439302, 0.11973658429350054, 0.07906701237971325, 0.07886435331230235, 0.05871417395437729, 0.10550919420964089, 0.0382336087381141, 0.10139426380478267, 0.10634635006961934, 0.03565698098718095, 0.10627013896932214, 0.10563380281689573, 0.06951686623668518, 0.06977150046575398, 0.0693120797106983, 0.2524099214411213, 0.1140787159065959, 0.16986488708854441, 0.08684350991827738, 0.1311195072932314, 0.0440237749703809, 0.10090190852458103, 0.08898787428854305, 0.13580875373181422, 0.13026489646683626, 0.08724104062102778, 0.08771128456980248, 0.08701327364112887, 0.08624407072013174, 0.08639310467335459, 0.04244482173174635, 0.04327131112074056, 0.08678721597382699, 0.08814457419449861, 0.08800880514056586, 0.04482295835543937, 0.088869144577719, 0.17700366088213015, 0.13392923872587725, 0.056388479424979154, 0.0894654484478119, 0.0892259691954389, 0.2189622140354508, 0.3004137457620404, 0.11244379390755178, 0.14508678368334027, 0.10604455195646345, 0.10893259113611276, 0.0712123932131048, 0.0714413310876405, 0.07299332297573402, 0.07220225075529885, 0.09726578884249881, 0.03558085862478966, 0.10725777618876277, 0.07172320629610374, 0.07291302417200551, 0.0906864975264321, 0.035880875493354836, 0.05308898807935836, 0.07997639315754368, 0.07141582088029952, 0.11463514528701202, 0.16659751123717648, 0.12620970882034066, 0.1275781473797588, 0.20550774264991298, 0.12427506213753547, 0.08259346551471314, 0.08303923961776594, 0.08259343734342006, 0.11904778776614111, 0.12219959266802638, 0.08190008190007703, 0.08014426288474041, 0.08156608208201854, 0.08014426288474596, 0.08153283050876259, 0.08228759862747292, 0.08074283407347171, 0.12133686975576707, 0.08128429516720548, 0.08213557902463364, 0.10205928202751124, 0.08144983695105301, 0.12599750227547668, 0.07936512935571294, 0.1213357979454266, 0.09950344389783039, 0.07977724741447534, 0.08024072539547511, 0.060524811422491, 0.07998400639743775, 0.07972892483391614, 0.08061266928335309, 0.0806614269541539, 0.12153133918328952, 0.08100446853325227, 0.07990412781596068, 0.11661808020725362, 0.0769674840888912, 0.14970194074671594, 0.05539361795389164, 0.11265515466076792, 0.11104945122703547, 0.0919378618051081, 0.07295276551254615, 0.0553036070597257, 0.09376965873597598, 0.07565727526197552, 0.03795068600004892, 0.06602178943289072, 0.07562873085295019, 0.12590790851660372, 0.07838529066149791, 0.07665773452741886, 0.11727914223924608, 0.16701953466056185, 0.13067669743570542, 0.13357081899846301, 0.0890275920146677, 0.044483985765109146, 0.04463289666604142, 0.0447828034034925, 0.08845643520565273, 0.04420866489832642, 0.08808643139189831, 0.08680562096536892, 0.04383081516671772, 0.043525573334574075, 0.08605853572734024, 0.08841734707698001, 0.08928578546379962, 0.0880863288698408, 0.08847600521345034, 0.044883746463242336, 0.08828077143092639, 0.06595489693771439, 0.08686214327104924, 0.04348771677666896, 0.08847600521345589, 0.0894654484478119, 0.29260479770573666, 0.1576228441262595, 0.12828905862839468, 0.1566120417278105, 0.08635580193722125, 0.08733864281088022, 0.13528749278569618, 0.05672837146427634, 0.08954556078934472, 0.06576187376804432, 0.07505438373605622, 0.08442380751372269, 0.041736227045063856, 0.04144218814752243, 0.08227089324496228, 0.08128429516721658, 0.04071661237785573, 0.04056797802450829, 0.04043671653862213, 0.04060089321965554, 0.040404040404046215, 0.040428544203013894, 0.08079192074265107, 0.07860097978616976, 0.18703521033153758, 0.07656967840734552, 0.05742739348229553, 0.07748935684734337, 0.08032128514056658, 0.04001600640255454, 0.08238932085066207, 0.06100438416510312, 0.08121827746010446, 0.04072490496692671, 0.05028450378945504, 0.08030524052555776, 0.040559725861988305, 0.04078304104101482, 0.09335316518238046, 0.08285005564236947, 0.08077545744077375, 0.08266173873560656, 0.08169934640522848, 0.08159946945363039, 0.08156608208202965, 0.06120245893865282, 0.08211866492684461, 0.08200094406990432, 0.061830196761980545, 0.08118530880406039, 0.1198083831418506, 0.03943995420577773, 0.11333599748808433, 0.07741438646065579, 0.07720517562276874, 0.03798670602372933, 0.06725846626154386, 0.07770007770008247, 0.07726482807119961, 0.0804343487179171, 0.07892659826361226, 0.039706188395099984, 0.07933361026144503, 0.07947546508859649, 0.12155609533058831, 0.040088324486431404, 0.07947549018833544, 0.04925758780220391, 0.03960396039604519, 0.059833404654752176, 0.048824662704038964, 0.05900982720878911, 0.04918962285734296, 0.03965107057890549, 0.07927070947284509, 0.05914221627575378, 0.07877116975187337, 0.039635354736411436, 0.0791609224213563, 0.08059641673170415, 0.12315279237329334, 0.09174399741643802, 0.05040973787180936, 0.04014452027298887, 0.03998400639744215, 0.07742935732734102, 0.1169363807745738, 0.28469562415495697, 0.08830037565669135, 0.0901104995479296], \"yaxis\": \"y2\"}],\n",
       "                        {\"height\": 1000, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"SUM -> Demand  and Sell_price\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0]}, \"xaxis2\": {\"anchor\": \"y2\", \"domain\": [0.0, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.575, 1.0]}, \"yaxis2\": {\"anchor\": \"x2\", \"domain\": [0.0, 0.425]}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c78d2219-f4fd-4998-a2ed-76406a76c168');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                \n",
       "            </script>\n",
       "        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=day_sum.day, \n",
    "                         y=day_sum.median_vol,\n",
    "                         #showlegend=False,\n",
    "                         mode=\"lines\",\n",
    "                         name=\"median_vol\",\n",
    "                         #marker=dict(color=\"mediumseagreen\"),\n",
    "                         ),\n",
    "\n",
    "              row=1,col=1         \n",
    "              )\n",
    "\n",
    "fig.add_trace(go.Scatter(x=day_sum.day, \n",
    "                         y=day_sum.sum_ret,\n",
    "                         #showlegend=False,\n",
    "                         mode=\"lines\",\n",
    "                         name=\"sum_ret\",\n",
    "                         #marker=dict(color=\"mediumseagreen\")\n",
    "                         ),\n",
    "             \n",
    "              row=2,col=1           \n",
    "              )\n",
    "\n",
    "fig.update_layout(height=1000, title_text=\"SUM -> Demand  and Sell_price\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtzGAJiYRslV"
   },
   "outputs": [],
   "source": [
    "Observation\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6hUxQWzT2AR"
   },
   "source": [
    "Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 1349,
     "status": "ok",
     "timestamp": 1613317088220,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "zD6C2XAxMRPe",
    "outputId": "e412ec9f-760d-4465-e34f-694327e7071a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>pid</th>\n",
       "      <th>day</th>\n",
       "      <th>abs_ret0</th>\n",
       "      <th>abs_ret1</th>\n",
       "      <th>abs_ret2</th>\n",
       "      <th>abs_ret3</th>\n",
       "      <th>abs_ret4</th>\n",
       "      <th>abs_ret5</th>\n",
       "      <th>abs_ret6</th>\n",
       "      <th>abs_ret7</th>\n",
       "      <th>abs_ret8</th>\n",
       "      <th>abs_ret9</th>\n",
       "      <th>abs_ret10</th>\n",
       "      <th>abs_ret11</th>\n",
       "      <th>abs_ret12</th>\n",
       "      <th>abs_ret13</th>\n",
       "      <th>abs_ret14</th>\n",
       "      <th>abs_ret15</th>\n",
       "      <th>abs_ret16</th>\n",
       "      <th>abs_ret17</th>\n",
       "      <th>abs_ret18</th>\n",
       "      <th>abs_ret19</th>\n",
       "      <th>abs_ret20</th>\n",
       "      <th>abs_ret21</th>\n",
       "      <th>abs_ret22</th>\n",
       "      <th>abs_ret23</th>\n",
       "      <th>abs_ret24</th>\n",
       "      <th>abs_ret25</th>\n",
       "      <th>abs_ret26</th>\n",
       "      <th>abs_ret27</th>\n",
       "      <th>abs_ret28</th>\n",
       "      <th>abs_ret29</th>\n",
       "      <th>abs_ret30</th>\n",
       "      <th>abs_ret31</th>\n",
       "      <th>abs_ret32</th>\n",
       "      <th>abs_ret33</th>\n",
       "      <th>abs_ret34</th>\n",
       "      <th>abs_ret35</th>\n",
       "      <th>...</th>\n",
       "      <th>rel_vol32</th>\n",
       "      <th>rel_vol33</th>\n",
       "      <th>rel_vol34</th>\n",
       "      <th>rel_vol35</th>\n",
       "      <th>rel_vol36</th>\n",
       "      <th>rel_vol37</th>\n",
       "      <th>rel_vol38</th>\n",
       "      <th>rel_vol39</th>\n",
       "      <th>rel_vol40</th>\n",
       "      <th>rel_vol41</th>\n",
       "      <th>rel_vol42</th>\n",
       "      <th>rel_vol43</th>\n",
       "      <th>rel_vol44</th>\n",
       "      <th>rel_vol45</th>\n",
       "      <th>rel_vol46</th>\n",
       "      <th>rel_vol47</th>\n",
       "      <th>rel_vol48</th>\n",
       "      <th>rel_vol49</th>\n",
       "      <th>rel_vol50</th>\n",
       "      <th>rel_vol51</th>\n",
       "      <th>rel_vol52</th>\n",
       "      <th>rel_vol53</th>\n",
       "      <th>rel_vol54</th>\n",
       "      <th>rel_vol55</th>\n",
       "      <th>rel_vol56</th>\n",
       "      <th>rel_vol57</th>\n",
       "      <th>rel_vol58</th>\n",
       "      <th>rel_vol59</th>\n",
       "      <th>rel_vol60</th>\n",
       "      <th>LS</th>\n",
       "      <th>NLV</th>\n",
       "      <th>min_ret</th>\n",
       "      <th>max_ret</th>\n",
       "      <th>std_ret</th>\n",
       "      <th>median_ret</th>\n",
       "      <th>sum_ret</th>\n",
       "      <th>min_vol</th>\n",
       "      <th>max_vol</th>\n",
       "      <th>std_vol</th>\n",
       "      <th>median_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>-3.403606</td>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>0.073265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036601</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>0.029261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073206</td>\n",
       "      <td>0.032942</td>\n",
       "      <td>0.036609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>0.036643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036630</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014621</td>\n",
       "      <td>0.004915</td>\n",
       "      <td>0.009695</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>0.005018</td>\n",
       "      <td>0.003520</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.007395</td>\n",
       "      <td>0.007272</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.008937</td>\n",
       "      <td>0.008786</td>\n",
       "      <td>0.009677</td>\n",
       "      <td>0.008480</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>0.009431</td>\n",
       "      <td>0.004521</td>\n",
       "      <td>0.015771</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.012697</td>\n",
       "      <td>0.019507</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>-2.307230</td>\n",
       "      <td>0.646580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>0.022135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.076994</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>0.011438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>-2.904798</td>\n",
       "      <td>360</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014802</td>\n",
       "      <td>0.014818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>0.004953</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.008522</td>\n",
       "      <td>0.011424</td>\n",
       "      <td>0.042281</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>0.017466</td>\n",
       "      <td>0.023188</td>\n",
       "      <td>0.008817</td>\n",
       "      <td>0.006589</td>\n",
       "      <td>0.005276</td>\n",
       "      <td>0.011648</td>\n",
       "      <td>0.011829</td>\n",
       "      <td>0.015558</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>-2.263960</td>\n",
       "      <td>0.487708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058949</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.128265</td>\n",
       "      <td>0.023473</td>\n",
       "      <td>0.008817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>-2.337807</td>\n",
       "      <td>360</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029678</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.048146</td>\n",
       "      <td>0.092654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118457</td>\n",
       "      <td>0.059084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029588</td>\n",
       "      <td>0.044418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>0.007961</td>\n",
       "      <td>0.003208</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.007806</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.004943</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.022513</td>\n",
       "      <td>0.124384</td>\n",
       "      <td>0.017086</td>\n",
       "      <td>0.023940</td>\n",
       "      <td>0.036648</td>\n",
       "      <td>0.025713</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.009542</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>0.018640</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>0.007921</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.029692</td>\n",
       "      <td>0.016440</td>\n",
       "      <td>0.011970</td>\n",
       "      <td>-2.343106</td>\n",
       "      <td>0.544476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118457</td>\n",
       "      <td>0.024431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124384</td>\n",
       "      <td>0.017673</td>\n",
       "      <td>0.010645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151</td>\n",
       "      <td>-2.285676</td>\n",
       "      <td>360</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044557</td>\n",
       "      <td>0.052169</td>\n",
       "      <td>0.044613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009762</td>\n",
       "      <td>0.016240</td>\n",
       "      <td>0.012835</td>\n",
       "      <td>0.008973</td>\n",
       "      <td>0.023312</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009139</td>\n",
       "      <td>0.006265</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.012355</td>\n",
       "      <td>0.011745</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.005562</td>\n",
       "      <td>0.007811</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>0.062768</td>\n",
       "      <td>0.072686</td>\n",
       "      <td>0.009503</td>\n",
       "      <td>0.017088</td>\n",
       "      <td>0.013628</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>0.016834</td>\n",
       "      <td>0.031869</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.065173</td>\n",
       "      <td>0.048286</td>\n",
       "      <td>0.021237</td>\n",
       "      <td>-2.331621</td>\n",
       "      <td>0.633181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052169</td>\n",
       "      <td>0.012455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.072686</td>\n",
       "      <td>0.014455</td>\n",
       "      <td>0.011745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>-2.606321</td>\n",
       "      <td>360</td>\n",
       "      <td>4</td>\n",
       "      <td>0.088086</td>\n",
       "      <td>0.109737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.036627</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>0.197325</td>\n",
       "      <td>0.094835</td>\n",
       "      <td>0.029218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>0.058245</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.018169</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.014541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076542</td>\n",
       "      <td>0.182548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051136</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.005520</td>\n",
       "      <td>0.015532</td>\n",
       "      <td>0.031905</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.015763</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.014603</td>\n",
       "      <td>0.018986</td>\n",
       "      <td>0.006644</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.022630</td>\n",
       "      <td>0.025087</td>\n",
       "      <td>0.020777</td>\n",
       "      <td>0.055104</td>\n",
       "      <td>0.019893</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>0.008083</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.013227</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>0.013394</td>\n",
       "      <td>0.009415</td>\n",
       "      <td>0.024983</td>\n",
       "      <td>0.019524</td>\n",
       "      <td>0.018342</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>0.012525</td>\n",
       "      <td>-2.415575</td>\n",
       "      <td>0.880580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197325</td>\n",
       "      <td>0.042962</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>0.055104</td>\n",
       "      <td>0.010978</td>\n",
       "      <td>0.014539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684477</th>\n",
       "      <td>1536855</td>\n",
       "      <td>-1.691818</td>\n",
       "      <td>596</td>\n",
       "      <td>800</td>\n",
       "      <td>1.125704</td>\n",
       "      <td>0.865801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124611</td>\n",
       "      <td>0.124456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062344</td>\n",
       "      <td>0.124533</td>\n",
       "      <td>0.124688</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>0.876644</td>\n",
       "      <td>0.248293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435594</td>\n",
       "      <td>0.743494</td>\n",
       "      <td>0.187266</td>\n",
       "      <td>0.249221</td>\n",
       "      <td>0.062112</td>\n",
       "      <td>0.186451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.248139</td>\n",
       "      <td>0.123762</td>\n",
       "      <td>0.061958</td>\n",
       "      <td>0.124378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124301</td>\n",
       "      <td>0.061920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061920</td>\n",
       "      <td>0.061958</td>\n",
       "      <td>0.123839</td>\n",
       "      <td>0.494743</td>\n",
       "      <td>0.093226</td>\n",
       "      <td>0.093139</td>\n",
       "      <td>0.062035</td>\n",
       "      <td>0.123993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007108</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>0.012158</td>\n",
       "      <td>0.011558</td>\n",
       "      <td>0.016540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.042061</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>0.021215</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>0.013004</td>\n",
       "      <td>0.018094</td>\n",
       "      <td>0.043910</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>0.036264</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.024601</td>\n",
       "      <td>-2.933123</td>\n",
       "      <td>-1.488287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.125704</td>\n",
       "      <td>0.227919</td>\n",
       "      <td>0.123916</td>\n",
       "      <td>0.123916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057383</td>\n",
       "      <td>0.014235</td>\n",
       "      <td>0.013105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684478</th>\n",
       "      <td>1536856</td>\n",
       "      <td>-2.389487</td>\n",
       "      <td>596</td>\n",
       "      <td>801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.502197</td>\n",
       "      <td>0.441640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125392</td>\n",
       "      <td>0.187852</td>\n",
       "      <td>0.062735</td>\n",
       "      <td>0.690521</td>\n",
       "      <td>0.124844</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310945</td>\n",
       "      <td>0.247219</td>\n",
       "      <td>0.185874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124069</td>\n",
       "      <td>0.186567</td>\n",
       "      <td>0.186220</td>\n",
       "      <td>0.062035</td>\n",
       "      <td>0.124146</td>\n",
       "      <td>0.186451</td>\n",
       "      <td>0.249066</td>\n",
       "      <td>0.312110</td>\n",
       "      <td>0.124456</td>\n",
       "      <td>0.062305</td>\n",
       "      <td>0.062305</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062539</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125235</td>\n",
       "      <td>0.250156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007532</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.012639</td>\n",
       "      <td>0.022021</td>\n",
       "      <td>0.024407</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.020184</td>\n",
       "      <td>0.011214</td>\n",
       "      <td>0.036825</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.032483</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.020222</td>\n",
       "      <td>0.012808</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022595</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.007037</td>\n",
       "      <td>0.017105</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019984</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>-3.221969</td>\n",
       "      <td>-1.450029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.165331</td>\n",
       "      <td>0.124650</td>\n",
       "      <td>0.124650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093095</td>\n",
       "      <td>0.015825</td>\n",
       "      <td>0.012639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684479</th>\n",
       "      <td>1536857</td>\n",
       "      <td>-1.426908</td>\n",
       "      <td>596</td>\n",
       "      <td>802</td>\n",
       "      <td>1.086262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568900</td>\n",
       "      <td>0.471846</td>\n",
       "      <td>0.189753</td>\n",
       "      <td>0.475285</td>\n",
       "      <td>0.441362</td>\n",
       "      <td>0.251572</td>\n",
       "      <td>0.219642</td>\n",
       "      <td>0.031447</td>\n",
       "      <td>0.220057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438048</td>\n",
       "      <td>0.405869</td>\n",
       "      <td>0.434243</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.187617</td>\n",
       "      <td>0.249688</td>\n",
       "      <td>0.188088</td>\n",
       "      <td>0.125235</td>\n",
       "      <td>0.501567</td>\n",
       "      <td>0.063052</td>\n",
       "      <td>0.063012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063131</td>\n",
       "      <td>0.378549</td>\n",
       "      <td>0.125707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094607</td>\n",
       "      <td>0.125549</td>\n",
       "      <td>0.062854</td>\n",
       "      <td>0.062814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313873</td>\n",
       "      <td>0.125945</td>\n",
       "      <td>0.062814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.005640</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.024132</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.017338</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015499</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.009232</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.017401</td>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>0.020105</td>\n",
       "      <td>0.037890</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.028835</td>\n",
       "      <td>-3.292824</td>\n",
       "      <td>-1.375085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.086262</td>\n",
       "      <td>0.194174</td>\n",
       "      <td>0.126342</td>\n",
       "      <td>0.126342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065599</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>0.014552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684480</th>\n",
       "      <td>1536858</td>\n",
       "      <td>-1.510139</td>\n",
       "      <td>596</td>\n",
       "      <td>803</td>\n",
       "      <td>1.505646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.157360</td>\n",
       "      <td>1.556420</td>\n",
       "      <td>2.305665</td>\n",
       "      <td>0.515132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390371</td>\n",
       "      <td>0.522876</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.065660</td>\n",
       "      <td>0.788955</td>\n",
       "      <td>0.260756</td>\n",
       "      <td>0.195950</td>\n",
       "      <td>0.195567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.454841</td>\n",
       "      <td>0.587467</td>\n",
       "      <td>0.523560</td>\n",
       "      <td>0.688750</td>\n",
       "      <td>0.065147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.194553</td>\n",
       "      <td>0.454841</td>\n",
       "      <td>0.324886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.388853</td>\n",
       "      <td>0.065062</td>\n",
       "      <td>0.455729</td>\n",
       "      <td>0.130804</td>\n",
       "      <td>0.065274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004685</td>\n",
       "      <td>0.008470</td>\n",
       "      <td>0.010223</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.028942</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>0.014393</td>\n",
       "      <td>0.014687</td>\n",
       "      <td>0.014645</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>0.032888</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.036051</td>\n",
       "      <td>0.008518</td>\n",
       "      <td>0.010525</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.017556</td>\n",
       "      <td>0.012673</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>0.013435</td>\n",
       "      <td>0.011664</td>\n",
       "      <td>0.003293</td>\n",
       "      <td>0.010009</td>\n",
       "      <td>0.021838</td>\n",
       "      <td>0.021954</td>\n",
       "      <td>0.014121</td>\n",
       "      <td>0.031530</td>\n",
       "      <td>-2.915789</td>\n",
       "      <td>-1.099627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.305665</td>\n",
       "      <td>0.467558</td>\n",
       "      <td>0.227593</td>\n",
       "      <td>0.227593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087063</td>\n",
       "      <td>0.014836</td>\n",
       "      <td>0.012455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684481</th>\n",
       "      <td>1536859</td>\n",
       "      <td>-1.372685</td>\n",
       "      <td>596</td>\n",
       "      <td>804</td>\n",
       "      <td>0.262812</td>\n",
       "      <td>0.392413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393959</td>\n",
       "      <td>0.264201</td>\n",
       "      <td>0.131752</td>\n",
       "      <td>0.066094</td>\n",
       "      <td>0.264375</td>\n",
       "      <td>0.133156</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.533689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232404</td>\n",
       "      <td>0.166389</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200535</td>\n",
       "      <td>0.133422</td>\n",
       "      <td>0.467602</td>\n",
       "      <td>0.201342</td>\n",
       "      <td>0.134499</td>\n",
       "      <td>0.606878</td>\n",
       "      <td>0.815772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201613</td>\n",
       "      <td>0.201207</td>\n",
       "      <td>0.134409</td>\n",
       "      <td>0.268276</td>\n",
       "      <td>0.267738</td>\n",
       "      <td>0.134228</td>\n",
       "      <td>0.134499</td>\n",
       "      <td>0.201748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134771</td>\n",
       "      <td>0.201884</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>0.014651</td>\n",
       "      <td>0.011775</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.022659</td>\n",
       "      <td>0.008305</td>\n",
       "      <td>0.017365</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.009337</td>\n",
       "      <td>0.018775</td>\n",
       "      <td>0.009399</td>\n",
       "      <td>0.005955</td>\n",
       "      <td>0.015095</td>\n",
       "      <td>0.007021</td>\n",
       "      <td>0.011471</td>\n",
       "      <td>0.014585</td>\n",
       "      <td>0.017187</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.024149</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.023243</td>\n",
       "      <td>0.015592</td>\n",
       "      <td>0.008518</td>\n",
       "      <td>-3.095411</td>\n",
       "      <td>-1.303538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.815772</td>\n",
       "      <td>0.162025</td>\n",
       "      <td>0.134635</td>\n",
       "      <td>0.134635</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.014312</td>\n",
       "      <td>0.013707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>684482 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID    target  pid  day  ...   min_vol   max_vol   std_vol  median_vol\n",
       "0           148 -3.403606  360    0  ...  0.001629  0.076994  0.014242    0.011438\n",
       "1           149 -2.904798  360    1  ...  0.000753  0.128265  0.023473    0.008817\n",
       "2           150 -2.337807  360    2  ...  0.000000  0.124384  0.017673    0.010645\n",
       "3           151 -2.285676  360    3  ...  0.001385  0.072686  0.014455    0.011745\n",
       "4           152 -2.606321  360    4  ...  0.002315  0.055104  0.010978    0.014539\n",
       "...         ...       ...  ...  ...  ...       ...       ...       ...         ...\n",
       "684477  1536855 -1.691818  596  800  ...  0.000000  0.057383  0.014235    0.013105\n",
       "684478  1536856 -2.389487  596  801  ...  0.000000  0.093095  0.015825    0.012639\n",
       "684479  1536857 -1.426908  596  802  ...  0.000000  0.065599  0.014772    0.014552\n",
       "684480  1536858 -1.510139  596  803  ...  0.000000  0.087063  0.014836    0.012455\n",
       "684481  1536859 -1.372685  596  804  ...  0.000924  0.068966  0.014312    0.013707\n",
       "\n",
       "[684482 rows x 137 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1613317239272,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "Hkz8QS1MUMF-"
   },
   "outputs": [],
   "source": [
    "## For each Deportment mean of deman and sell_price\n",
    "\n",
    "#dep_pid = train_dataset.groupby([\"day\",\"pid\"])[[\"sum_ret\", \"median_vol\"]].agg([\"mean\", \"max\"]).reset_index()\n",
    "dep_pid = train_dataset[[\"day\", \"pid\", \"sum_ret\", \"median_vol\"]].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1613317239972,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "58F9yf-pVUnh",
    "outputId": "1899d41a-87b4-4646-aaa5-25b9d7a3a423"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>day</th>\n",
       "      <th>pid</th>\n",
       "      <th>sum_ret</th>\n",
       "      <th>median_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>360</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.014539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684477</th>\n",
       "      <td>684477</td>\n",
       "      <td>800</td>\n",
       "      <td>596</td>\n",
       "      <td>0.123916</td>\n",
       "      <td>0.013105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684478</th>\n",
       "      <td>684478</td>\n",
       "      <td>801</td>\n",
       "      <td>596</td>\n",
       "      <td>0.124650</td>\n",
       "      <td>0.012639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684479</th>\n",
       "      <td>684479</td>\n",
       "      <td>802</td>\n",
       "      <td>596</td>\n",
       "      <td>0.126342</td>\n",
       "      <td>0.014552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684480</th>\n",
       "      <td>684480</td>\n",
       "      <td>803</td>\n",
       "      <td>596</td>\n",
       "      <td>0.227593</td>\n",
       "      <td>0.012455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684481</th>\n",
       "      <td>684481</td>\n",
       "      <td>804</td>\n",
       "      <td>596</td>\n",
       "      <td>0.134635</td>\n",
       "      <td>0.013707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>684482 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  day  pid   sum_ret  median_vol\n",
       "0            0    0  360  0.000000    0.011438\n",
       "1            1    1  360  0.000000    0.008817\n",
       "2            2    2  360  0.000000    0.010645\n",
       "3            3    3  360  0.000000    0.011745\n",
       "4            4    4  360  0.007275    0.014539\n",
       "...        ...  ...  ...       ...         ...\n",
       "684477  684477  800  596  0.123916    0.013105\n",
       "684478  684478  801  596  0.124650    0.012639\n",
       "684479  684479  802  596  0.126342    0.014552\n",
       "684480  684480  803  596  0.227593    0.012455\n",
       "684481  684481  804  596  0.134635    0.013707\n",
       "\n",
       "[684482 rows x 5 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1613317376316,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "EnGXW2p3TXXB",
    "outputId": "c59a5db3-e5d8-46c8-c203-50bab2c14ca5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
       "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
       "            <div id=\"8a65fd81-2385-4969-9966-b60508bba93b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                \n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"8a65fd81-2385-4969-9966-b60508bba93b\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '8a65fd81-2385-4969-9966-b60508bba93b',\n",
       "                        [{\"mode\": \"lines\", \"name\": \"360\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804], \"xaxis\": \"x\", \"y\": [0.0114379848521036, 0.008816771735157648, 0.010644831392831941, 0.011744603126336954, 0.014538543127225894, 0.012778805540207085, 0.014362803259707497, 0.013653229155147727, 0.01123278215929267, 0.01294097642258598, 0.011733467938065765, 0.01200313758984324, 0.010739077260290731, 0.013024360802563546, 0.008113880343464192, 0.009644727594897163, 0.013100749651514849, 0.012862809423107023, 0.014102547476179837, 0.013647118097770559, 0.012966887580462398, 0.012229016626383309, 0.013163404962144241, 0.01043904566563292, 0.01470850671191812, 0.013798012698608279, 0.01295985743480526, 0.013813819890428809, 0.013564517695599956, 0.012913655430178042, 0.013913418555836877, 0.01214192170761462, 0.014036097560463558, 0.00976263379142065, 0.013457929705902906, 0.01199450130551094, 0.011578082482495334, 0.014739620961674724, 0.012283584021220966, 0.011798572470623684, 0.013193167213107052, 0.013309759231901556, 0.014572633267061862, 0.014260842464256154, 0.011944989543083505, 0.014210409884001688, 0.015867224540725346, 0.012556410770214466, 0.015290989140896103, 0.015189793088088885, 0.009223210441573813, 0.013563072724614772, 0.013944103581744936, 0.013762036068264752, 0.014879701446788797, 0.011836066786163446, 0.012931516106937352, 0.011188829959326107, 0.012985083274613795, 0.012714362827370549, 0.01229370554203105, 0.01331204512687806, 0.010733558335703426, 0.010150446691960672, 0.009335475674319726, 0.0102928273750666, 0.013169930285960191, 0.00957783885406082, 0.012311341160543693, 0.01298868384515711, 0.010119651347595005, 0.012224753864608051, 0.014713811329495173, 0.013687366004769552, 0.014143153788506212, 0.01051015252366359, 0.010363076860809277, 0.009653570684611387, 0.011543004235126235, 0.012902315860835213, 0.010250833077294312, 0.013959270126462382, 0.013080052840576851, 0.0109306305907405, 0.0143992068536667, 0.013384084181031085, 0.012935028349002795, 0.012084548564818241, 0.010986010161778149, 0.014680308864115137, 0.014653882419078603, 0.014641711876795353, 0.012315459415984713, 0.011927701050082751, 0.013406662402047726, 0.015802853470080074, 0.01408539794243232, 0.01322582879200828, 0.013854144008683927, 0.01402101176277922, 0.012318471309013687, 0.012582226879139286, 0.015509108871277372, 0.012811982243997112, 0.01307722468347302, 0.010635139007181623, 0.013011964084690564, 0.012000051991953278, 0.013496338830472387, 0.0125098876376883, 0.011665288181772036, 0.01294348187991378, 0.012795044980275509, 0.011421575000726393, 0.008782728530947267, 0.012733021124044614, 0.012468352588135152, 0.013640704791792756, 0.010475785760966349, 0.011243475983669905, 0.012941760071830193, 0.013348307249536343, 0.010464376740986157, 0.01331216217470751, 0.011960962121522277, 0.010850573393179793, 0.013464173911825892, 0.011459584874299428, 0.013653220288967307, 0.010689322067764193, 0.013259073958100078, 0.014322740617825772, 0.012858206393808422, 0.01300518754640153, 0.015384554753021303, 0.011610571903942013, 0.013954792388700787, 0.011696981557062669, 0.011135510351888268, 0.015580328364159022, 0.013099686260722184, 0.010624656143376907, 0.013584656162567087, 0.0109871377187433, 0.010577489261747838, 0.014488199008560503, 0.010997145067297869, 0.010782043603467214, 0.01231885353322231, 0.0129406063404708, 0.013027030108770649, 0.015685637491679873, 0.013673168198861718, 0.013462546075760204, 0.010514715549951623, 0.013490191625681469, 0.011500484253295765, 0.014763517058646718, 0.014911425645082575, 0.012833165806162526, 0.01521681949614034, 0.011721317174545168, 0.011660650739240322, 0.01237098101696932, 0.011623279597269078, 0.01153429072806392, 0.009481758315033213, 0.013771139666409662, 0.012248708669271672, 0.014608602780143675, 0.012104376754163768, 0.013297042767193871, 0.013595161156724956, 0.01448640562687644, 0.0136231797867649, 0.012967302296212674, 0.012400192677554423, 0.01297226493158845, 0.012146124467240027, 0.01335686637804035, 0.01262914328031609, 0.009645235981468388, 0.012144414343164987, 0.012292608053527509, 0.014714499522865097, 0.012601604065103132, 0.013576656359227227, 0.0105288710474956, 0.011134390695089246, 0.008868328470383173, 0.0112414502742842, 0.010921944913064795, 0.01228010267125852, 0.010102965416289435, 0.01086544509829284, 0.012278241411778005, 0.01341643042829389, 0.010833271576505174, 0.011545766109945984, 0.010094190219980807, 0.012860248679144202, 0.01161708057676476, 0.009367901216684177, 0.01199136481982183, 0.013130162294914021, 0.014868714806620569, 0.012402666572019149, 0.012740496658351742, 0.01304353457446232, 0.013148700176527209, 0.00974284801388, 0.01224129407336749, 0.011970929871479151, 0.012287812705508659, 0.01148433981764939, 0.009789228191926149, 0.011437651097851605, 0.006642702043712722, 0.008055429424014344, 0.013577596409049469, 0.013120572117329352, 0.015295949842623696, 0.013290556490438064, 0.012968541532771041, 0.008651047550816148, 0.010791352066989229, 0.01263882706739968, 0.012733532453241865, 0.01372787343603695, 0.013129806582773079, 0.01459522518455714, 0.010696484550379562, 0.012087791094932528, 0.014486656473802937, 0.014650297968684063, 0.012582878840913412, 0.014458396032040719, 0.013830557597166148, 0.016481280592956827, 0.014564291776475153, 0.013318042703359234, 0.00891613708870295, 0.013503373018614723, 0.010744542819507213, 0.01142040871283213, 0.010027523749899231, 0.0115902017221124, 0.009832370012219909, 0.010872079650777954, 0.01189465280834652, 0.013086940845561787, 0.011929614189712874, 0.013681119087065385, 0.00990469357167333, 0.011345552545017836, 0.011974190501715692, 0.010760312369233674, 0.013871801130207015, 0.010798611338129016, 0.010885715226328769, 0.01311136307414038, 0.014549865909164425, 0.008134867514529194, 0.012252791980083664, 0.006978941708404353, 0.011708017892243315, 0.009606128978617836, 0.011980782261879814, 0.013111273720847787, 0.014179831568267885, 0.011988842934732284, 0.012547863112797713, 0.011317555816004979, 0.011260229079949365, 0.013315408967013276, 0.015435659073316356, 0.011852630401374973, 0.01348227340843195, 0.012037909264613443, 0.013106822322429669, 0.014031520491896987, 0.00930808936428709, 0.011491582572304012, 0.01074534963219476, 0.01249453902618967, 0.010472389515024504, 0.01282497083606088, 0.014919505730854644, 0.010546905637275963, 0.01134814619900996, 0.013597492001060023, 0.013392726081090704, 0.014622599278281178, 0.014348572106303624, 0.011062162225975973, 0.011652972942596259, 0.014137969579244259, 0.015506732413169591, 0.012763832662864572, 0.016129984280631796, 0.015997700593360496, 0.013993946273239041, 0.014977013224864315, 0.010984781772814936, 0.01411150444037105, 0.012593688392038985, 0.015210426001844813, 0.014030325455359094, 0.01209137303068609, 0.01290208860275866, 0.01441060889508946, 0.015300480078508625, 0.012704830968993846, 0.014111181332465254, 0.012978555034433443, 0.01292109862381665, 0.013569889105212762, 0.013531184618738458, 0.014209671122730744, 0.014005645856196859, 0.013070811302202128, 0.011848966656992744, 0.013668803050946664, 0.014791832606444113, 0.012909742264150722, 0.013850030346627523, 0.012021321358061795, 0.00910393028814905, 0.012966385308361805, 0.013891964305634628, 0.011809813805532615, 0.013030670902066308, 0.013317189903431429, 0.012982525005238976, 0.013645537100281494, 0.01197682113616642, 0.011520468352092179, 0.012286880344233805, 0.015090889086940137, 0.012246970372290309, 0.01396191911968645, 0.012052395475950634, 0.011176357337858659, 0.013278242547824, 0.013965728676156062, 0.012365369975100578, 0.011219712839961204, 0.013843996811422836, 0.012528110531105623, 0.0144931283731216, 0.011978479797714813, 0.012966286843399849, 0.013446423485738102, 0.013123923084031892, 0.00875313912824089, 0.013208803271602636, 0.011807998290626944, 0.0130326227496213, 0.015301177170783484, 0.01195051301785652, 0.011445666492004349, 0.013766584632356572, 0.011336931479178249, 0.012753633565557204, 0.013922194243358024, 0.01169558531889706, 0.0105604811309457, 0.011717994106681295, 0.012527369167437671, 0.010424976418793131, 0.0130799886115775, 0.012277304870698476, 0.012888351331417405, 0.011365437012792077, 0.015368307902967188, 0.012753361873377094, 0.010804927200489287, 0.014378990557386787, 0.012231387278097592, 0.013658550065978272, 0.012018890421759802, 0.012087549793093521, 0.013103801233096323, 0.013880075718948058, 0.013488313347158193, 0.01273871809033917, 0.0132000991782685, 0.014081170258136185, 0.013006803915625784, 0.013649462181438941, 0.011630039821487496, 0.010195819005585608, 0.014108205050189376, 0.012341909832260745, 0.01136450333288506, 0.0114364548527459, 0.01391400489748566, 0.01181159930413459, 0.01373472561270641, 0.012210596712504924, 0.0140096724211189, 0.013130366720877487, 0.01444363907044057, 0.014305613935924513, 0.013011329159885369, 0.01179817881795341, 0.012529058260770191, 0.015588972777955725, 0.014460453127446544, 0.012722699104681792, 0.0140857678079431, 0.013843574486722144, 0.01296486337474416, 0.012955450598994856, 0.015111467431102478, 0.013065478396394195, 0.01564592194608272, 0.01220361773353278, 0.015364797681663885, 0.01389574622196922, 0.013392835523778212, 0.015234925016769478, 0.012693773856292792, 0.013791003917327358, 0.013526393024266262, 0.01377822059917117, 0.01133234373959021, 0.013196615261417865, 0.01386474490524062, 0.0097729448297036, 0.016071029674197733, 0.013399995839721677, 0.013362448430515615, 0.015423602635685825, 0.014086482521129987, 0.014070681893979451, 0.014826684932951662, 0.01106001619335692, 0.012748272558638935, 0.014275663481402225, 0.014568733608322613, 0.013266660268345698, 0.012228908344049293, 0.012964696110056243, 0.013025407602243284, 0.01265596897432643, 0.012566872241642107, 0.01349126332156122, 0.011648046985424413, 0.012975353022310102, 0.012399508698150605, 0.013773483598916891, 0.011636891264158128, 0.012934774803239436, 0.012903132717667956, 0.013568907619214794, 0.014159159709513825, 0.015696959430901596, 0.012201304086286521, 0.014656838628363212, 0.015555786990221303, 0.012695409405846651, 0.014668878785152016, 0.013318039364071558, 0.012757185074903638, 0.014620124323298966, 0.016696426660722533, 0.012882520884911241, 0.012929757733950631, 0.013333511124213087, 0.014378216721682331, 0.012598762992357996, 0.013521781347130566, 0.013594351294691234, 0.014577444017112903, 0.01202456576688636, 0.013811261258420795, 0.013446713786201393, 0.014346924519870515, 0.014125343833930862, 0.014543089063993684, 0.012220836618468824, 0.01272361109645266, 0.014213273376772891, 0.01333959285277188, 0.01333029148668817, 0.013550021823572391, 0.01338699330623017, 0.013093032251413093, 0.010611146778969748, 0.015969181247271726, 0.01266020382461755, 0.012207317884667028, 0.01347285786029789, 0.012583385917749398, 0.015345943114015353, 0.016300790243545582, 0.011716384500572136, 0.01489528322637072, 0.014361445764569325, 0.014177320003610888, 0.012251060814326198, 0.014227986500061638, 0.00811446491683221, 0.01482840650413509, 0.014348772125301197, 0.011509786000452323, 0.014083786600168359, 0.012228439259570976, 0.012628785387990523, 0.01410221423795108, 0.013352749042007162, 0.01302465080793587, 0.014238207668893338, 0.015219858005400162, 0.014687628260482491, 0.01382645887096809, 0.013735387822670494, 0.014924100454337653, 0.015652141406458937, 0.01407375086778604, 0.013477912005578713, 0.01332321820082898, 0.013225017327444228, 0.012589844887807605, 0.01420670657904832, 0.01533059447890734, 0.01177691989874617, 0.015259198892907838, 0.012739963142222395, 0.011381326282948048, 0.0138042281472913, 0.014359618485720925, 0.01164643855937915, 0.013876190498088695, 0.013310028937552568, 0.013153141669195036, 0.014430815633540518, 0.012416611567085014, 0.013766599460197312, 0.012454564802861138, 0.013067916835736302, 0.013482194971315769, 0.013186138612887376, 0.013960070063057715, 0.01397921781304268, 0.014604707331230062, 0.013325365787703995, 0.01243849477429163, 0.013140957320276429, 0.015192916769172575, 0.013701579889510969, 0.013009397810631752, 0.012803238284079137, 0.012158542921201358, 0.013909750412995804, 0.014848138283594322, 0.013904118169537384, 0.014454276627256657, 0.015220356590498259, 0.012881276963159835, 0.013274620425404823, 0.014311579723611013, 0.014577607274831272, 0.015101647884393018, 0.015997302948725884, 0.016516660219033624, 0.012929279532260649, 0.013097258764777485, 0.014334632342530426, 0.013404301401413785, 0.0134504780025469, 0.013231128485672669, 0.0154914080617347, 0.012021040692324769, 0.012908003392717278, 0.011835701752091431, 0.0104858709451724, 0.01413508312798956, 0.01256242296540092, 0.015341794600964334, 0.013602252739850816, 0.014747245573835896, 0.011012539387194379, 0.012323126629195802, 0.009623619009061857, 0.013394301747399268, 0.015371433897677915, 0.012497174171843052, 0.012900184963745221, 0.009820146984421612, 0.0125691630390503, 0.01211461775089714, 0.011119040843070242, 0.014483208228045413, 0.016344082885971167, 0.013572625247543848, 0.010667466463026186, 0.016062595634853138, 0.0105196317620231, 0.013025967454920495, 0.01367369012235631, 0.013449242268796652, 0.014756208069343862, 0.015179746637941263, 0.014856640241931791, 0.014264457243441803, 0.013540739517009322, 0.012778056174805916, 0.014086040397638225, 0.0158037233819483, 0.014006205355146037, 0.013027299506357786, 0.01370648423731315, 0.012638982625241657, 0.014912890304819, 0.013268402113195459, 0.013305436522527995, 0.014065790049258724, 0.01354435452834655, 0.0133742056200151, 0.012191673717739954, 0.013539373770138365, 0.010216022340091084, 0.0132571029936022, 0.013446014231856472, 0.012238918262711718, 0.012460556627849392, 0.01556427808987961, 0.015154386655820425, 0.015933131024974068, 0.01449673054626074, 0.014477454076543212, 0.01129714182426297, 0.013917691317799876, 0.01297973893032148, 0.013397025168355644, 0.012928025641536068, 0.011902935152116646, 0.015589311202189768, 0.01362208638563069, 0.015449403069879459, 0.014675518086150197, 0.008947683140885626, 0.010577411434925663, 0.012894678035573516, 0.009788801174899402, 0.011195048407312192, 0.011458196703006586, 0.01441473339556909, 0.01370624408032188, 0.014738244130090969, 0.010136624152760272, 0.01289089020212312, 0.015001462387310515, 0.012835031358799369, 0.016351001434200898, 0.012789683364918493, 0.012843559331685805, 0.011140049674388006, 0.011661837931235302, 0.013893823757362093, 0.013714777400509491, 0.014729857487633832, 0.01318096064576441, 0.01371619381354382, 0.013240553477705756, 0.015555551606968676, 0.013311781211027338, 0.01461536869521359, 0.013632302665440051, 0.012944046392376644, 0.012317325408306323, 0.013968703831313135, 0.013099458505404912, 0.011406805735857244, 0.014100262976658043, 0.01273190458467369, 0.015766819744828492, 0.014532228469199788, 0.011160063613026796, 0.011181914144205586, 0.009399244298159571, 0.01345124618665445, 0.010640834537787035, 0.009282944359014916, 0.01146990228524723, 0.011988012403783744, 0.014137622304268382, 0.013403891334980364, 0.01452975401567114, 0.01568146493249245, 0.011982143421721306, 0.012841668126975748, 0.013760116835186369, 0.01177005071835774, 0.01287057108581177, 0.013033603972147765, 0.012556723370349156, 0.008919765841819409, 0.01375497486649766, 0.015239734607159513, 0.009460120951129915, 0.014260144030474868, 0.008375804541652161, 0.012115204874875524, 0.014741247089787891, 0.012742100713452371, 0.015045110418784513, 0.013026346302774564, 0.013555799955758577, 0.011048560143379871, 0.011018004431451421, 0.01635202059382912, 0.012339783169750035, 0.013072230478602842, 0.012104245249378715, 0.012897695843108237, 0.012441490968818223, 0.015877686758349798, 0.014625101700964526, 0.014756213883562462, 0.012554205426015572, 0.010286879991949379, 0.014074098384814574, 0.014857790816086403, 0.016490292545817287, 0.014665341808408387, 0.012024434901945208, 0.013173784350616537, 0.01411909640483537, 0.01327245099060793, 0.015395454324201426, 0.010479619191556638, 0.012659635485327128, 0.011564116613214149, 0.01497006905703104, 0.014160675786497441, 0.01380552036444592, 0.01182210886211502, 0.014763929989995875, 0.012688235495363784, 0.012100025734219056, 0.01048583163165364, 0.011987273431606554, 0.014802570341131409, 0.01457468156528644, 0.013358463667435254, 0.012009758814067684, 0.013422299798314854, 0.01394830750509748, 0.014138548347623512, 0.012418780861470594, 0.0124532503199297, 0.011962723938550366, 0.0122522839944502, 0.014384005850534716, 0.0134980971235454, 0.01360194111757728, 0.013954926534630091, 0.013379777770083708, 0.007362441048173297, 0.009556588464165879, 0.01079771971152586, 0.014874018375042012, 0.010263935636858202, 0.01364317842443058, 0.014256236306100347, 0.012990861155575285, 0.015582263247670728, 0.012766848156771964, 0.015383140683975156, 0.013014562481871873, 0.014326832954314769, 0.016227729989005197, 0.012819871176317195, 0.009256767407321077, 0.01298673854787758, 0.01110171181012428, 0.011387695430325659, 0.01335748158285432, 0.011788963499643307, 0.014730237784164053, 0.012939669518341709, 0.01188002343990131, 0.012531209939653792, 0.014521699017574916, 0.013690680148246448, 0.018118814471950972, 0.0127868517676992, 0.011505644472899595, 0.014178668260666409, 0.013754459755116223, 0.015305807773308441, 0.0149645088640115, 0.016709390702408344, 0.014095777218342138, 0.01465025094246573, 0.01414775846005311, 0.011071743840595023], \"yaxis\": \"y\"}, {\"mode\": \"lines\", \"name\": \"203\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804], \"xaxis\": \"x\", \"y\": [0.00947171188174445, 0.012991327487931926, 0.01209382499541827, 0.014139323739689218, 0.010003861886254812, 0.01336831486824913, 0.012118235798107262, 0.013283858281849662, 0.01234214454389852, 0.013847624058325523, 0.014180118963277644, 0.014456425639748615, 0.013145477129780456, 0.013250048232783656, 0.013743478535462443, 0.014610435071282832, 0.01453297626517767, 0.012203816965565358, 0.0140196110495116, 0.014855880628557718, 0.015957187936981178, 0.01230237011896586, 0.011572894120711849, 0.011996870879044587, 0.013370193157969822, 0.012074747595003306, 0.01414755844501455, 0.0116247453185954, 0.013451546025161241, 0.013651913431551269, 0.011796522399623506, 0.012628665054489804, 0.013016724895829971, 0.013720139483769656, 0.012709564772130122, 0.013528130638064913, 0.014103616062897669, 0.011387589931143229, 0.012497142927714169, 0.014111320125675009, 0.014283725923598056, 0.013191248511109593, 0.015564285507522629, 0.014090658726275613, 0.014183258752547848, 0.012819718497536978, 0.008968490739270469, 0.011959198249141816, 0.012011484974683774, 0.012256008390454638, 0.012186399680239077, 0.01373561907083202, 0.011121867611552651, 0.012054845841684385, 0.010539593036538049, 0.011452694466598324, 0.01351423398884382, 0.013271272218453345, 0.013095192995333798, 0.013133278619900013, 0.011454096310312729, 0.010451545727394256, 0.010924530887434144, 0.009927952534068248, 0.012435446391385895, 0.01338840724576367, 0.013816734035468334, 0.01395278505220438, 0.011718399050274126, 0.010725940768456423, 0.01268819154601268, 0.013697215452254288, 0.01263554111522601, 0.0126035381695066, 0.011313281714728634, 0.008706650105342037, 0.01279630326089748, 0.013248956546513736, 0.014110453645663153, 0.014068050249951149, 0.012513013842952758, 0.010748515617802977, 0.01267268925362902, 0.01446156758474403, 0.013378001343359435, 0.014758343745189438, 0.01589059514319164, 0.011356478981899605, 0.013588955201945105, 0.013353663171316137, 0.014057489106129584, 0.01499474198699635, 0.014908653286557844, 0.01437838610434556, 0.014644503168213575, 0.014293604222942513, 0.011998621692212084, 0.011513164464791259, 0.012113760957830176, 0.012593433757660241, 0.0145141329583475, 0.015340662813956478, 0.013238219976035085, 0.014413990587338122, 0.014867512691895337, 0.01382237925289556, 0.01413308906799652, 0.011870855011410778, 0.012433852173404465, 0.015597677690804791, 0.014631926869174975, 0.0149662126380707, 0.012543675389649548, 0.014792443028472909, 0.01335311162252386, 0.016694932833495192, 0.013344500267420845, 0.014323357712371068, 0.013131623749232614, 0.015001536177296496, 0.014431108384957998, 0.013402663851224102, 0.01468583502869074, 0.013674899395179435, 0.0133045898545245, 0.014360100631503104, 0.014043856134807232, 0.013960441958511323, 0.013905852688993359, 0.010831465726742873, 0.013532008713618702, 0.012854931432776074, 0.014214385123927403, 0.012765917647896191, 0.013671064886492215, 0.014705302840477734, 0.013109437128873271, 0.014165287563863932, 0.014699137239533996, 0.013864105325562452, 0.013789455106166392, 0.012714170535816706, 0.01445991046933444, 0.013312580120311059, 0.014434061260925448, 0.012790883745601864, 0.013442511681325469, 0.01235905034492477, 0.01437700143324533, 0.012867690376315194, 0.013549826861131795, 0.015824574421197824, 0.015089075633185588, 0.01452829271554649, 0.01487475843608604, 0.013344443268891205, 0.015300061405349487, 0.013391218968785677, 0.012154823605060826, 0.010870059681987915, 0.01199289698243513, 0.015651425819531342, 0.01401575498091288, 0.011743401727132988, 0.01276277915563896, 0.012233947421755492, 0.013786265775107812, 0.013897595409273423, 0.014570888497263169, 0.014971095141110759, 0.012998280781580707, 0.013226157235484108, 0.011894694579871313, 0.011438074425445107, 0.010370864240793105, 0.012993499289371655, 0.01393994332066999, 0.012199853336393049, 0.0106447799906743, 0.01349530430360175, 0.011111679211455004, 0.01136894125694954, 0.013249256608501122, 0.01406105901050366, 0.012127043360962769, 0.0133635398419018, 0.01565714778813534, 0.01424147954055131, 0.009894886195911412, 0.01288332170736687, 0.012589561698866613, 0.014202835613727854, 0.012390326445646214, 0.013321179774316443, 0.014009530830634827, 0.012479047369192836, 0.014964226534475947, 0.012411655207386895, 0.012776066169602592, 0.010278801236540977, 0.011934279299572262, 0.011222092875718659, 0.011153494304996812, 0.012155048025478248, 0.0121124347026471, 0.013919515744683334, 0.010798305487902645, 0.013889957828506643, 0.013932224119691429, 0.013277316471666774, 0.013022245746465405, 0.011677606036708787, 0.011777669950016336, 0.012560515170845938, 0.014224429713898272, 0.013149054215174923, 0.012619649627154068, 0.01245590940236362, 0.01442815913978784, 0.012131479531263813, 0.01268782193888838, 0.010424621985495885, 0.013896839019499431, 0.012278197182085209, 0.013324531460703127, 0.01356779208814244, 0.013417397025336623, 0.011124047881622934, 0.011425905242333893, 0.013175361566710227, 0.012065812857778786, 0.01339584522962974, 0.01015728759442091, 0.013149883754407352, 0.012100486567812384, 0.011524417792486587, 0.010656306117983912, 0.014121608058738596, 0.010813335696043236, 0.010468989954893577, 0.011091734600186757, 0.012151327177167388, 0.011006759134000036, 0.015030996877552587, 0.015245032134860222, 0.013441980905566269, 0.01291854204898209, 0.011991536014164849, 0.013432347220188457, 0.013351922896385869, 0.01110567871105235, 0.009717287376922193, 0.012740307773129643, 0.011286372531410429, 0.008875907394431039, 0.013100345116241776, 0.012440032113068034, 0.010202729957583263, 0.012344131491100205, 0.01376759608660088, 0.014759933739432171, 0.012973051696290882, 0.011740017300402241, 0.01188802550678328, 0.010307170523902462, 0.01119064151945047, 0.015516542821969318, 0.013205698598756593, 0.012860477676692931, 0.013006968287932814, 0.010775404742152049, 0.012218468068851492, 0.012990425162156791, 0.014166192003912644, 0.01229327855648812, 0.014294788010792415, 0.014222357349231694, 0.0128230922950881, 0.012616792730082777, 0.013337240045830505, 0.013853903417040564, 0.01062640215149718, 0.01466458867455174, 0.011992953126567505, 0.013799618555134991, 0.01443128787392223, 0.014732529293524674, 0.015016128525214913, 0.013306880822313549, 0.012651265802162448, 0.013992685783453468, 0.014517934355184718, 0.015508330610694088, 0.012837583420774384, 0.013750777544666632, 0.015093804643651709, 0.013989508254662608, 0.013943069871783693, 0.0144819506774456, 0.01500028333069504, 0.013132039336797395, 0.012744875287309028, 0.013986394926197614, 0.013443965353444488, 0.012279882927548674, 0.013197292476033051, 0.012537832733264958, 0.010729585636209764, 0.013690528931941896, 0.012108887182556729, 0.012513378880061434, 0.012931914999021668, 0.015294551564980885, 0.014614141890574259, 0.013463651797140885, 0.011848108034565777, 0.014736245249132304, 0.015334817100903344, 0.013840822435958177, 0.01325817228501228, 0.01250726264994364, 0.013404412942602976, 0.013247630344934824, 0.01555377641978465, 0.015376040932707126, 0.0137658660267946, 0.012868188650114763, 0.012934505247915116, 0.012459857134773378, 0.014735183849945663, 0.013328275617015509, 0.013750791294263863, 0.013328422639199124, 0.01271197857048966, 0.012284758337989062, 0.014659047405742047, 0.0125152030967952, 0.012345519459213495, 0.013956943752961169, 0.015174353518846726, 0.013009293462248344, 0.013969511267902386, 0.01047583136537499, 0.0128063310890947, 0.013814114116842012, 0.014127948051571585, 0.014200743855422688, 0.0122018853569194, 0.01368525921856812, 0.01382020013195643, 0.014884700044348803, 0.013329350664387316, 0.01414196718401294, 0.013283535635711477, 0.013467889070545773, 0.011920019696222743, 0.01332101378749891, 0.01520771976456852, 0.014477554788772138, 0.012720730902348772, 0.012777663055430608, 0.014131868914212191, 0.012331928917812051, 0.01097432954755342, 0.013871354795370588, 0.011975045794916666, 0.012450342611281714, 0.011434302569898202, 0.014949294533230916, 0.012504115387055302, 0.012293662228777357, 0.013360935990106751, 0.014046133437511779, 0.013414978180212641, 0.013743377958579713, 0.013198069345306278, 0.01243722728155453, 0.012865858982611928, 0.011731985865533687, 0.013134826353878077, 0.01345102090330708, 0.012569613176148162, 0.013552536225449495, 0.014052031275235334, 0.01437148046338665, 0.015121906318116203, 0.011336354082880041, 0.011725085612457978, 0.010338864776151268, 0.011734982231875971, 0.01097940689657889, 0.013654071608345888, 0.014448743242333438, 0.01257489595375016, 0.011473990439206076, 0.014121864957267503, 0.012636164704316196, 0.012168266939803841, 0.012369494591760394, 0.0141474869717066, 0.014571097285485637, 0.01544732549512666, 0.012021588570601966, 0.012613196040291806, 0.012546302176314892, 0.014130325005137013, 0.012658735084740169, 0.014338804901656574, 0.013032773476446852, 0.012941303346072484, 0.01221186984235189, 0.012969823701176565, 0.011705811681953136, 0.013848724755119515, 0.014074321543012474, 0.013621738405668198, 0.012842047059795487, 0.01433091836659444, 0.009993736563524105, 0.013677386532596748, 0.012830322940501545, 0.012129414389527077, 0.01435000893122446, 0.014738403509649103, 0.01346239447208552, 0.014441431758845815, 0.015362942368727069, 0.014145405850449029, 0.011513174257727413, 0.01354326083498302, 0.011873698671365776, 0.013154456078939012, 0.013198634733223556, 0.012381447023741715, 0.011058847731054729, 0.011985856113831664, 0.011476196234789007, 0.011462105959819646, 0.012891989608888791, 0.014240708611750882, 0.012509109764514907, 0.0127843176602795, 0.013767915408584895, 0.013508498060557426, 0.013361177312717857, 0.013689067452377078, 0.009989702517307023, 0.014953925578223738, 0.014030666839251922, 0.01326353093552654, 0.013003304276104776, 0.014074060513575253, 0.011593929665484415, 0.012542042809184386, 0.01329060328271123, 0.012467076215364028, 0.012224778002116606, 0.011652043251792707, 0.012916883506668052, 0.013524685800556218, 0.013364088037813457, 0.011791349107150552, 0.012361132242745864, 0.012936376452455005, 0.01270358188858184, 0.01354054108684158, 0.013750920606237492, 0.012183104774803123, 0.011977955146335215, 0.012055790446347064, 0.01411738495098945, 0.01422835495909316, 0.01518580192381993, 0.01325688037873729, 0.012709091031835549, 0.01422773428718325, 0.014062335325406879, 0.01482539118176331, 0.013300434158111793, 0.014222150005779413, 0.014375701691712876, 0.011250770882220084, 0.013594893660658056, 0.012027796897736415, 0.014515452663847656, 0.0132527880512054, 0.013652190502698126, 0.013557544228803646, 0.012521058759189114, 0.01544476048397701, 0.012068392788087938, 0.015175789155162682, 0.013950916712765144, 0.012231540573066609, 0.012289906889778723, 0.012036382806244916, 0.0138021478011055, 0.01446410534155816, 0.013617585004799872, 0.013686564733106407, 0.012542377316237837, 0.011715331429024622, 0.014171779286805302, 0.013213535439659273, 0.013395429441255158, 0.011617044046996051, 0.012821911489080805, 0.012454212977657449, 0.014360881504291415, 0.011735560231139777, 0.015022144153207176, 0.011683692370070959, 0.012224476773947454, 0.012725301725944192, 0.013894787021973744, 0.011850677410296393, 0.009882257827666156, 0.014157783023800745, 0.010179036880710332, 0.01217469888103667, 0.013700733000625706, 0.01279010937439126, 0.012761117353618744, 0.011968194267984974, 0.01270598976651457, 0.013824151629431534, 0.0130404929840512, 0.014452386537917762, 0.01424977152782063, 0.011463361671195984, 0.013059611711499915, 0.013790575804633151, 0.013491487335942395, 0.01289192426259768, 0.014686904564812588, 0.013568274560965774, 0.01311698819386826, 0.013375533977099304, 0.011929541163649762, 0.014029531717402186, 0.014299184709013169, 0.012416190327113646, 0.0136234695483222, 0.012114490346528577, 0.013853752992385577, 0.012478588081414384, 0.012529820401821077, 0.01331755677350373, 0.013459317319199492, 0.014330976528821484, 0.011568531735643392, 0.014701418366506588, 0.014194814383221803, 0.014198294907554388, 0.013575830257720113, 0.01264756223973627, 0.013154454681799193, 0.015387008892775516, 0.014050429371571456, 0.014612353210391432, 0.012462907565037604, 0.013152982475244192, 0.014046877651110559, 0.012536971601556757, 0.013665637746550751, 0.010565356411984816, 0.01338263096629463, 0.01311166519788987, 0.01247782812888472, 0.015502207315246087, 0.013452843371033066, 0.015256971596204075, 0.012640618184600688, 0.015099491627303013, 0.012744177791660904, 0.014942741145565288, 0.016294859826832698, 0.010577549451934277, 0.014006477989766451, 0.012831588236762902, 0.014534847877167328, 0.014018048228024394, 0.013377972227241595, 0.01432485939518596, 0.013702045531506562, 0.014293750415026216, 0.013986306864604729, 0.013340200962350841, 0.014751148447037078, 0.01193710223736975, 0.01342866056200114, 0.014187413060489063, 0.012302786448591646, 0.014240904204215869, 0.013506304333006465, 0.013641241005436422, 0.014353461417236668, 0.014045882097830163, 0.012622037462343713, 0.013254998097777157, 0.01297012979004986, 0.014624616792727854, 0.014948873327006288, 0.013442210197852043, 0.015505150697727456, 0.015881822601400673, 0.013936595373982069, 0.013954080101823205, 0.014543292339104957, 0.016083344844218285, 0.01356957493043927, 0.013843222417449329, 0.012460403464162878, 0.013595229702154885, 0.016432519389568568, 0.014304031862600038, 0.015845903537333197, 0.013741976032803005, 0.014964204874474212, 0.012650294118734622, 0.014956429748952752, 0.013222478680484914, 0.013678563908705471, 0.016647996269268616, 0.015013644406843043, 0.013991980472264643, 0.014030711865523349, 0.012968019351320529, 0.012748260326302784, 0.0140817356772895, 0.013073565568967691, 0.014599193296835504, 0.01198202978015727, 0.016633517057844726, 0.014467345608577266, 0.014251932114805762, 0.011709780089223036, 0.010643796061196262, 0.014659376137767046, 0.013521850400447734, 0.013844976617574395, 0.0139966980315801, 0.012103377501362074, 0.015263814748063778, 0.013924094867036637, 0.015007498884683215, 0.015043991598846215, 0.012958355127417885, 0.01371675844688594, 0.013603870685021944, 0.01218046393453672, 0.013944055158972095, 0.014117129983314863, 0.013318720114714624, 0.014910511861107891, 0.01320334214593568, 0.011591451136533218, 0.012495840496711205, 0.014323690500803113, 0.012285358780214604, 0.011964912261247, 0.013158832395848523, 0.014882213687182028, 0.013032529693442669, 0.010706291094364393, 0.013270304727549542, 0.012779170108743007, 0.012487602320347624, 0.013590184712140196, 0.01216627148364606, 0.012104916578541588, 0.013702704778780087, 0.01454452574053354, 0.013383531133833736, 0.011608398618188943, 0.0124066368336526, 0.013029120873688049, 0.011682368851225628, 0.013428715989382771, 0.01217546618792603, 0.01332803913417268, 0.0119468675016539, 0.013017511009257965, 0.01273856674473806, 0.011736354937068694, 0.013517289991448312, 0.014240553685763001, 0.013910924595775314, 0.014745672351433882, 0.013098763812893956, 0.01357355180151133, 0.011189882550878171, 0.011465053170715488, 0.013749665251310895, 0.013261479034510708, 0.014913490951709553, 0.013726166083641984, 0.012144408640804452, 0.01177343835266454, 0.011893895647701665, 0.014437233887140247, 0.01351287367925214, 0.014823935059633257, 0.01224097308348213, 0.014231984647498174, 0.01193641754510958, 0.01278375571568854, 0.012979027588670387, 0.013052253340277457, 0.010472819502991072, 0.014502621537170447, 0.012761537176004657, 0.010961342403746393, 0.012765707864380641, 0.011237656173134727, 0.01245453045710297, 0.013987471140400538, 0.014050301314504856, 0.012278967465927038, 0.01191197722627392, 0.01265727505741155, 0.013156037450136304, 0.014372927964467304, 0.012999824541193512, 0.013249293447593204, 0.0131653019593901, 0.013795771493676707, 0.015073800542068722, 0.013492494607645771, 0.01491055319228044, 0.011773558280055272, 0.0141821229427815, 0.01368782181826995, 0.01403433354783589, 0.012956510520621974, 0.013743365788772176, 0.012243493786172741, 0.01196082209322381, 0.011779419808702323, 0.013627058153113854, 0.01481097084997751, 0.013153643531836713, 0.013037149147975784, 0.01265339156111545, 0.01330082221868278, 0.012117712994549423, 0.011741910616390691, 0.013459816470762144, 0.0126349862884518, 0.0138516385420522, 0.014175628543949074, 0.013264342884593563, 0.012525686581860313, 0.015040012753268744, 0.01245638060344031, 0.014774350519986962, 0.013535455119511736, 0.013721619450229993, 0.014406041475322713, 0.014119896917344485, 0.012576127776640971, 0.013071982774145357, 0.01179242143724682, 0.014908244667195788, 0.013562833320580142, 0.012582165899802041, 0.012497538699113173, 0.014300339908182181, 0.01450804062958647, 0.014175895897303803, 0.01425484845997473, 0.01250326369714608, 0.013136806482742805, 0.015150906955990627, 0.01045807133251153, 0.013577714019377128, 0.014945463897044162, 0.012927899835498705, 0.012793070926067443, 0.011732107439961992, 0.012298444596902904, 0.012808705192890738, 0.01263567030461401, 0.014264161230963596, 0.013387653018972006, 0.01420998747722781, 0.0140991280839262, 0.015413293846374637, 0.012584412273062031, 0.01347533614339426, 0.014752546698108578, 0.013554850180978358, 0.015332422872025646, 0.014658382739868862, 0.013437910210782713, 0.013893886514390727, 0.013293827226982472, 0.013957624454925805, 0.012042211244838234, 0.012914856989836273, 0.013834390031006623, 0.01366641113951628], \"yaxis\": \"y\"}],\n",
       "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Vol Mean Over pid by day-by-day\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0]}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0]}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('8a65fd81-2385-4969-9966-b60508bba93b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                \n",
       "            </script>\n",
       "        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "for pid_i in dep_pid['pid'].unique()[0:2]:\n",
    "    dep_pid_df = dep_pid[dep_pid['pid']==pid_i]\n",
    "    fig.add_trace(go.Scatter(x=dep_pid_df[\"day\"], \n",
    "                             y=dep_pid_df[\"median_vol\"],\n",
    "                             #showlegend=Ture,\n",
    "                             mode=\"lines\",\n",
    "                             name=str(pid_i),\n",
    "                             #marker=dict(color=\"mediumseagreen\")\n",
    "                             ),\n",
    "\n",
    "                  row=1,col=1         \n",
    "                  )\n",
    "    \n",
    "fig.update_layout(title_text=\"Vol Mean Over pid by day-by-day\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542,
     "output_embedded_package_id": "1b5TYVxPdxdScIAg4E8dN0bU-mn3ALbGJ"
    },
    "executionInfo": {
     "elapsed": 15812,
     "status": "ok",
     "timestamp": 1613317515013,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "j8l2syQQW9F7",
    "outputId": "05c59216-e425-41c8-8fec-abc1b30431d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "for pid_i in dep_pid['pid'].unique():\n",
    "    dep_pid_df = dep_pid[dep_pid['pid']==pid_i]\n",
    "    fig.add_trace(go.Scatter(x=dep_pid_df[\"day\"], \n",
    "                             y=dep_pid_df[\"sum_ret\"],\n",
    "                             #showlegend=Ture,\n",
    "                             mode=\"lines\",\n",
    "                             name=str(pid_i),\n",
    "                             #marker=dict(color=\"mediumseagreen\")\n",
    "                             ),\n",
    "\n",
    "                  row=1,col=1         \n",
    "                  )\n",
    "    \n",
    "fig.update_layout(title_text=\"Sum ret Mean Over pid day-by-day\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iufg2O3muyg"
   },
   "source": [
    "### Task 4 : Linear regression modelling \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoACulGTnIkQ"
   },
   "source": [
    "We first run a linear regression on our train_dataset in order to get a new error variable that we will try to minimize in a second time with XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Q_aLRAMoIKU"
   },
   "source": [
    "#### 3.1 Predict with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6397,
     "status": "ok",
     "timestamp": 1613317552391,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "2LZgfgstnbxN",
    "outputId": "dd81fc20-6991-4764-f27c-581f144ae797"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 140,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regrLin = LinearRegression()\n",
    "regrLin.fit(train_X_, train_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1613317554617,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "ufix-Rkln52F"
   },
   "outputs": [],
   "source": [
    "test_X_['predict'] = regrLin.predict(test_X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1613317556212,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "A9y9trGpn-Ai",
    "outputId": "46b854ae-03c1-4791-9fa0-045623203f40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6239513625421437"
      ]
     },
     "execution_count": 142,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_squared_error(test_y_, test_X_['predict']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFGIU1lfoCV3"
   },
   "source": [
    "#### 3.2 Create error variable from linear prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1613317558820,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "JHnh0i76DegA"
   },
   "outputs": [],
   "source": [
    "test_y_=pd.DataFrame(test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1613317562194,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "Y549SMiWDegA"
   },
   "outputs": [],
   "source": [
    "test_X_['error']=test_y_[\"target\"] - test_X_['predict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spi7aOlDosCG"
   },
   "source": [
    "### Task 5 : XGboost for error modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fizeLcQBovfW"
   },
   "source": [
    "#### 5.1 Prepare data for modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKvXygrhpddp"
   },
   "source": [
    "Categorize pid in XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "executionInfo": {
     "elapsed": 1524,
     "status": "ok",
     "timestamp": 1613317570211,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "rX4N22_nDegA"
   },
   "outputs": [],
   "source": [
    "test_X_=test_X_.join(train_df[train_df.columns[134:1034]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3HtVuInpfXz"
   },
   "source": [
    "Store days for time series split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1613317572487,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "A5fmIL78DegB"
   },
   "outputs": [],
   "source": [
    "date_series = test_X_['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1613317573579,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "XBn8OA9rDegC"
   },
   "outputs": [],
   "source": [
    "min_date = date_series.min()\n",
    "max_date = date_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1613317574714,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "9DvI83CkDegC",
    "outputId": "cb978f3c-8c31-465c-bc35-f950327acf13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 149,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = list(range(min_date, max_date + 1))\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ASCUy_kprTx"
   },
   "source": [
    "Create new train and test dataset from X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "executionInfo": {
     "elapsed": 1840,
     "status": "ok",
     "timestamp": 1613317577820,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "TZrbn9p2YNl7"
   },
   "outputs": [],
   "source": [
    "X_error_train = test_X_[test_X_['day']<=600].drop(['error'], axis = 1)\n",
    "y_error_train = test_X_[test_X_['day']<=600]['error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "executionInfo": {
     "elapsed": 1009,
     "status": "ok",
     "timestamp": 1613317578735,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "McjejeewZ-RS"
   },
   "outputs": [],
   "source": [
    "X_error_test = test_X_[test_X_['day']>600].drop(['error'], axis = 1)\n",
    "y_error_test = test_X_[test_X_['day']>600]['error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6z0scE2qDju"
   },
   "source": [
    "#### 5.2 Xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1613317585302,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "I3ZSLE79SEyo"
   },
   "outputs": [],
   "source": [
    "def learning_rate_010_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_010_decay_power_0995(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_005_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.05\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1613317587280,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "2KCm9_KVoE5N"
   },
   "outputs": [],
   "source": [
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'neg_mean_squared_error', \n",
    "            \"eval_set\" : [(X_error_test,y_error_test)],\n",
    "            'eval_names': ['valid'],\n",
    "            'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature':'auto'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1613317588463,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "SIYhlUHmoUou"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "executionInfo": {
     "elapsed": 663,
     "status": "ok",
     "timestamp": 1613317591426,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "SYmRt8wEqaWr"
   },
   "outputs": [],
   "source": [
    "model = LGBMRegressor( random_state=314, n_estimators=1000, device='gpu')\n",
    "tscv = TimeSeriesSplit(n_splits=2)\n",
    "n_HP_points_to_test = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJiP30TxqriS"
   },
   "source": [
    "#### 5.3 Gridsearching for best XGboost parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1613317597611,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "eLCA4UK7q1-D"
   },
   "outputs": [],
   "source": [
    "gs = RandomizedSearchCV(\n",
    "    estimator=model, \n",
    "    param_distributions=param_test, \n",
    "    n_iter= n_HP_points_to_test,\n",
    "    scoring= 'neg_mean_squared_error',\n",
    "    cv= tscv,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEl99_mSq3VB"
   },
   "outputs": [],
   "source": [
    "#gs.fit(X_error_train, y_error_train, **fit_params)\n",
    "#print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZD6Cl0e7q6SW"
   },
   "source": [
    "We got following parameters from our GridSearch : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1613317599768,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "-Hh13Xxkq5fO"
   },
   "outputs": [],
   "source": [
    "opt_parameters = {'colsample_bytree': 0.7916380440478592, \n",
    "                  'min_child_samples': 211, \n",
    "                  'min_child_weight': 1, \n",
    "                  'num_leaves': 45, \n",
    "                  'reg_alpha': 2, \n",
    "                  'reg_lambda': 20, \n",
    "                  'subsample': 0.5211522776637936}\n",
    "                  \n",
    "model_final = LGBMRegressor(**model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1613317601682,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "n82Quv5RrBTI",
    "outputId": "b67ebef1-e10e-4921-eabd-4cfeeed496c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.7916380440478592, device='gpu',\n",
       "              min_child_samples=211, min_child_weight=1, n_estimators=1000,\n",
       "              num_leaves=45, random_state=314, reg_alpha=2, reg_lambda=20,\n",
       "              subsample=0.5211522776637936)"
      ]
     },
     "execution_count": 158,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.set_params(**opt_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdTYhMcfrDGo"
   },
   "source": [
    "#### 5.4 Fit error Xgboost model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B818vMERrLBd"
   },
   "source": [
    "We fit the xgboost model with the entire train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCcvoMEZOazf"
   },
   "outputs": [],
   "source": [
    "!rm -r /content/LightGBM\n",
    "!git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "%cd /content/LightGBM\n",
    "!mkdir build\n",
    "!cmake -DUSE_GPU=1 #avoid ..\n",
    "!make -j$(nproc)\n",
    "!sudo apt-get -y install python-pip\n",
    "!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
    "%cd /content/LightGBM/python-package\n",
    "!sudo python setup.py install --precompile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 6692,
     "status": "error",
     "timestamp": 1613317614458,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "6HgrzjH5rIFc",
    "outputId": "099874e2-b28a-4af2-ca3a-c9f3f09d48fa"
   },
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-3b519360f932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_error_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_error_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    683\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                                        callbacks=callbacks)\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                 ctypes.byref(self.handle)))\n\u001b[0m\u001b[1;32m   1555\u001b[0m             \u001b[0;31m# save reference to data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \"\"\"\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLightGBMError\u001b[0m: GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1"
     ]
    }
   ],
   "source": [
    "model_final.fit(X_error_train, y_error_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeaSUdf6rJ8I"
   },
   "outputs": [],
   "source": [
    "X_error_train['predict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQI2qmNstDdJ"
   },
   "source": [
    "### Task 6 :  Benchmark prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfSO6Q9QtTkK"
   },
   "source": [
    "#### 6.1 Predict the target with linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmlObBCxtZcY"
   },
   "source": [
    "We refit the linear model with the entire train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ge_UUPR-dR0G"
   },
   "outputs": [],
   "source": [
    "regrLin = LinearRegression()\n",
    "regrLin.fit(x_train.iloc[:,1:134], y_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzBhsbA2tR8v"
   },
   "outputs": [],
   "source": [
    "x_test['predict'] = regrLin.predict(x_test.iloc[:,1:134])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8yyEs2mtoO8"
   },
   "source": [
    "#### 6.2 Predict the error with XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Z3xxX40u95P"
   },
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6K5WV9dQhvte"
   },
   "outputs": [],
   "source": [
    "x_test['error'] = model_final.predict(x_test.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zXWyHE4tsG7"
   },
   "source": [
    "#### 6.3 Calculate target without error predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnPmtERmh1pE"
   },
   "outputs": [],
   "source": [
    "x_test['target'] = x_test['predict'] + x_test['error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhXnGuaBt3xJ"
   },
   "source": [
    "#### 6.4 Export prediction to csv for challenge provider submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIQFqMY_tR85"
   },
   "outputs": [],
   "source": [
    "predictions = x_test[['ID', 'target']]\n",
    "predictions.columns = ['ID', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRLM7G8ztR86"
   },
   "outputs": [],
   "source": [
    "predictions.to_csv('predictions_02.02.17.15.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p4yZq-tqwOK"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCRXN9rOflmb"
   },
   "source": [
    "## Step 2 : Advanced Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPAq97y20Ah6"
   },
   "source": [
    "### Task 1 : Loading libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "executionInfo": {
     "elapsed": 1697,
     "status": "ok",
     "timestamp": 1613317837152,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "MMyYorza0Kmn"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1613317838196,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "lk4IVzLYz3x5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "import delayed\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score \n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sys\n",
    "from numpy import mean\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1613317846770,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "UBBDDq3afJY6"
   },
   "outputs": [],
   "source": [
    "data_dir  = \"/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/dataset\"\n",
    "data_list = glob.glob(os.path.join(data_dir, '**.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "executionInfo": {
     "elapsed": 24851,
     "status": "ok",
     "timestamp": 1613317872427,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "EULmRWX6fSN8"
   },
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(\"%s/y_train.csv\" % data_dir, sep=\",\")\n",
    "x_train = pd.read_csv(\"%s/x_train.csv\" % data_dir, sep=\",\")\n",
    "x_test=pd.read_csv(\"%s/x_test.csv\" % data_dir, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-iJlMgn0iGO"
   },
   "source": [
    "### Task 2 : Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8yQGuvI1V8Z"
   },
   "source": [
    "#### 2.1 Identify missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1613317880633,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "uJAlBVgjfsTl"
   },
   "outputs": [],
   "source": [
    "all_features = list(x_train.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 955,
     "status": "ok",
     "timestamp": 1613317882145,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "6sDEY-HrgnyB",
    "outputId": "a5ee0cc7-ec37-4b04-fa61-af982dfc70bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abs_ret0      4996\n",
       "abs_ret1     27779\n",
       "abs_ret2     26892\n",
       "abs_ret3     21945\n",
       "abs_ret4     23514\n",
       "             ...  \n",
       "rel_vol56    32030\n",
       "rel_vol57    31882\n",
       "rel_vol58    31479\n",
       "rel_vol59    30558\n",
       "rel_vol60    29959\n",
       "Length: 122, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_with_missing_values = x_train.columns[x_train.isnull().any()]\n",
    "x_train[columns_with_missing_values].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1613317884379,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "kMSLZlXyhtEi"
   },
   "outputs": [],
   "source": [
    "return_cols = [c for c in x_train.columns if c.startswith(\"abs_ret\")]\n",
    "volume_cols = [c for c in x_train.columns if c.startswith(\"rel_vol\")]\n",
    "target_exp_col = [\"target_exp\"]\n",
    "date_col = [\"day\"]\n",
    "prod_id_col=[\"pid\"]\n",
    "other_cols = [\"LS\" , \"NLV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1613317885680,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "-x5mEGZ0isvM"
   },
   "outputs": [],
   "source": [
    "#sum of nans\n",
    "for df in [x_train,x_test]:\n",
    "    df[\"return_nan\"] = df[return_cols].isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18OynhQ21aCw"
   },
   "source": [
    "#### 2.2 Replace nanas by interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "executionInfo": {
     "elapsed": 135856,
     "status": "ok",
     "timestamp": 1613318024067,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "gsVsCTybjlU5"
   },
   "outputs": [],
   "source": [
    "#replace nans by interpolation\n",
    "for df in [x_train,x_test]:\n",
    "    for x in [return_cols,volume_cols]:\n",
    "        df[x] = df[x].interpolate(axis=1, limit_direction=\"both\", inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1613318026989,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "tgWmIhMmXMEZ"
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohRJjzjnlGuo"
   },
   "source": [
    "### Task 3 : Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n8ggmEQ2AOO"
   },
   "source": [
    "#### 3.1 Merge and transform target to exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "executionInfo": {
     "elapsed": 1353,
     "status": "ok",
     "timestamp": 1613318031446,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "iXlJHUrpL99b"
   },
   "outputs": [],
   "source": [
    "x_train = y_train.merge(x_train, on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1613318031447,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "dicK5DVVRQRB"
   },
   "outputs": [],
   "source": [
    "x_train[\"is_train\"] = True\n",
    "x_test[\"is_train\"] = False\n",
    "x_test[\"target\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "executionInfo": {
     "elapsed": 1642,
     "status": "ok",
     "timestamp": 1613318033399,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "SPLMgciVL8bZ"
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat([x_train, x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "executionInfo": {
     "elapsed": 1427,
     "status": "ok",
     "timestamp": 1613318033742,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "L7qNAdw4PllT"
   },
   "outputs": [],
   "source": [
    "all_data['target_exp'] = all_data['target'].apply(lambda x : math.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew0ZSUap2e9I"
   },
   "source": [
    "#### 3.2 Add median, std, avg for each period of volume and ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1613318036288,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "X9vUyvU-OeOK"
   },
   "outputs": [],
   "source": [
    "def get_stats_groupby(all_data, groupby_cols):\n",
    "    for groupby_obj in groupby_cols:\n",
    "        groupby_col = groupby_obj[\"id\"]\n",
    "        #print(groupby_col)\n",
    "        cols = groupby_obj[\"cols\"]\n",
    "        group_by = all_data.groupby([groupby_col])\n",
    "        #print(group_by)\n",
    "        data_arr = []\n",
    "        data_arr.append({\"i\": \"avg\", \"d\": group_by[cols].mean()})\n",
    "        #data_arr.append({\"i\": \"skew\", \"d\": group_by[cols].skew()})\n",
    "        #data_arr.append({\"i\": \"kurt\", \"d\": group_by[cols].apply(pd.DataFrame.kurt)})\n",
    "        data_arr.append({\"i\": \"std\", \"d\": group_by[cols].std()})\n",
    "        data_arr.append({\"i\": \"median\", \"d\": group_by[cols].median()})\n",
    "        #data_arr.append({\"i\": \"nan\", \"d\": all_data.isnull().groupby(all_data[groupby_col])[cols].sum()})\n",
    "        #print(data_arr)\n",
    "        all_data_stats = all_data.copy()\n",
    "        all_data_stats.set_index([groupby_col], inplace=True)\n",
    "        for obj_data in data_arr:\n",
    "            names = ['%s_%s_%s' % (obj_data[\"i\"], groupby_col, col) for col in cols]            \n",
    "            all_data_stats[names] = (obj_data[\"d\"]).astype(\"float32\")\n",
    "        all_data_stats.reset_index(inplace=True)\n",
    "    return all_data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1613318038006,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "cRVcU5l9PQWs"
   },
   "outputs": [],
   "source": [
    "#group by day to get more features\n",
    "calculation_group_by =[\n",
    "    {\"id\":\"day\",\n",
    "     \"cols\": volume_cols + return_cols,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "executionInfo": {
     "elapsed": 10445,
     "status": "ok",
     "timestamp": 1613318048868,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "QXlo4oHXPhhV"
   },
   "outputs": [],
   "source": [
    "all_data_stats = get_stats_groupby(all_data, calculation_group_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7F1NfZO232s"
   },
   "source": [
    "#### 3.3 Add median, std, avg for each period of target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "171xWYb-3Fst"
   },
   "source": [
    "In order to identify later the \"special days\" of high fixing volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "executionInfo": {
     "elapsed": 6099,
     "status": "ok",
     "timestamp": 1613318048869,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "YlTVuugkRmvw"
   },
   "outputs": [],
   "source": [
    "def get_target_groupby(all_data, groupby_cols):\n",
    "    for groupby_obj in groupby_cols:\n",
    "        groupby_col = groupby_obj[\"id\"]\n",
    "        #print(groupby_col)\n",
    "        cols = groupby_obj[\"cols\"]\n",
    "        group_by = all_data.groupby([groupby_col])\n",
    "        #print(group_by)\n",
    "        data_arr = []\n",
    "        data_arr.append({\"i\": \"avg\", \"d\": group_by[cols].mean()})\n",
    "        #data_arr.append({\"i\": \"skew\", \"d\": group_by[cols].skew()})\n",
    "        #data_arr.append({\"i\": \"kurt\", \"d\": group_by[cols].apply(pd.DataFrame.kurt)})\n",
    "        data_arr.append({\"i\": \"std\", \"d\": group_by[cols].std()})\n",
    "        data_arr.append({\"i\": \"median\", \"d\": group_by[cols].median()})\n",
    "        data_arr.append({\"i\": \"sum\", \"d\": group_by[cols].sum()})\n",
    "        #data_arr.append({\"i\": \"min\", \"d\": group_by[cols].min()})\n",
    "        #data_arr.append({\"i\": \"max\", \"d\": group_by[cols].max()})\n",
    "        #data_arr.append({\"i\": \"nan\", \"d\": all_data.isnull().groupby(all_data[groupby_col])[cols].sum()})\n",
    "        #print(data_arr)\n",
    "        all_data_stats = all_data.copy()\n",
    "        all_data_stats.set_index([groupby_col], inplace=True)\n",
    "        for obj_data in data_arr:\n",
    "            names = ['%s_%s_%s' % (obj_data[\"i\"], groupby_col, col) for col in cols]            \n",
    "            all_data_stats[names] = (obj_data[\"d\"]).astype(\"float32\")\n",
    "        all_data_stats.reset_index(inplace=True)\n",
    "    return all_data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1613318049468,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "TmGUzH7xP0zV"
   },
   "outputs": [],
   "source": [
    "target_group_by =[\n",
    "    {\"id\":\"day\",\n",
    "     \"cols\": [\"target_exp\"],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "executionInfo": {
     "elapsed": 2733,
     "status": "ok",
     "timestamp": 1613318051608,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "qC-2EpRkZOrh"
   },
   "outputs": [],
   "source": [
    "all_data_stats = get_target_groupby(all_data_stats, target_group_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrGElDDK3acZ"
   },
   "source": [
    "#### 3.4 Add basic feature for each day and each pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "executionInfo": {
     "elapsed": 4073,
     "status": "ok",
     "timestamp": 1613318055684,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "woKfeawSGwq9"
   },
   "outputs": [],
   "source": [
    "all_data_stats['min_ret']    = np.min(all_data_stats.iloc[:,4:64], axis=1)\n",
    "all_data_stats['max_ret']    = np.max(all_data_stats.iloc[:,4:64], axis=1)\n",
    "all_data_stats['std_ret']    = np.std(all_data_stats.iloc[:,4:64], axis=1)\n",
    "all_data_stats['median_ret'] = np.median(all_data_stats.iloc[:,4:64], axis=1)\n",
    "all_data_stats['sum_ret']    = np.sum(all_data_stats.iloc[:,4:64], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "executionInfo": {
     "elapsed": 7815,
     "status": "ok",
     "timestamp": 1613318059429,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "M9f5PrRVIR3R"
   },
   "outputs": [],
   "source": [
    "all_data_stats['min_vol']    = np.min(all_data_stats.iloc[:,65:126], axis=1)\n",
    "all_data_stats['max_vol']    = np.max(all_data_stats.iloc[:,65:126], axis=1)\n",
    "all_data_stats['std_vol']    = np.std(all_data_stats.iloc[:,65:126], axis=1)\n",
    "all_data_stats['median_vol'] = np.median(all_data_stats.iloc[:,65:126], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOOBSmVT3r6B"
   },
   "source": [
    "#### 3.5 Add median by day of sum ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1613318059430,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "23LrS41hvRMr"
   },
   "outputs": [],
   "source": [
    "def get_groupby_med(all_data, groupby_cols):\n",
    "    for groupby_obj in groupby_cols:\n",
    "        groupby_col = groupby_obj[\"id\"]\n",
    "        #print(groupby_col)\n",
    "        cols = groupby_obj[\"cols\"]\n",
    "        group_by = all_data.groupby([groupby_col])\n",
    "        #print(group_by)\n",
    "        data_arr = []\n",
    "        data_arr.append({\"i\": \"median\", \"d\": group_by[cols].median()})\n",
    "        #data_arr.append({\"i\": \"nan\", \"d\": all_data.isnull().groupby(all_data[groupby_col])[cols].sum()})\n",
    "        #print(data_arr)\n",
    "        all_data_stats = all_data.copy()\n",
    "        all_data_stats.set_index([groupby_col], inplace=True)\n",
    "        for obj_data in data_arr:\n",
    "            names = ['%s_%s_%s' % (obj_data[\"i\"], groupby_col, col) for col in cols]            \n",
    "            all_data_stats[names] = (obj_data[\"d\"]).astype(\"float32\")\n",
    "        all_data_stats.reset_index(inplace=True)\n",
    "    return all_data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "executionInfo": {
     "elapsed": 3291,
     "status": "ok",
     "timestamp": 1613318059431,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "g1rR-h-kvRMu"
   },
   "outputs": [],
   "source": [
    "dic_med =[\n",
    "    {\"id\":\"day\",\n",
    "     \"cols\": [\"sum_ret\"],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "executionInfo": {
     "elapsed": 4618,
     "status": "ok",
     "timestamp": 1613318061501,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "YANE6Sw4vvLK"
   },
   "outputs": [],
   "source": [
    "all_data_stats = get_groupby_med(all_data_stats, dic_med)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xd4jBHYyJ_C9"
   },
   "source": [
    "#### 3.6 Add median sum ret before\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTxmyExr4RjM"
   },
   "source": [
    "Before : getting daily variation of ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "executionInfo": {
     "elapsed": 1869,
     "status": "ok",
     "timestamp": 1613318063031,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "aqyDqeMdISLc"
   },
   "outputs": [],
   "source": [
    "target_analysis = all_data_stats[['day','median_day_sum_ret']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "executionInfo": {
     "elapsed": 1227,
     "status": "ok",
     "timestamp": 1613318063032,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "Hw2CZ7ehdi4H"
   },
   "outputs": [],
   "source": [
    "target_analysis=target_analysis.sort_values(by='day').groupby(by='day').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1613318063233,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "WOcHnBemR3Ry",
    "outputId": "cc615fec-adf2-40ee-a466-7f45aa5dfe59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_analysis['median_day_sum_ret_before'] = None\n",
    "for i in target_analysis.index: \n",
    "  if i == 0:\n",
    "    target_analysis['median_day_sum_ret_before'][i] = 0\n",
    "  else:\n",
    "    target_analysis['median_day_sum_ret_before'][i] = (target_analysis['median_day_sum_ret'][i]-target_analysis['median_day_sum_ret'][j])/target_analysis['median_day_sum_ret'][j]\n",
    "  j = i \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1613318066477,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "XeUHW-44fzwF"
   },
   "outputs": [],
   "source": [
    "target_analysis['day']=target_analysis.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1613318067025,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "U0UsGBqRf8Mo"
   },
   "outputs": [],
   "source": [
    "target_analysis.index.name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1613318067852,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "zzupOhf6klCz",
    "outputId": "9cb199dc-b957-42d0-90f6-5d272c8d8ce1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>median_day_sum_ret</th>\n",
       "      <th>median_day_sum_ret_before</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.120654</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.019131</td>\n",
       "      <td>-0.0905932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.902086</td>\n",
       "      <td>-0.114848</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.168097</td>\n",
       "      <td>0.294885</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.988616</td>\n",
       "      <td>-0.153652</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>13.301187</td>\n",
       "      <td>0.0125656</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>17.758280</td>\n",
       "      <td>0.33509</td>\n",
       "      <td>1148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>12.368063</td>\n",
       "      <td>-0.303533</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>13.051916</td>\n",
       "      <td>0.0552919</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>10.988750</td>\n",
       "      <td>-0.158074</td>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      median_day_sum_ret median_day_sum_ret_before   day\n",
       "                                                        \n",
       "0               1.120654                         0     0\n",
       "1               1.019131                -0.0905932     1\n",
       "2               0.902086                 -0.114848     2\n",
       "3               1.168097                  0.294885     3\n",
       "4               0.988616                 -0.153652     4\n",
       "...                  ...                       ...   ...\n",
       "1147           13.301187                 0.0125656  1147\n",
       "1148           17.758280                   0.33509  1148\n",
       "1149           12.368063                 -0.303533  1149\n",
       "1150           13.051916                 0.0552919  1150\n",
       "1151           10.988750                 -0.158074  1151\n",
       "\n",
       "[1152 rows x 3 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1613318070528,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "n0PcR67jgnDt",
    "outputId": "1d02e05c-1b51-4ae9-cc83-015cbf6cb7ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>median_day_sum_ret_before</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1.21533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0.590906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>0.59866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>5.11149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>0.609638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>148</td>\n",
       "      <td>0.488069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>309</td>\n",
       "      <td>1.07988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>399</td>\n",
       "      <td>0.629169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>519</td>\n",
       "      <td>0.599741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>615</td>\n",
       "      <td>0.84748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>776</td>\n",
       "      <td>0.567795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>926</td>\n",
       "      <td>0.71753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     day median_day_sum_ret_before\n",
       "                                  \n",
       "23    23                   1.21533\n",
       "53    53                  0.590906\n",
       "64    64                   0.59866\n",
       "81    81                   5.11149\n",
       "126  126                  0.609638\n",
       "148  148                  0.488069\n",
       "309  309                   1.07988\n",
       "399  399                  0.629169\n",
       "519  519                  0.599741\n",
       "615  615                   0.84748\n",
       "776  776                  0.567795\n",
       "926  926                   0.71753"
      ]
     },
     "execution_count": 194,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target_analysis.sort_values(by = 'median_day_sum_ret_before', ascending=False)[['day','median_day_sum_ret_before']].head(n =12)).sort_values(by = 'day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ylTcogq4xku"
   },
   "source": [
    "#### 3.7 Add clustering of sum ret before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSmnQoU4n-zA"
   },
   "source": [
    "K_means according to the sign of 'median_day_sum_ret_before'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1613318077072,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "gb5BEkFikCk4"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(target_analysis['median_day_sum_ret_before'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1613318077739,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "GDyMjB_7kQQC"
   },
   "outputs": [],
   "source": [
    "target_analysis['kmeans_cluster_median_day_sum_ret_before'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1613318078415,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "oOvtKRL1mkTb",
    "outputId": "dae929fe-eb60-41fd-dbe8-4fcc143e335d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>median_day_sum_ret</th>\n",
       "      <th>median_day_sum_ret_before</th>\n",
       "      <th>day</th>\n",
       "      <th>kmeans_cluster_median_day_sum_ret_before</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.120654</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.019131</td>\n",
       "      <td>-0.0905932</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.902086</td>\n",
       "      <td>-0.114848</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.168097</td>\n",
       "      <td>0.294885</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.988616</td>\n",
       "      <td>-0.153652</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>13.301187</td>\n",
       "      <td>0.0125656</td>\n",
       "      <td>1147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>17.758280</td>\n",
       "      <td>0.33509</td>\n",
       "      <td>1148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>12.368063</td>\n",
       "      <td>-0.303533</td>\n",
       "      <td>1149</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>13.051916</td>\n",
       "      <td>0.0552919</td>\n",
       "      <td>1150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>10.988750</td>\n",
       "      <td>-0.158074</td>\n",
       "      <td>1151</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      median_day_sum_ret  ... kmeans_cluster_median_day_sum_ret_before\n",
       "                          ...                                         \n",
       "0               1.120654  ...                                        0\n",
       "1               1.019131  ...                                        4\n",
       "2               0.902086  ...                                        4\n",
       "3               1.168097  ...                                        1\n",
       "4               0.988616  ...                                        4\n",
       "...                  ...  ...                                      ...\n",
       "1147           13.301187  ...                                        0\n",
       "1148           17.758280  ...                                        1\n",
       "1149           12.368063  ...                                        4\n",
       "1150           13.051916  ...                                        0\n",
       "1151           10.988750  ...                                        4\n",
       "\n",
       "[1152 rows x 4 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXLvAHNNoHrH"
   },
   "source": [
    "K_means absolute value of 'median_day_sum_ret_before'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1613318081074,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "7dwsaJf8oPtf"
   },
   "outputs": [],
   "source": [
    "kmeans_abs = KMeans(n_clusters=3, random_state=0).fit(abs(target_analysis['median_day_sum_ret_before'].values).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1613318081942,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "6l5QXyZqoWyv"
   },
   "outputs": [],
   "source": [
    "target_analysis['abs_kmeans_cluster_median_day_sum_ret_before'] = kmeans_abs.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1613318082690,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "yVw2MuqKoZP7",
    "outputId": "59eb390f-4fdf-4540-aed1-3d05dcebbc95"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>median_day_sum_ret</th>\n",
       "      <th>median_day_sum_ret_before</th>\n",
       "      <th>day</th>\n",
       "      <th>kmeans_cluster_median_day_sum_ret_before</th>\n",
       "      <th>abs_kmeans_cluster_median_day_sum_ret_before</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.120654</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.019131</td>\n",
       "      <td>-0.0905932</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.902086</td>\n",
       "      <td>-0.114848</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.168097</td>\n",
       "      <td>0.294885</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.988616</td>\n",
       "      <td>-0.153652</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>13.301187</td>\n",
       "      <td>0.0125656</td>\n",
       "      <td>1147</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>17.758280</td>\n",
       "      <td>0.33509</td>\n",
       "      <td>1148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>12.368063</td>\n",
       "      <td>-0.303533</td>\n",
       "      <td>1149</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>13.051916</td>\n",
       "      <td>0.0552919</td>\n",
       "      <td>1150</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>10.988750</td>\n",
       "      <td>-0.158074</td>\n",
       "      <td>1151</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      median_day_sum_ret  ... abs_kmeans_cluster_median_day_sum_ret_before\n",
       "                          ...                                             \n",
       "0               1.120654  ...                                            2\n",
       "1               1.019131  ...                                            2\n",
       "2               0.902086  ...                                            2\n",
       "3               1.168097  ...                                            0\n",
       "4               0.988616  ...                                            2\n",
       "...                  ...  ...                                          ...\n",
       "1147           13.301187  ...                                            2\n",
       "1148           17.758280  ...                                            0\n",
       "1149           12.368063  ...                                            0\n",
       "1150           13.051916  ...                                            2\n",
       "1151           10.988750  ...                                            2\n",
       "\n",
       "[1152 rows x 5 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "executionInfo": {
     "elapsed": 8591,
     "status": "ok",
     "timestamp": 1613318093422,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "KkYgYlTBn28t"
   },
   "outputs": [],
   "source": [
    "all_data_stats = all_data_stats.merge(target_analysis[['day','median_day_sum_ret_before','kmeans_cluster_median_day_sum_ret_before','abs_kmeans_cluster_median_day_sum_ret_before']], how=\"inner\",on=\"day\")\n",
    "del target_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GG-S7bZN7rNJ"
   },
   "source": [
    "### Task 4 : Machine learning for feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPzwO5XRA0J1"
   },
   "source": [
    "#### 4.1 Create train, test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "executionInfo": {
     "elapsed": 3334,
     "status": "ok",
     "timestamp": 1613318097548,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "QbFxIC4H_ZPs"
   },
   "outputs": [],
   "source": [
    "train_dataset = all_data_stats[all_data_stats['day']<805]\n",
    "test_dataset = all_data_stats[all_data_stats['day']>=805]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGP2soPGA-mN"
   },
   "source": [
    "#### 4.2 Create target dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1613318098451,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "1gluMOJ97g-R",
    "outputId": "091c7c56-1ecf-4f19-e48d-3319bd6e2d2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset['median_target_dummy'] = (train_dataset['median_day_target_exp'] >0.52).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1613318100838,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "H7gqu_tr7g-T"
   },
   "outputs": [],
   "source": [
    "return_cols = [c for c in train_dataset.columns if c.startswith(\"abs_ret\")]\n",
    "volume_cols = [c for c in train_dataset.columns if c.startswith(\"rel_vol\")]\n",
    "all_cols = [c for c in train_dataset.columns if not (c.endswith(\"day_target_exp\") or (c =='median_day_target_dummy')  or (c == 'target') or (c== 'target_exp') or (c=='is_train'))]\n",
    "date_col = [\"day\"]\n",
    "prod_id_col=[\"pid\"]\n",
    "other_cols = [\"LS\" , \"NLV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1613318102807,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "Hd8Wl7PxooOI"
   },
   "outputs": [],
   "source": [
    "test_dataset= test_dataset[test_dataset.columns & all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "executionInfo": {
     "elapsed": 1593,
     "status": "ok",
     "timestamp": 1613318104388,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "kQ26YkAX7g-W"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[train_dataset.columns & all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1613318105242,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "s6ZaqRabDK6E"
   },
   "outputs": [],
   "source": [
    "train_dataset['pid']=train_dataset['pid'].astype('category')\n",
    "test_dataset['pid']=test_dataset['pid'].astype('category')\n",
    "\n",
    "train_dataset['median_target_dummy']=train_dataset['median_target_dummy'].astype('category')\n",
    "\n",
    "\n",
    "train_dataset['ID']=train_dataset['ID'].astype('category')\n",
    "test_dataset['ID']= test_dataset['ID'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1613318105810,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "l0cWbVISCjB_"
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_dataset.columns if not (c.endswith(\"median_target_dummy\"))]\n",
    "label = [\"median_target_dummy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "executionInfo": {
     "elapsed": 1009,
     "status": "ok",
     "timestamp": 1613318108075,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "eu3YS5O2CHkD"
   },
   "outputs": [],
   "source": [
    "X_classif = train_dataset[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1613318108076,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "zd5jWhpnC4ue"
   },
   "outputs": [],
   "source": [
    "y_classif = train_dataset[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yk8Ezaulr1Ha"
   },
   "source": [
    "#### 4.3 Machine learning feature creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1613318112016,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "OlEfidaQCmTA"
   },
   "outputs": [],
   "source": [
    "def evaluate_clf(clf, features, labels, num_iters=10, test_size=0.3):\n",
    "    print (clf)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    first = True\n",
    "    for trial in range(num_iters):\n",
    "        features_train, features_test, labels_train, labels_test =\\\n",
    "            train_test_split(features, labels, test_size=test_size)\n",
    "        clf.fit(features_train,labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        accuracy.append(accuracy_score(labels_test, predictions))\n",
    "        precision.append(precision_score(labels_test, predictions))\n",
    "        recall.append(recall_score(labels_test, predictions))\n",
    "        if trial % 10 == 0:\n",
    "            if first:\n",
    "                sys.stdout.write('\\nProcessing')\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "            first = False\n",
    "\n",
    "    print (\"done.\\n\")\n",
    "    print (\"precision: {}\".format(mean(precision)))\n",
    "    print (\"recall:    {}\".format(mean(recall)))\n",
    "    print (\"accuracy:    {}\".format(mean(accuracy)))\n",
    "    return len(labels_test)\n",
    "    return mean(precision), mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1613318114982,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "sc8q90_NCqmh"
   },
   "outputs": [],
   "source": [
    "tre_clf=DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3t5XdpHCsX2"
   },
   "outputs": [],
   "source": [
    "#evaluate_clf(tre_clf, X_classif,y_classif['median_target_dummy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMWvhzMKXUeT"
   },
   "source": [
    "precision: 1.0\n",
    "recall:    1.0\n",
    "accuracy:    1.0\n",
    "\n",
    "205345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yo1FLeOOFjvZ"
   },
   "source": [
    "#### 4.4 fit the model with train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-LjpOhzFsRK"
   },
   "outputs": [],
   "source": [
    "tre_clf.fit(X_classif,y_classif['median_target_dummy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmXxSZexiwdn"
   },
   "outputs": [],
   "source": [
    "# save the classifier\n",
    "#with open('/content/drive/MyDrive/M2/U4_Prediction_stock_auction_volumes/notebook/rf_classifier.pkl', 'wb') as fid:\n",
    "    #pickle.dump(tre_clf, fid)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrKTRLqamdYZ"
   },
   "outputs": [],
   "source": [
    "# load it again\n",
    "with open('/content/drive/MyDrive/M2/U4_Prediction_stock_auction_volumes/notebook/rf_classifier.pkl', 'rb') as fid:\n",
    "    tre_clf = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V366KVyLFt_L"
   },
   "source": [
    "#### 4.5 Predict the dummy target variable on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSONtbCD90FI"
   },
   "outputs": [],
   "source": [
    "test_dataset['median_target_dummy'] = tre_clf.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqeS9p2SGUpA"
   },
   "source": [
    "### Task 5 : Create final dataset from Advanced FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iej03HNHUNWV"
   },
   "outputs": [],
   "source": [
    "data_dir  = \"/content/drive/MyDrive/M2/U4_Prediction_stock_auction_volumes/dataset\"\n",
    "data_list = glob.glob(os.path.join(data_dir, '**.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmhJnhYMUAHv"
   },
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(\"%s/y_train.csv\" % data_dir, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVsibTb8Y6QC"
   },
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbka6jWmUFHr"
   },
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.merge(y_train, on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuNsMu-2UxI7"
   },
   "outputs": [],
   "source": [
    "del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7DvKoH3xg1S"
   },
   "outputs": [],
   "source": [
    "train_dataset.to_csv('/content/drive/MyDrive/M2/U4_Prediction_stock_auction_volumes/dataset/clean_dataset/train_dataset.csv',sep=',', index=False)\n",
    "test_dataset.to_csv('/content/drive/MyDrive/M2/U4_Prediction_stock_auction_volumes/dataset/clean_dataset/test_dataset.csv',sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5Xk8nM0wBMz"
   },
   "source": [
    "## Step 3 : Machine Learning modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KupPXhblIKau"
   },
   "source": [
    "### Task 1 Loading libraries and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQoJjg-VLzxg"
   },
   "source": [
    "#### 1.1 Lib and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1613318193663,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "duQFsBRHIT_i"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1613318194274,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "pBbq90CmIJyE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "import delayed\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score \n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sys\n",
    "from numpy import mean\n",
    "import pickle\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble  import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "elapsed": 2772,
     "status": "ok",
     "timestamp": 1613225690875,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "bVWKXxtTk6Eg",
    "outputId": "0319594e-a92e-4b9f-e191-5ec054f1f86f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'!rm -r /content/LightGBM\\n!git clone --recursive https://github.com/Microsoft/LightGBM\\n%cd /content/LightGBM\\n!mkdir build\\n!cmake -DUSE_GPU=1 #avoid ..\\n!make -j$(nproc)\\n!sudo apt-get -y install python-pip\\n!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\\n%cd /content/LightGBM/python-package\\n!sudo python setup.py install --precompile'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!rm -r /content/LightGBM\n",
    "!git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "%cd /content/LightGBM\n",
    "!mkdir build\n",
    "!cmake -DUSE_GPU=1 #avoid ..\n",
    "!make -j$(nproc)\n",
    "!sudo apt-get -y install python-pip\n",
    "!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
    "%cd /content/LightGBM/python-package\n",
    "!sudo python setup.py install --precompile\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "executionInfo": {
     "elapsed": 477,
     "status": "ok",
     "timestamp": 1613318200004,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "GVlmCE_IIJyF"
   },
   "outputs": [],
   "source": [
    "data_dir  = \"/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/dataset/clean_dataset\"\n",
    "data_list = glob.glob(os.path.join(data_dir, '**.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "executionInfo": {
     "elapsed": 167812,
     "status": "ok",
     "timestamp": 1613318368306,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "ouYGjlSkIJyF"
   },
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"%s/train_dataset.csv\" % data_dir, sep=\",\")\n",
    "test_dataset = pd.read_csv(\"%s/test_dataset.csv\" % data_dir, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK7SHHB7L4ZI"
   },
   "source": [
    "#### 1.2 Store ID "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRfb2DnhYmhM"
   },
   "source": [
    "We don't need ID in ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1613318371402,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "Rg7T93qxXPLH"
   },
   "outputs": [],
   "source": [
    "ID_train=train_dataset['ID']\n",
    "ID_test=test_dataset['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "executionInfo": {
     "elapsed": 3238,
     "status": "ok",
     "timestamp": 1613318376001,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "ej5ssauUYS6K"
   },
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.drop(\"ID\",axis=1)\n",
    "test_dataset=test_dataset.drop(\"ID\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKeAeB-yMMK3"
   },
   "source": [
    "#### 1.3 Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "executionInfo": {
     "elapsed": 1636,
     "status": "ok",
     "timestamp": 1613318376001,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "gWneU3-o0ukQ"
   },
   "outputs": [],
   "source": [
    "train_dataset['pid']=train_dataset['pid'].astype('category')\n",
    "train_dataset['median_target_dummy']=train_dataset['median_target_dummy'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "executionInfo": {
     "elapsed": 878,
     "status": "ok",
     "timestamp": 1613318376002,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "2tpvkH3L1QH-"
   },
   "outputs": [],
   "source": [
    "test_dataset['pid']=test_dataset['pid'].astype('category')\n",
    "test_dataset['median_target_dummy']=test_dataset['median_target_dummy'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1613318376286,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "-DjIKUr53pST"
   },
   "outputs": [],
   "source": [
    "cat_features=[\"pid\",\"median_target_dummy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uz8t3wu24aBX"
   },
   "outputs": [],
   "source": [
    "#train_dataset=pd.get_dummies(train_dataset,columns=['pid','median_target_dummy'])\n",
    "#test_dataset=pd.get_dummies(test_dataset,columns=['pid','median_target_dummy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WIuT5-eMhCQ"
   },
   "source": [
    "#### 1.4 Store target and pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "executionInfo": {
     "elapsed": 1573,
     "status": "ok",
     "timestamp": 1613318381031,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "G0-INlcq_okH"
   },
   "outputs": [],
   "source": [
    "y_train = train_dataset[['pid','target']]\n",
    "train_dataset = train_dataset.drop(columns = ['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LfHamUlMraP"
   },
   "source": [
    "### Task 2 : K-best for feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1613318385156,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "Iq2gVbtxM5iL"
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_dataset.columns if c !=\"target\"]\n",
    "label = [\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1613318385760,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "8cQhdXvZM75V"
   },
   "outputs": [],
   "source": [
    "def getTopFeatures(train_x, train_y, n_features=15):\n",
    "    f_val_dict = {}\n",
    "    p_val_dict = {} \n",
    "    f_val, p_val = f_regression(train_x,train_y)\n",
    "    for i in range(len(f_val)):\n",
    "        if math.isnan(f_val[i]):\n",
    "            f_val[i] = 0.0\n",
    "        f_val_dict[i] = f_val[i]\n",
    "        if math.isnan(p_val[i]):\n",
    "            p_val[i] = 0.0\n",
    "        p_val_dict[i] = p_val[i]\n",
    "    \n",
    "    sorted_f = sorted(f_val_dict.items(), key=lambda item: item[1],reverse=True)\n",
    "    sorted_p = sorted(p_val_dict.items(), key=lambda item: item[1],reverse=True)\n",
    "    \n",
    "    feature_indexs = []\n",
    "    for i in range(0,n_features):\n",
    "        feature_indexs.append(sorted_f[i][0])\n",
    "    \n",
    "    return feature_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JjwGGZ0M-IS"
   },
   "outputs": [],
   "source": [
    "Selected_features = getTopFeatures(train_dataset,y_train['target'])\n",
    "Selected_features = np.array(Selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 128061,
     "status": "ok",
     "timestamp": 1613225817428,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "zNIn5zckNAph",
    "outputId": "44945588-38b4-47a3-95b9-0e182af6d5da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLV</th>\n",
       "      <th>sum_ret</th>\n",
       "      <th>median_ret</th>\n",
       "      <th>median_target_dummy</th>\n",
       "      <th>std_ret</th>\n",
       "      <th>return_nan</th>\n",
       "      <th>avg_day_rel_vol0</th>\n",
       "      <th>median_day_rel_vol0</th>\n",
       "      <th>std_day_rel_vol0</th>\n",
       "      <th>max_ret</th>\n",
       "      <th>day</th>\n",
       "      <th>max_vol</th>\n",
       "      <th>rel_vol0</th>\n",
       "      <th>abs_ret15</th>\n",
       "      <th>abs_ret9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.646580</td>\n",
       "      <td>0.739680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022135</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049047</td>\n",
       "      <td>0.039607</td>\n",
       "      <td>0.045797</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076994</td>\n",
       "      <td>0.017012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.835479</td>\n",
       "      <td>1.878094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047647</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049047</td>\n",
       "      <td>0.039607</td>\n",
       "      <td>0.045797</td>\n",
       "      <td>0.218818</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135543</td>\n",
       "      <td>0.086902</td>\n",
       "      <td>0.088771</td>\n",
       "      <td>0.110302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.270225</td>\n",
       "      <td>1.492592</td>\n",
       "      <td>0.020502</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028894</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049047</td>\n",
       "      <td>0.039607</td>\n",
       "      <td>0.045797</td>\n",
       "      <td>0.109649</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056237</td>\n",
       "      <td>0.050771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.288022</td>\n",
       "      <td>1.120654</td>\n",
       "      <td>0.012723</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021687</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049047</td>\n",
       "      <td>0.039607</td>\n",
       "      <td>0.045797</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038170</td>\n",
       "      <td>0.033444</td>\n",
       "      <td>0.025569</td>\n",
       "      <td>0.050988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.553135</td>\n",
       "      <td>1.284760</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0</td>\n",
       "      <td>0.024410</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049047</td>\n",
       "      <td>0.039607</td>\n",
       "      <td>0.045797</td>\n",
       "      <td>0.106270</td>\n",
       "      <td>0</td>\n",
       "      <td>0.082588</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.063884</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684477</th>\n",
       "      <td>-1.292121</td>\n",
       "      <td>16.987811</td>\n",
       "      <td>0.252966</td>\n",
       "      <td>1</td>\n",
       "      <td>0.272910</td>\n",
       "      <td>2</td>\n",
       "      <td>0.045077</td>\n",
       "      <td>0.035815</td>\n",
       "      <td>0.035454</td>\n",
       "      <td>1.434599</td>\n",
       "      <td>770</td>\n",
       "      <td>0.151295</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.665004</td>\n",
       "      <td>0.672834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684478</th>\n",
       "      <td>-1.104483</td>\n",
       "      <td>8.717839</td>\n",
       "      <td>0.102250</td>\n",
       "      <td>1</td>\n",
       "      <td>0.143534</td>\n",
       "      <td>5</td>\n",
       "      <td>0.045077</td>\n",
       "      <td>0.035815</td>\n",
       "      <td>0.035454</td>\n",
       "      <td>0.754875</td>\n",
       "      <td>770</td>\n",
       "      <td>0.086605</td>\n",
       "      <td>0.024668</td>\n",
       "      <td>0.040858</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684479</th>\n",
       "      <td>-1.179556</td>\n",
       "      <td>5.703267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.127382</td>\n",
       "      <td>3</td>\n",
       "      <td>0.045077</td>\n",
       "      <td>0.035815</td>\n",
       "      <td>0.035454</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>770</td>\n",
       "      <td>0.098189</td>\n",
       "      <td>0.030452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684480</th>\n",
       "      <td>-1.370424</td>\n",
       "      <td>8.491105</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149656</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045077</td>\n",
       "      <td>0.035815</td>\n",
       "      <td>0.035454</td>\n",
       "      <td>0.601052</td>\n",
       "      <td>770</td>\n",
       "      <td>0.092877</td>\n",
       "      <td>0.045159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684481</th>\n",
       "      <td>-1.490859</td>\n",
       "      <td>9.398699</td>\n",
       "      <td>0.112740</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200471</td>\n",
       "      <td>6</td>\n",
       "      <td>0.045077</td>\n",
       "      <td>0.035815</td>\n",
       "      <td>0.035454</td>\n",
       "      <td>1.186441</td>\n",
       "      <td>770</td>\n",
       "      <td>0.071583</td>\n",
       "      <td>0.007989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>684482 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             NLV    sum_ret  median_ret  ...  rel_vol0  abs_ret15  abs_ret9\n",
       "0       0.646580   0.739680    0.000000  ...  0.017012   0.000000  0.073260\n",
       "1       0.835479   1.878094    0.000000  ...  0.086902   0.088771  0.110302\n",
       "2       1.270225   1.492592    0.020502  ...  0.050771   0.000000  0.082079\n",
       "3       1.288022   1.120654    0.012723  ...  0.033444   0.025569  0.050988\n",
       "4       0.553135   1.284760    0.021200  ...  0.071315   0.063884  0.000000\n",
       "...          ...        ...         ...  ...       ...        ...       ...\n",
       "684477 -1.292121  16.987811    0.252966  ...  0.019060   0.665004  0.672834\n",
       "684478 -1.104483   8.717839    0.102250  ...  0.024668   0.040858  0.000000\n",
       "684479 -1.179556   5.703267    0.000000  ...  0.030452   0.000000  0.428266\n",
       "684480 -1.370424   8.491105    0.074686  ...  0.045159   0.000000  0.370370\n",
       "684481 -1.490859   9.398699    0.112740  ...  0.007989   0.000000  0.396601\n",
       "\n",
       "[684482 rows x 15 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.iloc[:,Selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyCM_K3lNPvc"
   },
   "source": [
    "### Task 3 : Evaluate ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0_bGYM5NfXo"
   },
   "source": [
    "#### 3.1 Define Ml models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo8-rxP72ff8"
   },
   "source": [
    "##### 3.1.1 Linear Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQjSUdpeNh-m"
   },
   "outputs": [],
   "source": [
    "l_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yp36yqWOCip"
   },
   "source": [
    "##### 3.1.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1OzUEhiNt_y"
   },
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(max_depth = 5,max_features = 'sqrt',n_estimators = 10, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4ateMWsOFJ6"
   },
   "source": [
    "##### 3.1.3 Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D1LKUOLOg5l"
   },
   "outputs": [],
   "source": [
    "gb_reg = GradientBoostingRegressor(random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4VIWG-2OInI"
   },
   "source": [
    "##### 3.1.4 Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMfXIqQ6Oi1S"
   },
   "outputs": [],
   "source": [
    "tre_reg =DecisionTreeRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8NJfN-AOWHM"
   },
   "source": [
    "##### 3.1.5 KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Pas6RSUOkxU"
   },
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsRegressor(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRyFKafRe2qK"
   },
   "source": [
    "#### 3.2 Define Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMK3wZi2fWDL"
   },
   "outputs": [],
   "source": [
    "def evaluate_pred_reg(clf, features, labels, num_iters=5, test_size=0.3):\n",
    "    print (clf)\n",
    "    mean_squared_error_score = []\n",
    "    first = True\n",
    "    for trial in range(num_iters):\n",
    "        features_train, features_test, labels_train, labels_test =\\\n",
    "            train_test_split(features, labels, test_size=test_size)\n",
    "        clf.fit(features_train,labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        mean_squared_error_score.append(mean_squared_error(labels_test, predictions))\n",
    "\n",
    "        if trial % 10 == 0:\n",
    "            if first:\n",
    "                sys.stdout.write('\\nProcessing')\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "            first = False\n",
    "    #print (\"done.\\n\")\n",
    "    #print (\"mse: {}\".format(mean(mean_squared_error_score)))\n",
    "    return mean(mean_squared_error_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpmlksUpPY2p"
   },
   "source": [
    "#### 3.3 Run evaluate function for ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BBEJ9fG5bV5"
   },
   "source": [
    "##### 3.3.1 Evaluate linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 125645,
     "status": "ok",
     "timestamp": 1613225817482,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "j3j5yFEM54ZB",
    "outputId": "f1eaff12-1333-4ee3-d137-cae66259a84f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "#evaluate_pred_reg(l_reg, train_dataset.iloc[:,Selected_features], y_train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwd9NnTk5-p0"
   },
   "source": [
    "##### 3.3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1433DDhqHUH"
   },
   "outputs": [],
   "source": [
    "#evaluate_pred_reg(rf_reg, train_dataset.iloc[:,Selected_features], y_train['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbI2xkzx58KO"
   },
   "source": [
    "##### 3.3.3 Evaluate Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8zkdJMOqzM0"
   },
   "outputs": [],
   "source": [
    "#evaluate_pred_reg(gb_reg, train_dataset.iloc[:,Selected_features], y_train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2i-zDGgPyll"
   },
   "source": [
    "##### 3.3.4 Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWpMzPqQ7MJU"
   },
   "outputs": [],
   "source": [
    "#evaluate_pred_reg(tre_reg, train_dataset.iloc[:,Selected_features], y_train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bZ95YQU8h7x"
   },
   "source": [
    "##### 3.3.5 Evaluate Knn regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPVq495U8tDY"
   },
   "outputs": [],
   "source": [
    "#evaluate_pred_reg(knn_clf,train_dataset.iloc[:,Selected_features],y_train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1tJTU3UQJvW"
   },
   "source": [
    "### Task 4 : Advanced ML modelling per pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYzfBIDjQz0S"
   },
   "source": [
    "#### 4.1 Data preperation for advanced ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svfj7xGqQ87m"
   },
   "source": [
    "Get unique pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Frs_wJXAQ-PU"
   },
   "outputs": [],
   "source": [
    "pid = list(train_dataset['pid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndJsU9-cQ_wN"
   },
   "outputs": [],
   "source": [
    "train_dataset_2 = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVeG1YvDRCdj"
   },
   "outputs": [],
   "source": [
    "test_dataset_2 = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUOqI-FqREIC"
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_dataset.columns if ((c !=\"target\") &(c !=\"pid\") & (c !=\"day\") & (c !=\"median_target_dummy\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHKIniFhRHEq"
   },
   "source": [
    "#### 4.2 Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ynQViWtRK05"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "train_dataset_2[features] = scaler.fit_transform(train_dataset_2[features])\n",
    "test_dataset_2[features]=scaler.fit_transform(test_dataset_2[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj2zwkn3RWEX"
   },
   "source": [
    "#### 4.3 Evalute Linear modelling for each pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245543,
     "status": "ok",
     "timestamp": 1613225938060,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "VmARTRZeRVIf",
    "outputId": "206d1a50-edd3-4916-ea8d-278b86733d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n",
      "\n",
      "Processing.0\n",
      "LinearRegression()\n",
      "\n",
      "Processing.1\n",
      "LinearRegression()\n",
      "\n",
      "Processing.2\n",
      "LinearRegression()\n",
      "\n",
      "Processing.3\n",
      "LinearRegression()\n",
      "\n",
      "Processing.4\n",
      "LinearRegression()\n",
      "\n",
      "Processing.5\n",
      "LinearRegression()\n",
      "\n",
      "Processing.6\n",
      "LinearRegression()\n",
      "\n",
      "Processing.7\n",
      "LinearRegression()\n",
      "\n",
      "Processing.8\n",
      "LinearRegression()\n",
      "\n",
      "Processing.9\n",
      "LinearRegression()\n",
      "\n",
      "Processing.10\n",
      "LinearRegression()\n",
      "\n",
      "Processing.11\n",
      "LinearRegression()\n",
      "\n",
      "Processing.12\n",
      "LinearRegression()\n",
      "\n",
      "Processing.13\n",
      "LinearRegression()\n",
      "\n",
      "Processing.14\n",
      "LinearRegression()\n",
      "\n",
      "Processing.15\n",
      "LinearRegression()\n",
      "\n",
      "Processing.16\n",
      "LinearRegression()\n",
      "\n",
      "Processing.17\n",
      "LinearRegression()\n",
      "\n",
      "Processing.18\n",
      "LinearRegression()\n",
      "\n",
      "Processing.19\n",
      "LinearRegression()\n",
      "\n",
      "Processing.20\n",
      "LinearRegression()\n",
      "\n",
      "Processing.21\n",
      "LinearRegression()\n",
      "\n",
      "Processing.22\n",
      "LinearRegression()\n",
      "\n",
      "Processing.23\n",
      "LinearRegression()\n",
      "\n",
      "Processing.24\n",
      "LinearRegression()\n",
      "\n",
      "Processing.25\n",
      "LinearRegression()\n",
      "\n",
      "Processing.26\n",
      "LinearRegression()\n",
      "\n",
      "Processing.27\n",
      "LinearRegression()\n",
      "\n",
      "Processing.28\n",
      "LinearRegression()\n",
      "\n",
      "Processing.29\n",
      "LinearRegression()\n",
      "\n",
      "Processing.30\n",
      "LinearRegression()\n",
      "\n",
      "Processing.31\n",
      "LinearRegression()\n",
      "\n",
      "Processing.32\n",
      "LinearRegression()\n",
      "\n",
      "Processing.33\n",
      "LinearRegression()\n",
      "\n",
      "Processing.34\n",
      "LinearRegression()\n",
      "\n",
      "Processing.35\n",
      "LinearRegression()\n",
      "\n",
      "Processing.36\n",
      "LinearRegression()\n",
      "\n",
      "Processing.37\n",
      "LinearRegression()\n",
      "\n",
      "Processing.38\n",
      "LinearRegression()\n",
      "\n",
      "Processing.39\n",
      "LinearRegression()\n",
      "\n",
      "Processing.40\n",
      "LinearRegression()\n",
      "\n",
      "Processing.41\n",
      "LinearRegression()\n",
      "\n",
      "Processing.42\n",
      "LinearRegression()\n",
      "\n",
      "Processing.43\n",
      "LinearRegression()\n",
      "\n",
      "Processing.44\n",
      "LinearRegression()\n",
      "\n",
      "Processing.45\n",
      "LinearRegression()\n",
      "\n",
      "Processing.46\n",
      "LinearRegression()\n",
      "\n",
      "Processing.47\n",
      "LinearRegression()\n",
      "\n",
      "Processing.48\n",
      "LinearRegression()\n",
      "\n",
      "Processing.49\n",
      "LinearRegression()\n",
      "\n",
      "Processing.50\n",
      "LinearRegression()\n",
      "\n",
      "Processing.51\n",
      "LinearRegression()\n",
      "\n",
      "Processing.52\n",
      "LinearRegression()\n",
      "\n",
      "Processing.53\n",
      "LinearRegression()\n",
      "\n",
      "Processing.54\n",
      "LinearRegression()\n",
      "\n",
      "Processing.55\n",
      "LinearRegression()\n",
      "\n",
      "Processing.56\n",
      "LinearRegression()\n",
      "\n",
      "Processing.57\n",
      "LinearRegression()\n",
      "\n",
      "Processing.58\n",
      "LinearRegression()\n",
      "\n",
      "Processing.59\n",
      "LinearRegression()\n",
      "\n",
      "Processing.60\n",
      "LinearRegression()\n",
      "\n",
      "Processing.61\n",
      "LinearRegression()\n",
      "\n",
      "Processing.62\n",
      "LinearRegression()\n",
      "\n",
      "Processing.63\n",
      "LinearRegression()\n",
      "\n",
      "Processing.64\n",
      "LinearRegression()\n",
      "\n",
      "Processing.65\n",
      "LinearRegression()\n",
      "\n",
      "Processing.66\n",
      "LinearRegression()\n",
      "\n",
      "Processing.67\n",
      "LinearRegression()\n",
      "\n",
      "Processing.68\n",
      "LinearRegression()\n",
      "\n",
      "Processing.69\n",
      "LinearRegression()\n",
      "\n",
      "Processing.70\n",
      "LinearRegression()\n",
      "\n",
      "Processing.71\n",
      "LinearRegression()\n",
      "\n",
      "Processing.72\n",
      "LinearRegression()\n",
      "\n",
      "Processing.73\n",
      "LinearRegression()\n",
      "\n",
      "Processing.74\n",
      "LinearRegression()\n",
      "\n",
      "Processing.75\n",
      "LinearRegression()\n",
      "\n",
      "Processing.76\n",
      "LinearRegression()\n",
      "\n",
      "Processing.77\n",
      "LinearRegression()\n",
      "\n",
      "Processing.78\n",
      "LinearRegression()\n",
      "\n",
      "Processing.79\n",
      "LinearRegression()\n",
      "\n",
      "Processing.80\n",
      "LinearRegression()\n",
      "\n",
      "Processing.81\n",
      "LinearRegression()\n",
      "\n",
      "Processing.82\n",
      "LinearRegression()\n",
      "\n",
      "Processing.83\n",
      "LinearRegression()\n",
      "\n",
      "Processing.84\n",
      "LinearRegression()\n",
      "\n",
      "Processing.85\n",
      "LinearRegression()\n",
      "\n",
      "Processing.86\n",
      "LinearRegression()\n",
      "\n",
      "Processing.87\n",
      "LinearRegression()\n",
      "\n",
      "Processing.88\n",
      "LinearRegression()\n",
      "\n",
      "Processing.89\n",
      "LinearRegression()\n",
      "\n",
      "Processing.90\n",
      "LinearRegression()\n",
      "\n",
      "Processing.91\n",
      "LinearRegression()\n",
      "\n",
      "Processing.92\n",
      "LinearRegression()\n",
      "\n",
      "Processing.93\n",
      "LinearRegression()\n",
      "\n",
      "Processing.94\n",
      "LinearRegression()\n",
      "\n",
      "Processing.95\n",
      "LinearRegression()\n",
      "\n",
      "Processing.96\n",
      "LinearRegression()\n",
      "\n",
      "Processing.97\n",
      "LinearRegression()\n",
      "\n",
      "Processing.98\n",
      "LinearRegression()\n",
      "\n",
      "Processing.99\n",
      "LinearRegression()\n",
      "\n",
      "Processing.100\n",
      "LinearRegression()\n",
      "\n",
      "Processing.101\n",
      "LinearRegression()\n",
      "\n",
      "Processing.102\n",
      "LinearRegression()\n",
      "\n",
      "Processing.103\n",
      "LinearRegression()\n",
      "\n",
      "Processing.104\n",
      "LinearRegression()\n",
      "\n",
      "Processing.105\n",
      "LinearRegression()\n",
      "\n",
      "Processing.106\n",
      "LinearRegression()\n",
      "\n",
      "Processing.107\n",
      "LinearRegression()\n",
      "\n",
      "Processing.108\n",
      "LinearRegression()\n",
      "\n",
      "Processing.109\n",
      "LinearRegression()\n",
      "\n",
      "Processing.110\n",
      "LinearRegression()\n",
      "\n",
      "Processing.111\n",
      "LinearRegression()\n",
      "\n",
      "Processing.112\n",
      "LinearRegression()\n",
      "\n",
      "Processing.113\n",
      "LinearRegression()\n",
      "\n",
      "Processing.114\n",
      "LinearRegression()\n",
      "\n",
      "Processing.115\n",
      "LinearRegression()\n",
      "\n",
      "Processing.116\n",
      "LinearRegression()\n",
      "\n",
      "Processing.117\n",
      "LinearRegression()\n",
      "\n",
      "Processing.118\n",
      "LinearRegression()\n",
      "\n",
      "Processing.119\n",
      "LinearRegression()\n",
      "\n",
      "Processing.120\n",
      "LinearRegression()\n",
      "\n",
      "Processing.121\n",
      "LinearRegression()\n",
      "\n",
      "Processing.122\n",
      "LinearRegression()\n",
      "\n",
      "Processing.123\n",
      "LinearRegression()\n",
      "\n",
      "Processing.124\n",
      "LinearRegression()\n",
      "\n",
      "Processing.125\n",
      "LinearRegression()\n",
      "\n",
      "Processing.126\n",
      "LinearRegression()\n",
      "\n",
      "Processing.127\n",
      "LinearRegression()\n",
      "\n",
      "Processing.128\n",
      "LinearRegression()\n",
      "\n",
      "Processing.129\n",
      "LinearRegression()\n",
      "\n",
      "Processing.130\n",
      "LinearRegression()\n",
      "\n",
      "Processing.131\n",
      "LinearRegression()\n",
      "\n",
      "Processing.132\n",
      "LinearRegression()\n",
      "\n",
      "Processing.133\n",
      "LinearRegression()\n",
      "\n",
      "Processing.134\n",
      "LinearRegression()\n",
      "\n",
      "Processing.135\n",
      "LinearRegression()\n",
      "\n",
      "Processing.136\n",
      "LinearRegression()\n",
      "\n",
      "Processing.137\n",
      "LinearRegression()\n",
      "\n",
      "Processing.138\n",
      "LinearRegression()\n",
      "\n",
      "Processing.139\n",
      "LinearRegression()\n",
      "\n",
      "Processing.140\n",
      "LinearRegression()\n",
      "\n",
      "Processing.141\n",
      "LinearRegression()\n",
      "\n",
      "Processing.142\n",
      "LinearRegression()\n",
      "\n",
      "Processing.143\n",
      "LinearRegression()\n",
      "\n",
      "Processing.144\n",
      "LinearRegression()\n",
      "\n",
      "Processing.145\n",
      "LinearRegression()\n",
      "\n",
      "Processing.146\n",
      "LinearRegression()\n",
      "\n",
      "Processing.147\n",
      "LinearRegression()\n",
      "\n",
      "Processing.148\n",
      "LinearRegression()\n",
      "\n",
      "Processing.149\n",
      "LinearRegression()\n",
      "\n",
      "Processing.150\n",
      "LinearRegression()\n",
      "\n",
      "Processing.151\n",
      "LinearRegression()\n",
      "\n",
      "Processing.152\n",
      "LinearRegression()\n",
      "\n",
      "Processing.153\n",
      "LinearRegression()\n",
      "\n",
      "Processing.154\n",
      "LinearRegression()\n",
      "\n",
      "Processing.155\n",
      "LinearRegression()\n",
      "\n",
      "Processing.156\n",
      "LinearRegression()\n",
      "\n",
      "Processing.157\n",
      "LinearRegression()\n",
      "\n",
      "Processing.158\n",
      "LinearRegression()\n",
      "\n",
      "Processing.159\n",
      "LinearRegression()\n",
      "\n",
      "Processing.160\n",
      "LinearRegression()\n",
      "\n",
      "Processing.161\n",
      "LinearRegression()\n",
      "\n",
      "Processing.162\n",
      "LinearRegression()\n",
      "\n",
      "Processing.163\n",
      "LinearRegression()\n",
      "\n",
      "Processing.164\n",
      "LinearRegression()\n",
      "\n",
      "Processing.165\n",
      "LinearRegression()\n",
      "\n",
      "Processing.166\n",
      "LinearRegression()\n",
      "\n",
      "Processing.167\n",
      "LinearRegression()\n",
      "\n",
      "Processing.168\n",
      "LinearRegression()\n",
      "\n",
      "Processing.169\n",
      "LinearRegression()\n",
      "\n",
      "Processing.170\n",
      "LinearRegression()\n",
      "\n",
      "Processing.171\n",
      "LinearRegression()\n",
      "\n",
      "Processing.172\n",
      "LinearRegression()\n",
      "\n",
      "Processing.173\n",
      "LinearRegression()\n",
      "\n",
      "Processing.174\n",
      "LinearRegression()\n",
      "\n",
      "Processing.175\n",
      "LinearRegression()\n",
      "\n",
      "Processing.176\n",
      "LinearRegression()\n",
      "\n",
      "Processing.177\n",
      "LinearRegression()\n",
      "\n",
      "Processing.178\n",
      "LinearRegression()\n",
      "\n",
      "Processing.179\n",
      "LinearRegression()\n",
      "\n",
      "Processing.180\n",
      "LinearRegression()\n",
      "\n",
      "Processing.181\n",
      "LinearRegression()\n",
      "\n",
      "Processing.182\n",
      "LinearRegression()\n",
      "\n",
      "Processing.183\n",
      "LinearRegression()\n",
      "\n",
      "Processing.184\n",
      "LinearRegression()\n",
      "\n",
      "Processing.185\n",
      "LinearRegression()\n",
      "\n",
      "Processing.186\n",
      "LinearRegression()\n",
      "\n",
      "Processing.187\n",
      "LinearRegression()\n",
      "\n",
      "Processing.188\n",
      "LinearRegression()\n",
      "\n",
      "Processing.189\n",
      "LinearRegression()\n",
      "\n",
      "Processing.190\n",
      "LinearRegression()\n",
      "\n",
      "Processing.191\n",
      "LinearRegression()\n",
      "\n",
      "Processing.192\n",
      "LinearRegression()\n",
      "\n",
      "Processing.193\n",
      "LinearRegression()\n",
      "\n",
      "Processing.194\n",
      "LinearRegression()\n",
      "\n",
      "Processing.195\n",
      "LinearRegression()\n",
      "\n",
      "Processing.196\n",
      "LinearRegression()\n",
      "\n",
      "Processing.197\n",
      "LinearRegression()\n",
      "\n",
      "Processing.198\n",
      "LinearRegression()\n",
      "\n",
      "Processing.199\n",
      "LinearRegression()\n",
      "\n",
      "Processing.200\n",
      "LinearRegression()\n",
      "\n",
      "Processing.201\n",
      "LinearRegression()\n",
      "\n",
      "Processing.202\n",
      "LinearRegression()\n",
      "\n",
      "Processing.203\n",
      "LinearRegression()\n",
      "\n",
      "Processing.204\n",
      "LinearRegression()\n",
      "\n",
      "Processing.205\n",
      "LinearRegression()\n",
      "\n",
      "Processing.206\n",
      "LinearRegression()\n",
      "\n",
      "Processing.207\n",
      "LinearRegression()\n",
      "\n",
      "Processing.208\n",
      "LinearRegression()\n",
      "\n",
      "Processing.209\n",
      "LinearRegression()\n",
      "\n",
      "Processing.210\n",
      "LinearRegression()\n",
      "\n",
      "Processing.211\n",
      "LinearRegression()\n",
      "\n",
      "Processing.212\n",
      "LinearRegression()\n",
      "\n",
      "Processing.213\n",
      "LinearRegression()\n",
      "\n",
      "Processing.214\n",
      "LinearRegression()\n",
      "\n",
      "Processing.215\n",
      "LinearRegression()\n",
      "\n",
      "Processing.216\n",
      "LinearRegression()\n",
      "\n",
      "Processing.217\n",
      "LinearRegression()\n",
      "\n",
      "Processing.218\n",
      "LinearRegression()\n",
      "\n",
      "Processing.219\n",
      "LinearRegression()\n",
      "\n",
      "Processing.220\n",
      "LinearRegression()\n",
      "\n",
      "Processing.221\n",
      "LinearRegression()\n",
      "\n",
      "Processing.222\n",
      "LinearRegression()\n",
      "\n",
      "Processing.223\n",
      "LinearRegression()\n",
      "\n",
      "Processing.224\n",
      "LinearRegression()\n",
      "\n",
      "Processing.225\n",
      "LinearRegression()\n",
      "\n",
      "Processing.226\n",
      "LinearRegression()\n",
      "\n",
      "Processing.227\n",
      "LinearRegression()\n",
      "\n",
      "Processing.228\n",
      "LinearRegression()\n",
      "\n",
      "Processing.229\n",
      "LinearRegression()\n",
      "\n",
      "Processing.230\n",
      "LinearRegression()\n",
      "\n",
      "Processing.231\n",
      "LinearRegression()\n",
      "\n",
      "Processing.232\n",
      "LinearRegression()\n",
      "\n",
      "Processing.233\n",
      "LinearRegression()\n",
      "\n",
      "Processing.234\n",
      "LinearRegression()\n",
      "\n",
      "Processing.235\n",
      "LinearRegression()\n",
      "\n",
      "Processing.236\n",
      "LinearRegression()\n",
      "\n",
      "Processing.237\n",
      "LinearRegression()\n",
      "\n",
      "Processing.238\n",
      "LinearRegression()\n",
      "\n",
      "Processing.239\n",
      "LinearRegression()\n",
      "\n",
      "Processing.240\n",
      "LinearRegression()\n",
      "\n",
      "Processing.241\n",
      "LinearRegression()\n",
      "\n",
      "Processing.242\n",
      "LinearRegression()\n",
      "\n",
      "Processing.243\n",
      "LinearRegression()\n",
      "\n",
      "Processing.244\n",
      "LinearRegression()\n",
      "\n",
      "Processing.245\n",
      "LinearRegression()\n",
      "\n",
      "Processing.246\n",
      "LinearRegression()\n",
      "\n",
      "Processing.247\n",
      "LinearRegression()\n",
      "\n",
      "Processing.248\n",
      "LinearRegression()\n",
      "\n",
      "Processing.249\n",
      "LinearRegression()\n",
      "\n",
      "Processing.250\n",
      "LinearRegression()\n",
      "\n",
      "Processing.251\n",
      "LinearRegression()\n",
      "\n",
      "Processing.252\n",
      "LinearRegression()\n",
      "\n",
      "Processing.253\n",
      "LinearRegression()\n",
      "\n",
      "Processing.254\n",
      "LinearRegression()\n",
      "\n",
      "Processing.255\n",
      "LinearRegression()\n",
      "\n",
      "Processing.256\n",
      "LinearRegression()\n",
      "\n",
      "Processing.257\n",
      "LinearRegression()\n",
      "\n",
      "Processing.258\n",
      "LinearRegression()\n",
      "\n",
      "Processing.259\n",
      "LinearRegression()\n",
      "\n",
      "Processing.260\n",
      "LinearRegression()\n",
      "\n",
      "Processing.261\n",
      "LinearRegression()\n",
      "\n",
      "Processing.262\n",
      "LinearRegression()\n",
      "\n",
      "Processing.263\n",
      "LinearRegression()\n",
      "\n",
      "Processing.264\n",
      "LinearRegression()\n",
      "\n",
      "Processing.265\n",
      "LinearRegression()\n",
      "\n",
      "Processing.266\n",
      "LinearRegression()\n",
      "\n",
      "Processing.267\n",
      "LinearRegression()\n",
      "\n",
      "Processing.268\n",
      "LinearRegression()\n",
      "\n",
      "Processing.269\n",
      "LinearRegression()\n",
      "\n",
      "Processing.270\n",
      "LinearRegression()\n",
      "\n",
      "Processing.271\n",
      "LinearRegression()\n",
      "\n",
      "Processing.272\n",
      "LinearRegression()\n",
      "\n",
      "Processing.273\n",
      "LinearRegression()\n",
      "\n",
      "Processing.274\n",
      "LinearRegression()\n",
      "\n",
      "Processing.275\n",
      "LinearRegression()\n",
      "\n",
      "Processing.276\n",
      "LinearRegression()\n",
      "\n",
      "Processing.277\n",
      "LinearRegression()\n",
      "\n",
      "Processing.278\n",
      "LinearRegression()\n",
      "\n",
      "Processing.279\n",
      "LinearRegression()\n",
      "\n",
      "Processing.280\n",
      "LinearRegression()\n",
      "\n",
      "Processing.281\n",
      "LinearRegression()\n",
      "\n",
      "Processing.282\n",
      "LinearRegression()\n",
      "\n",
      "Processing.283\n",
      "LinearRegression()\n",
      "\n",
      "Processing.284\n",
      "LinearRegression()\n",
      "\n",
      "Processing.285\n",
      "LinearRegression()\n",
      "\n",
      "Processing.286\n",
      "LinearRegression()\n",
      "\n",
      "Processing.287\n",
      "LinearRegression()\n",
      "\n",
      "Processing.288\n",
      "LinearRegression()\n",
      "\n",
      "Processing.289\n",
      "LinearRegression()\n",
      "\n",
      "Processing.290\n",
      "LinearRegression()\n",
      "\n",
      "Processing.291\n",
      "LinearRegression()\n",
      "\n",
      "Processing.292\n",
      "LinearRegression()\n",
      "\n",
      "Processing.293\n",
      "LinearRegression()\n",
      "\n",
      "Processing.294\n",
      "LinearRegression()\n",
      "\n",
      "Processing.295\n",
      "LinearRegression()\n",
      "\n",
      "Processing.296\n",
      "LinearRegression()\n",
      "\n",
      "Processing.297\n",
      "LinearRegression()\n",
      "\n",
      "Processing.298\n",
      "LinearRegression()\n",
      "\n",
      "Processing.299\n",
      "LinearRegression()\n",
      "\n",
      "Processing.300\n",
      "LinearRegression()\n",
      "\n",
      "Processing.301\n",
      "LinearRegression()\n",
      "\n",
      "Processing.302\n",
      "LinearRegression()\n",
      "\n",
      "Processing.303\n",
      "LinearRegression()\n",
      "\n",
      "Processing.304\n",
      "LinearRegression()\n",
      "\n",
      "Processing.305\n",
      "LinearRegression()\n",
      "\n",
      "Processing.306\n",
      "LinearRegression()\n",
      "\n",
      "Processing.307\n",
      "LinearRegression()\n",
      "\n",
      "Processing.308\n",
      "LinearRegression()\n",
      "\n",
      "Processing.309\n",
      "LinearRegression()\n",
      "\n",
      "Processing.310\n",
      "LinearRegression()\n",
      "\n",
      "Processing.311\n",
      "LinearRegression()\n",
      "\n",
      "Processing.312\n",
      "LinearRegression()\n",
      "\n",
      "Processing.313\n",
      "LinearRegression()\n",
      "\n",
      "Processing.314\n",
      "LinearRegression()\n",
      "\n",
      "Processing.315\n",
      "LinearRegression()\n",
      "\n",
      "Processing.316\n",
      "LinearRegression()\n",
      "\n",
      "Processing.317\n",
      "LinearRegression()\n",
      "\n",
      "Processing.318\n",
      "LinearRegression()\n",
      "\n",
      "Processing.319\n",
      "LinearRegression()\n",
      "\n",
      "Processing.320\n",
      "LinearRegression()\n",
      "\n",
      "Processing.321\n",
      "LinearRegression()\n",
      "\n",
      "Processing.322\n",
      "LinearRegression()\n",
      "\n",
      "Processing.323\n",
      "LinearRegression()\n",
      "\n",
      "Processing.324\n",
      "LinearRegression()\n",
      "\n",
      "Processing.325\n",
      "LinearRegression()\n",
      "\n",
      "Processing.326\n",
      "LinearRegression()\n",
      "\n",
      "Processing.327\n",
      "LinearRegression()\n",
      "\n",
      "Processing.328\n",
      "LinearRegression()\n",
      "\n",
      "Processing.329\n",
      "LinearRegression()\n",
      "\n",
      "Processing.330\n",
      "LinearRegression()\n",
      "\n",
      "Processing.331\n",
      "LinearRegression()\n",
      "\n",
      "Processing.332\n",
      "LinearRegression()\n",
      "\n",
      "Processing.333\n",
      "LinearRegression()\n",
      "\n",
      "Processing.334\n",
      "LinearRegression()\n",
      "\n",
      "Processing.335\n",
      "LinearRegression()\n",
      "\n",
      "Processing.336\n",
      "LinearRegression()\n",
      "\n",
      "Processing.337\n",
      "LinearRegression()\n",
      "\n",
      "Processing.338\n",
      "LinearRegression()\n",
      "\n",
      "Processing.339\n",
      "LinearRegression()\n",
      "\n",
      "Processing.340\n",
      "LinearRegression()\n",
      "\n",
      "Processing.341\n",
      "LinearRegression()\n",
      "\n",
      "Processing.342\n",
      "LinearRegression()\n",
      "\n",
      "Processing.343\n",
      "LinearRegression()\n",
      "\n",
      "Processing.344\n",
      "LinearRegression()\n",
      "\n",
      "Processing.345\n",
      "LinearRegression()\n",
      "\n",
      "Processing.346\n",
      "LinearRegression()\n",
      "\n",
      "Processing.347\n",
      "LinearRegression()\n",
      "\n",
      "Processing.348\n",
      "LinearRegression()\n",
      "\n",
      "Processing.349\n",
      "LinearRegression()\n",
      "\n",
      "Processing.350\n",
      "LinearRegression()\n",
      "\n",
      "Processing.351\n",
      "LinearRegression()\n",
      "\n",
      "Processing.352\n",
      "LinearRegression()\n",
      "\n",
      "Processing.353\n",
      "LinearRegression()\n",
      "\n",
      "Processing.354\n",
      "LinearRegression()\n",
      "\n",
      "Processing.355\n",
      "LinearRegression()\n",
      "\n",
      "Processing.356\n",
      "LinearRegression()\n",
      "\n",
      "Processing.357\n",
      "LinearRegression()\n",
      "\n",
      "Processing.358\n",
      "LinearRegression()\n",
      "\n",
      "Processing.359\n",
      "LinearRegression()\n",
      "\n",
      "Processing.360\n",
      "LinearRegression()\n",
      "\n",
      "Processing.361\n",
      "LinearRegression()\n",
      "\n",
      "Processing.362\n",
      "LinearRegression()\n",
      "\n",
      "Processing.363\n",
      "LinearRegression()\n",
      "\n",
      "Processing.364\n",
      "LinearRegression()\n",
      "\n",
      "Processing.365\n",
      "LinearRegression()\n",
      "\n",
      "Processing.366\n",
      "LinearRegression()\n",
      "\n",
      "Processing.367\n",
      "LinearRegression()\n",
      "\n",
      "Processing.368\n",
      "LinearRegression()\n",
      "\n",
      "Processing.369\n",
      "LinearRegression()\n",
      "\n",
      "Processing.370\n",
      "LinearRegression()\n",
      "\n",
      "Processing.371\n",
      "LinearRegression()\n",
      "\n",
      "Processing.372\n",
      "LinearRegression()\n",
      "\n",
      "Processing.373\n",
      "LinearRegression()\n",
      "\n",
      "Processing.374\n",
      "LinearRegression()\n",
      "\n",
      "Processing.375\n",
      "LinearRegression()\n",
      "\n",
      "Processing.376\n",
      "LinearRegression()\n",
      "\n",
      "Processing.377\n",
      "LinearRegression()\n",
      "\n",
      "Processing.378\n",
      "LinearRegression()\n",
      "\n",
      "Processing.379\n",
      "LinearRegression()\n",
      "\n",
      "Processing.380\n",
      "LinearRegression()\n",
      "\n",
      "Processing.381\n",
      "LinearRegression()\n",
      "\n",
      "Processing.382\n",
      "LinearRegression()\n",
      "\n",
      "Processing.383\n",
      "LinearRegression()\n",
      "\n",
      "Processing.384\n",
      "LinearRegression()\n",
      "\n",
      "Processing.385\n",
      "LinearRegression()\n",
      "\n",
      "Processing.386\n",
      "LinearRegression()\n",
      "\n",
      "Processing.387\n",
      "LinearRegression()\n",
      "\n",
      "Processing.388\n",
      "LinearRegression()\n",
      "\n",
      "Processing.389\n",
      "LinearRegression()\n",
      "\n",
      "Processing.390\n",
      "LinearRegression()\n",
      "\n",
      "Processing.391\n",
      "LinearRegression()\n",
      "\n",
      "Processing.392\n",
      "LinearRegression()\n",
      "\n",
      "Processing.393\n",
      "LinearRegression()\n",
      "\n",
      "Processing.394\n",
      "LinearRegression()\n",
      "\n",
      "Processing.395\n",
      "LinearRegression()\n",
      "\n",
      "Processing.396\n",
      "LinearRegression()\n",
      "\n",
      "Processing.397\n",
      "LinearRegression()\n",
      "\n",
      "Processing.398\n",
      "LinearRegression()\n",
      "\n",
      "Processing.399\n",
      "LinearRegression()\n",
      "\n",
      "Processing.400\n",
      "LinearRegression()\n",
      "\n",
      "Processing.401\n",
      "LinearRegression()\n",
      "\n",
      "Processing.402\n",
      "LinearRegression()\n",
      "\n",
      "Processing.403\n",
      "LinearRegression()\n",
      "\n",
      "Processing.404\n",
      "LinearRegression()\n",
      "\n",
      "Processing.405\n",
      "LinearRegression()\n",
      "\n",
      "Processing.406\n",
      "LinearRegression()\n",
      "\n",
      "Processing.407\n",
      "LinearRegression()\n",
      "\n",
      "Processing.408\n",
      "LinearRegression()\n",
      "\n",
      "Processing.409\n",
      "LinearRegression()\n",
      "\n",
      "Processing.410\n",
      "LinearRegression()\n",
      "\n",
      "Processing.411\n",
      "LinearRegression()\n",
      "\n",
      "Processing.412\n",
      "LinearRegression()\n",
      "\n",
      "Processing.413\n",
      "LinearRegression()\n",
      "\n",
      "Processing.414\n",
      "LinearRegression()\n",
      "\n",
      "Processing.415\n",
      "LinearRegression()\n",
      "\n",
      "Processing.416\n",
      "LinearRegression()\n",
      "\n",
      "Processing.417\n",
      "LinearRegression()\n",
      "\n",
      "Processing.418\n",
      "LinearRegression()\n",
      "\n",
      "Processing.419\n",
      "LinearRegression()\n",
      "\n",
      "Processing.420\n",
      "LinearRegression()\n",
      "\n",
      "Processing.421\n",
      "LinearRegression()\n",
      "\n",
      "Processing.422\n",
      "LinearRegression()\n",
      "\n",
      "Processing.423\n",
      "LinearRegression()\n",
      "\n",
      "Processing.424\n",
      "LinearRegression()\n",
      "\n",
      "Processing.425\n",
      "LinearRegression()\n",
      "\n",
      "Processing.426\n",
      "LinearRegression()\n",
      "\n",
      "Processing.427\n",
      "LinearRegression()\n",
      "\n",
      "Processing.428\n",
      "LinearRegression()\n",
      "\n",
      "Processing.429\n",
      "LinearRegression()\n",
      "\n",
      "Processing.430\n",
      "LinearRegression()\n",
      "\n",
      "Processing.431\n",
      "LinearRegression()\n",
      "\n",
      "Processing.432\n",
      "LinearRegression()\n",
      "\n",
      "Processing.433\n",
      "LinearRegression()\n",
      "\n",
      "Processing.434\n",
      "LinearRegression()\n",
      "\n",
      "Processing.435\n",
      "LinearRegression()\n",
      "\n",
      "Processing.436\n",
      "LinearRegression()\n",
      "\n",
      "Processing.437\n",
      "LinearRegression()\n",
      "\n",
      "Processing.438\n",
      "LinearRegression()\n",
      "\n",
      "Processing.439\n",
      "LinearRegression()\n",
      "\n",
      "Processing.440\n",
      "LinearRegression()\n",
      "\n",
      "Processing.441\n",
      "LinearRegression()\n",
      "\n",
      "Processing.442\n",
      "LinearRegression()\n",
      "\n",
      "Processing.443\n",
      "LinearRegression()\n",
      "\n",
      "Processing.444\n",
      "LinearRegression()\n",
      "\n",
      "Processing.445\n",
      "LinearRegression()\n",
      "\n",
      "Processing.446\n",
      "LinearRegression()\n",
      "\n",
      "Processing.447\n",
      "LinearRegression()\n",
      "\n",
      "Processing.448\n",
      "LinearRegression()\n",
      "\n",
      "Processing.449\n",
      "LinearRegression()\n",
      "\n",
      "Processing.450\n",
      "LinearRegression()\n",
      "\n",
      "Processing.451\n",
      "LinearRegression()\n",
      "\n",
      "Processing.452\n",
      "LinearRegression()\n",
      "\n",
      "Processing.453\n",
      "LinearRegression()\n",
      "\n",
      "Processing.454\n",
      "LinearRegression()\n",
      "\n",
      "Processing.455\n",
      "LinearRegression()\n",
      "\n",
      "Processing.456\n",
      "LinearRegression()\n",
      "\n",
      "Processing.457\n",
      "LinearRegression()\n",
      "\n",
      "Processing.458\n",
      "LinearRegression()\n",
      "\n",
      "Processing.459\n",
      "LinearRegression()\n",
      "\n",
      "Processing.460\n",
      "LinearRegression()\n",
      "\n",
      "Processing.461\n",
      "LinearRegression()\n",
      "\n",
      "Processing.462\n",
      "LinearRegression()\n",
      "\n",
      "Processing.463\n",
      "LinearRegression()\n",
      "\n",
      "Processing.464\n",
      "LinearRegression()\n",
      "\n",
      "Processing.465\n",
      "LinearRegression()\n",
      "\n",
      "Processing.466\n",
      "LinearRegression()\n",
      "\n",
      "Processing.467\n",
      "LinearRegression()\n",
      "\n",
      "Processing.468\n",
      "LinearRegression()\n",
      "\n",
      "Processing.469\n",
      "LinearRegression()\n",
      "\n",
      "Processing.470\n",
      "LinearRegression()\n",
      "\n",
      "Processing.471\n",
      "LinearRegression()\n",
      "\n",
      "Processing.472\n",
      "LinearRegression()\n",
      "\n",
      "Processing.473\n",
      "LinearRegression()\n",
      "\n",
      "Processing.474\n",
      "LinearRegression()\n",
      "\n",
      "Processing.475\n",
      "LinearRegression()\n",
      "\n",
      "Processing.476\n",
      "LinearRegression()\n",
      "\n",
      "Processing.477\n",
      "LinearRegression()\n",
      "\n",
      "Processing.478\n",
      "LinearRegression()\n",
      "\n",
      "Processing.479\n",
      "LinearRegression()\n",
      "\n",
      "Processing.480\n",
      "LinearRegression()\n",
      "\n",
      "Processing.481\n",
      "LinearRegression()\n",
      "\n",
      "Processing.482\n",
      "LinearRegression()\n",
      "\n",
      "Processing.483\n",
      "LinearRegression()\n",
      "\n",
      "Processing.484\n",
      "LinearRegression()\n",
      "\n",
      "Processing.485\n",
      "LinearRegression()\n",
      "\n",
      "Processing.486\n",
      "LinearRegression()\n",
      "\n",
      "Processing.487\n",
      "LinearRegression()\n",
      "\n",
      "Processing.488\n",
      "LinearRegression()\n",
      "\n",
      "Processing.489\n",
      "LinearRegression()\n",
      "\n",
      "Processing.490\n",
      "LinearRegression()\n",
      "\n",
      "Processing.491\n",
      "LinearRegression()\n",
      "\n",
      "Processing.492\n",
      "LinearRegression()\n",
      "\n",
      "Processing.493\n",
      "LinearRegression()\n",
      "\n",
      "Processing.494\n",
      "LinearRegression()\n",
      "\n",
      "Processing.495\n",
      "LinearRegression()\n",
      "\n",
      "Processing.496\n",
      "LinearRegression()\n",
      "\n",
      "Processing.497\n",
      "LinearRegression()\n",
      "\n",
      "Processing.498\n",
      "LinearRegression()\n",
      "\n",
      "Processing.499\n",
      "LinearRegression()\n",
      "\n",
      "Processing.500\n",
      "LinearRegression()\n",
      "\n",
      "Processing.501\n",
      "LinearRegression()\n",
      "\n",
      "Processing.502\n",
      "LinearRegression()\n",
      "\n",
      "Processing.503\n",
      "LinearRegression()\n",
      "\n",
      "Processing.504\n",
      "LinearRegression()\n",
      "\n",
      "Processing.505\n",
      "LinearRegression()\n",
      "\n",
      "Processing.506\n",
      "LinearRegression()\n",
      "\n",
      "Processing.507\n",
      "LinearRegression()\n",
      "\n",
      "Processing.508\n",
      "LinearRegression()\n",
      "\n",
      "Processing.509\n",
      "LinearRegression()\n",
      "\n",
      "Processing.510\n",
      "LinearRegression()\n",
      "\n",
      "Processing.511\n",
      "LinearRegression()\n",
      "\n",
      "Processing.512\n",
      "LinearRegression()\n",
      "\n",
      "Processing.513\n",
      "LinearRegression()\n",
      "\n",
      "Processing.514\n",
      "LinearRegression()\n",
      "\n",
      "Processing.515\n",
      "LinearRegression()\n",
      "\n",
      "Processing.516\n",
      "LinearRegression()\n",
      "\n",
      "Processing.517\n",
      "LinearRegression()\n",
      "\n",
      "Processing.518\n",
      "LinearRegression()\n",
      "\n",
      "Processing.519\n",
      "LinearRegression()\n",
      "\n",
      "Processing.520\n",
      "LinearRegression()\n",
      "\n",
      "Processing.521\n",
      "LinearRegression()\n",
      "\n",
      "Processing.522\n",
      "LinearRegression()\n",
      "\n",
      "Processing.523\n",
      "LinearRegression()\n",
      "\n",
      "Processing.524\n",
      "LinearRegression()\n",
      "\n",
      "Processing.525\n",
      "LinearRegression()\n",
      "\n",
      "Processing.526\n",
      "LinearRegression()\n",
      "\n",
      "Processing.527\n",
      "LinearRegression()\n",
      "\n",
      "Processing.528\n",
      "LinearRegression()\n",
      "\n",
      "Processing.529\n",
      "LinearRegression()\n",
      "\n",
      "Processing.530\n",
      "LinearRegression()\n",
      "\n",
      "Processing.531\n",
      "LinearRegression()\n",
      "\n",
      "Processing.532\n",
      "LinearRegression()\n",
      "\n",
      "Processing.533\n",
      "LinearRegression()\n",
      "\n",
      "Processing.534\n",
      "LinearRegression()\n",
      "\n",
      "Processing.535\n",
      "LinearRegression()\n",
      "\n",
      "Processing.536\n",
      "LinearRegression()\n",
      "\n",
      "Processing.537\n",
      "LinearRegression()\n",
      "\n",
      "Processing.538\n",
      "LinearRegression()\n",
      "\n",
      "Processing.539\n",
      "LinearRegression()\n",
      "\n",
      "Processing.540\n",
      "LinearRegression()\n",
      "\n",
      "Processing.541\n",
      "LinearRegression()\n",
      "\n",
      "Processing.542\n",
      "LinearRegression()\n",
      "\n",
      "Processing.543\n",
      "LinearRegression()\n",
      "\n",
      "Processing.544\n",
      "LinearRegression()\n",
      "\n",
      "Processing.545\n",
      "LinearRegression()\n",
      "\n",
      "Processing.546\n",
      "LinearRegression()\n",
      "\n",
      "Processing.547\n",
      "LinearRegression()\n",
      "\n",
      "Processing.548\n",
      "LinearRegression()\n",
      "\n",
      "Processing.549\n",
      "LinearRegression()\n",
      "\n",
      "Processing.550\n",
      "LinearRegression()\n",
      "\n",
      "Processing.551\n",
      "LinearRegression()\n",
      "\n",
      "Processing.552\n",
      "LinearRegression()\n",
      "\n",
      "Processing.553\n",
      "LinearRegression()\n",
      "\n",
      "Processing.554\n",
      "LinearRegression()\n",
      "\n",
      "Processing.555\n",
      "LinearRegression()\n",
      "\n",
      "Processing.556\n",
      "LinearRegression()\n",
      "\n",
      "Processing.557\n",
      "LinearRegression()\n",
      "\n",
      "Processing.558\n",
      "LinearRegression()\n",
      "\n",
      "Processing.559\n",
      "LinearRegression()\n",
      "\n",
      "Processing.560\n",
      "LinearRegression()\n",
      "\n",
      "Processing.561\n",
      "LinearRegression()\n",
      "\n",
      "Processing.562\n",
      "LinearRegression()\n",
      "\n",
      "Processing.563\n",
      "LinearRegression()\n",
      "\n",
      "Processing.564\n",
      "LinearRegression()\n",
      "\n",
      "Processing.565\n",
      "LinearRegression()\n",
      "\n",
      "Processing.566\n",
      "LinearRegression()\n",
      "\n",
      "Processing.567\n",
      "LinearRegression()\n",
      "\n",
      "Processing.568\n",
      "LinearRegression()\n",
      "\n",
      "Processing.569\n",
      "LinearRegression()\n",
      "\n",
      "Processing.570\n",
      "LinearRegression()\n",
      "\n",
      "Processing.571\n",
      "LinearRegression()\n",
      "\n",
      "Processing.572\n",
      "LinearRegression()\n",
      "\n",
      "Processing.573\n",
      "LinearRegression()\n",
      "\n",
      "Processing.574\n",
      "LinearRegression()\n",
      "\n",
      "Processing.575\n",
      "LinearRegression()\n",
      "\n",
      "Processing.576\n",
      "LinearRegression()\n",
      "\n",
      "Processing.577\n",
      "LinearRegression()\n",
      "\n",
      "Processing.578\n",
      "LinearRegression()\n",
      "\n",
      "Processing.579\n",
      "LinearRegression()\n",
      "\n",
      "Processing.580\n",
      "LinearRegression()\n",
      "\n",
      "Processing.581\n",
      "LinearRegression()\n",
      "\n",
      "Processing.582\n",
      "LinearRegression()\n",
      "\n",
      "Processing.583\n",
      "LinearRegression()\n",
      "\n",
      "Processing.584\n",
      "LinearRegression()\n",
      "\n",
      "Processing.585\n",
      "LinearRegression()\n",
      "\n",
      "Processing.586\n",
      "LinearRegression()\n",
      "\n",
      "Processing.587\n",
      "LinearRegression()\n",
      "\n",
      "Processing.588\n",
      "LinearRegression()\n",
      "\n",
      "Processing.589\n",
      "LinearRegression()\n",
      "\n",
      "Processing.590\n",
      "LinearRegression()\n",
      "\n",
      "Processing.591\n",
      "LinearRegression()\n",
      "\n",
      "Processing.592\n",
      "LinearRegression()\n",
      "\n",
      "Processing.593\n",
      "LinearRegression()\n",
      "\n",
      "Processing.594\n",
      "LinearRegression()\n",
      "\n",
      "Processing.595\n",
      "LinearRegression()\n",
      "\n",
      "Processing.596\n",
      "LinearRegression()\n",
      "\n",
      "Processing.597\n",
      "LinearRegression()\n",
      "\n",
      "Processing.598\n",
      "LinearRegression()\n",
      "\n",
      "Processing.599\n",
      "LinearRegression()\n",
      "\n",
      "Processing.600\n",
      "LinearRegression()\n",
      "\n",
      "Processing.601\n",
      "LinearRegression()\n",
      "\n",
      "Processing.602\n",
      "LinearRegression()\n",
      "\n",
      "Processing.603\n",
      "LinearRegression()\n",
      "\n",
      "Processing.604\n",
      "LinearRegression()\n",
      "\n",
      "Processing.605\n",
      "LinearRegression()\n",
      "\n",
      "Processing.606\n",
      "LinearRegression()\n",
      "\n",
      "Processing.607\n",
      "LinearRegression()\n",
      "\n",
      "Processing.608\n",
      "LinearRegression()\n",
      "\n",
      "Processing.609\n",
      "LinearRegression()\n",
      "\n",
      "Processing.610\n",
      "LinearRegression()\n",
      "\n",
      "Processing.611\n",
      "LinearRegression()\n",
      "\n",
      "Processing.612\n",
      "LinearRegression()\n",
      "\n",
      "Processing.613\n",
      "LinearRegression()\n",
      "\n",
      "Processing.614\n",
      "LinearRegression()\n",
      "\n",
      "Processing.615\n",
      "LinearRegression()\n",
      "\n",
      "Processing.616\n",
      "LinearRegression()\n",
      "\n",
      "Processing.617\n",
      "LinearRegression()\n",
      "\n",
      "Processing.618\n",
      "LinearRegression()\n",
      "\n",
      "Processing.619\n",
      "LinearRegression()\n",
      "\n",
      "Processing.620\n",
      "LinearRegression()\n",
      "\n",
      "Processing.621\n",
      "LinearRegression()\n",
      "\n",
      "Processing.622\n",
      "LinearRegression()\n",
      "\n",
      "Processing.623\n",
      "LinearRegression()\n",
      "\n",
      "Processing.624\n",
      "LinearRegression()\n",
      "\n",
      "Processing.625\n",
      "LinearRegression()\n",
      "\n",
      "Processing.626\n",
      "LinearRegression()\n",
      "\n",
      "Processing.627\n",
      "LinearRegression()\n",
      "\n",
      "Processing.628\n",
      "LinearRegression()\n",
      "\n",
      "Processing.629\n",
      "LinearRegression()\n",
      "\n",
      "Processing.630\n",
      "LinearRegression()\n",
      "\n",
      "Processing.631\n",
      "LinearRegression()\n",
      "\n",
      "Processing.632\n",
      "LinearRegression()\n",
      "\n",
      "Processing.633\n",
      "LinearRegression()\n",
      "\n",
      "Processing.634\n",
      "LinearRegression()\n",
      "\n",
      "Processing.635\n",
      "LinearRegression()\n",
      "\n",
      "Processing.636\n",
      "LinearRegression()\n",
      "\n",
      "Processing.637\n",
      "LinearRegression()\n",
      "\n",
      "Processing.638\n",
      "LinearRegression()\n",
      "\n",
      "Processing.639\n",
      "LinearRegression()\n",
      "\n",
      "Processing.640\n",
      "LinearRegression()\n",
      "\n",
      "Processing.641\n",
      "LinearRegression()\n",
      "\n",
      "Processing.642\n",
      "LinearRegression()\n",
      "\n",
      "Processing.643\n",
      "LinearRegression()\n",
      "\n",
      "Processing.644\n",
      "LinearRegression()\n",
      "\n",
      "Processing.645\n",
      "LinearRegression()\n",
      "\n",
      "Processing.646\n",
      "LinearRegression()\n",
      "\n",
      "Processing.647\n",
      "LinearRegression()\n",
      "\n",
      "Processing.648\n",
      "LinearRegression()\n",
      "\n",
      "Processing.649\n",
      "LinearRegression()\n",
      "\n",
      "Processing.650\n",
      "LinearRegression()\n",
      "\n",
      "Processing.651\n",
      "LinearRegression()\n",
      "\n",
      "Processing.652\n",
      "LinearRegression()\n",
      "\n",
      "Processing.653\n",
      "LinearRegression()\n",
      "\n",
      "Processing.654\n",
      "LinearRegression()\n",
      "\n",
      "Processing.655\n",
      "LinearRegression()\n",
      "\n",
      "Processing.656\n",
      "LinearRegression()\n",
      "\n",
      "Processing.657\n",
      "LinearRegression()\n",
      "\n",
      "Processing.658\n",
      "LinearRegression()\n",
      "\n",
      "Processing.659\n",
      "LinearRegression()\n",
      "\n",
      "Processing.660\n",
      "LinearRegression()\n",
      "\n",
      "Processing.661\n",
      "LinearRegression()\n",
      "\n",
      "Processing.662\n",
      "LinearRegression()\n",
      "\n",
      "Processing.663\n",
      "LinearRegression()\n",
      "\n",
      "Processing.664\n",
      "LinearRegression()\n",
      "\n",
      "Processing.665\n",
      "LinearRegression()\n",
      "\n",
      "Processing.666\n",
      "LinearRegression()\n",
      "\n",
      "Processing.667\n",
      "LinearRegression()\n",
      "\n",
      "Processing.668\n",
      "LinearRegression()\n",
      "\n",
      "Processing.669\n",
      "LinearRegression()\n",
      "\n",
      "Processing.670\n",
      "LinearRegression()\n",
      "\n",
      "Processing.671\n",
      "LinearRegression()\n",
      "\n",
      "Processing.672\n",
      "LinearRegression()\n",
      "\n",
      "Processing.673\n",
      "LinearRegression()\n",
      "\n",
      "Processing.674\n",
      "LinearRegression()\n",
      "\n",
      "Processing.675\n",
      "LinearRegression()\n",
      "\n",
      "Processing.676\n",
      "LinearRegression()\n",
      "\n",
      "Processing.677\n",
      "LinearRegression()\n",
      "\n",
      "Processing.678\n",
      "LinearRegression()\n",
      "\n",
      "Processing.679\n",
      "LinearRegression()\n",
      "\n",
      "Processing.680\n",
      "LinearRegression()\n",
      "\n",
      "Processing.681\n",
      "LinearRegression()\n",
      "\n",
      "Processing.682\n",
      "LinearRegression()\n",
      "\n",
      "Processing.683\n",
      "LinearRegression()\n",
      "\n",
      "Processing.684\n",
      "LinearRegression()\n",
      "\n",
      "Processing.685\n",
      "LinearRegression()\n",
      "\n",
      "Processing.686\n",
      "LinearRegression()\n",
      "\n",
      "Processing.687\n",
      "LinearRegression()\n",
      "\n",
      "Processing.688\n",
      "LinearRegression()\n",
      "\n",
      "Processing.689\n",
      "LinearRegression()\n",
      "\n",
      "Processing.690\n",
      "LinearRegression()\n",
      "\n",
      "Processing.691\n",
      "LinearRegression()\n",
      "\n",
      "Processing.692\n",
      "LinearRegression()\n",
      "\n",
      "Processing.693\n",
      "LinearRegression()\n",
      "\n",
      "Processing.694\n",
      "LinearRegression()\n",
      "\n",
      "Processing.695\n",
      "LinearRegression()\n",
      "\n",
      "Processing.696\n",
      "LinearRegression()\n",
      "\n",
      "Processing.697\n",
      "LinearRegression()\n",
      "\n",
      "Processing.698\n",
      "LinearRegression()\n",
      "\n",
      "Processing.699\n",
      "LinearRegression()\n",
      "\n",
      "Processing.700\n",
      "LinearRegression()\n",
      "\n",
      "Processing.701\n",
      "LinearRegression()\n",
      "\n",
      "Processing.702\n",
      "LinearRegression()\n",
      "\n",
      "Processing.703\n",
      "LinearRegression()\n",
      "\n",
      "Processing.704\n",
      "LinearRegression()\n",
      "\n",
      "Processing.705\n",
      "LinearRegression()\n",
      "\n",
      "Processing.706\n",
      "LinearRegression()\n",
      "\n",
      "Processing.707\n",
      "LinearRegression()\n",
      "\n",
      "Processing.708\n",
      "LinearRegression()\n",
      "\n",
      "Processing.709\n",
      "LinearRegression()\n",
      "\n",
      "Processing.710\n",
      "LinearRegression()\n",
      "\n",
      "Processing.711\n",
      "LinearRegression()\n",
      "\n",
      "Processing.712\n",
      "LinearRegression()\n",
      "\n",
      "Processing.713\n",
      "LinearRegression()\n",
      "\n",
      "Processing.714\n",
      "LinearRegression()\n",
      "\n",
      "Processing.715\n",
      "LinearRegression()\n",
      "\n",
      "Processing.716\n",
      "LinearRegression()\n",
      "\n",
      "Processing.717\n",
      "LinearRegression()\n",
      "\n",
      "Processing.718\n",
      "LinearRegression()\n",
      "\n",
      "Processing.719\n",
      "LinearRegression()\n",
      "\n",
      "Processing.720\n",
      "LinearRegression()\n",
      "\n",
      "Processing.721\n",
      "LinearRegression()\n",
      "\n",
      "Processing.722\n",
      "LinearRegression()\n",
      "\n",
      "Processing.723\n",
      "LinearRegression()\n",
      "\n",
      "Processing.724\n",
      "LinearRegression()\n",
      "\n",
      "Processing.725\n",
      "LinearRegression()\n",
      "\n",
      "Processing.726\n",
      "LinearRegression()\n",
      "\n",
      "Processing.727\n",
      "LinearRegression()\n",
      "\n",
      "Processing.728\n",
      "LinearRegression()\n",
      "\n",
      "Processing.729\n",
      "LinearRegression()\n",
      "\n",
      "Processing.730\n",
      "LinearRegression()\n",
      "\n",
      "Processing.731\n",
      "LinearRegression()\n",
      "\n",
      "Processing.732\n",
      "LinearRegression()\n",
      "\n",
      "Processing.733\n",
      "LinearRegression()\n",
      "\n",
      "Processing.734\n",
      "LinearRegression()\n",
      "\n",
      "Processing.735\n",
      "LinearRegression()\n",
      "\n",
      "Processing.736\n",
      "LinearRegression()\n",
      "\n",
      "Processing.737\n",
      "LinearRegression()\n",
      "\n",
      "Processing.738\n",
      "LinearRegression()\n",
      "\n",
      "Processing.739\n",
      "LinearRegression()\n",
      "\n",
      "Processing.740\n",
      "LinearRegression()\n",
      "\n",
      "Processing.741\n",
      "LinearRegression()\n",
      "\n",
      "Processing.742\n",
      "LinearRegression()\n",
      "\n",
      "Processing.743\n",
      "LinearRegression()\n",
      "\n",
      "Processing.744\n",
      "LinearRegression()\n",
      "\n",
      "Processing.745\n",
      "LinearRegression()\n",
      "\n",
      "Processing.746\n",
      "LinearRegression()\n",
      "\n",
      "Processing.747\n",
      "LinearRegression()\n",
      "\n",
      "Processing.748\n",
      "LinearRegression()\n",
      "\n",
      "Processing.749\n",
      "LinearRegression()\n",
      "\n",
      "Processing.750\n",
      "LinearRegression()\n",
      "\n",
      "Processing.751\n",
      "LinearRegression()\n",
      "\n",
      "Processing.752\n",
      "LinearRegression()\n",
      "\n",
      "Processing.753\n",
      "LinearRegression()\n",
      "\n",
      "Processing.754\n",
      "LinearRegression()\n",
      "\n",
      "Processing.755\n",
      "LinearRegression()\n",
      "\n",
      "Processing.756\n",
      "LinearRegression()\n",
      "\n",
      "Processing.757\n",
      "LinearRegression()\n",
      "\n",
      "Processing.758\n",
      "LinearRegression()\n",
      "\n",
      "Processing.759\n",
      "LinearRegression()\n",
      "\n",
      "Processing.760\n",
      "LinearRegression()\n",
      "\n",
      "Processing.761\n",
      "LinearRegression()\n",
      "\n",
      "Processing.762\n",
      "LinearRegression()\n",
      "\n",
      "Processing.763\n",
      "LinearRegression()\n",
      "\n",
      "Processing.764\n",
      "LinearRegression()\n",
      "\n",
      "Processing.765\n",
      "LinearRegression()\n",
      "\n",
      "Processing.766\n",
      "LinearRegression()\n",
      "\n",
      "Processing.767\n",
      "LinearRegression()\n",
      "\n",
      "Processing.768\n",
      "LinearRegression()\n",
      "\n",
      "Processing.769\n",
      "LinearRegression()\n",
      "\n",
      "Processing.770\n",
      "LinearRegression()\n",
      "\n",
      "Processing.771\n",
      "LinearRegression()\n",
      "\n",
      "Processing.772\n",
      "LinearRegression()\n",
      "\n",
      "Processing.773\n",
      "LinearRegression()\n",
      "\n",
      "Processing.774\n",
      "LinearRegression()\n",
      "\n",
      "Processing.775\n",
      "LinearRegression()\n",
      "\n",
      "Processing.776\n",
      "LinearRegression()\n",
      "\n",
      "Processing.777\n",
      "LinearRegression()\n",
      "\n",
      "Processing.778\n",
      "LinearRegression()\n",
      "\n",
      "Processing.779\n",
      "LinearRegression()\n",
      "\n",
      "Processing.780\n",
      "LinearRegression()\n",
      "\n",
      "Processing.781\n",
      "LinearRegression()\n",
      "\n",
      "Processing.782\n",
      "LinearRegression()\n",
      "\n",
      "Processing.783\n",
      "LinearRegression()\n",
      "\n",
      "Processing.784\n",
      "LinearRegression()\n",
      "\n",
      "Processing.785\n",
      "LinearRegression()\n",
      "\n",
      "Processing.786\n",
      "LinearRegression()\n",
      "\n",
      "Processing.787\n",
      "LinearRegression()\n",
      "\n",
      "Processing.788\n",
      "LinearRegression()\n",
      "\n",
      "Processing.789\n",
      "LinearRegression()\n",
      "\n",
      "Processing.790\n",
      "LinearRegression()\n",
      "\n",
      "Processing.791\n",
      "LinearRegression()\n",
      "\n",
      "Processing.792\n",
      "LinearRegression()\n",
      "\n",
      "Processing.793\n",
      "LinearRegression()\n",
      "\n",
      "Processing.794\n",
      "LinearRegression()\n",
      "\n",
      "Processing.795\n",
      "LinearRegression()\n",
      "\n",
      "Processing.796\n",
      "LinearRegression()\n",
      "\n",
      "Processing.797\n",
      "LinearRegression()\n",
      "\n",
      "Processing.798\n",
      "LinearRegression()\n",
      "\n",
      "Processing.799\n",
      "LinearRegression()\n",
      "\n",
      "Processing.800\n",
      "LinearRegression()\n",
      "\n",
      "Processing.801\n",
      "LinearRegression()\n",
      "\n",
      "Processing.802\n",
      "LinearRegression()\n",
      "\n",
      "Processing.803\n",
      "LinearRegression()\n",
      "\n",
      "Processing.804\n",
      "LinearRegression()\n",
      "\n",
      "Processing.805\n",
      "LinearRegression()\n",
      "\n",
      "Processing.806\n",
      "LinearRegression()\n",
      "\n",
      "Processing.807\n",
      "LinearRegression()\n",
      "\n",
      "Processing.808\n",
      "LinearRegression()\n",
      "\n",
      "Processing.809\n",
      "LinearRegression()\n",
      "\n",
      "Processing.810\n",
      "LinearRegression()\n",
      "\n",
      "Processing.811\n",
      "LinearRegression()\n",
      "\n",
      "Processing.812\n",
      "LinearRegression()\n",
      "\n",
      "Processing.813\n",
      "LinearRegression()\n",
      "\n",
      "Processing.814\n",
      "LinearRegression()\n",
      "\n",
      "Processing.815\n",
      "LinearRegression()\n",
      "\n",
      "Processing.816\n",
      "LinearRegression()\n",
      "\n",
      "Processing.817\n",
      "LinearRegression()\n",
      "\n",
      "Processing.818\n",
      "LinearRegression()\n",
      "\n",
      "Processing.819\n",
      "LinearRegression()\n",
      "\n",
      "Processing.820\n",
      "LinearRegression()\n",
      "\n",
      "Processing.821\n",
      "LinearRegression()\n",
      "\n",
      "Processing.822\n",
      "LinearRegression()\n",
      "\n",
      "Processing.823\n",
      "LinearRegression()\n",
      "\n",
      "Processing.824\n",
      "LinearRegression()\n",
      "\n",
      "Processing.825\n",
      "LinearRegression()\n",
      "\n",
      "Processing.826\n",
      "LinearRegression()\n",
      "\n",
      "Processing.827\n",
      "LinearRegression()\n",
      "\n",
      "Processing.828\n",
      "LinearRegression()\n",
      "\n",
      "Processing.829\n",
      "LinearRegression()\n",
      "\n",
      "Processing.830\n",
      "LinearRegression()\n",
      "\n",
      "Processing.831\n",
      "LinearRegression()\n",
      "\n",
      "Processing.832\n",
      "LinearRegression()\n",
      "\n",
      "Processing.833\n",
      "LinearRegression()\n",
      "\n",
      "Processing.834\n",
      "LinearRegression()\n",
      "\n",
      "Processing.835\n",
      "LinearRegression()\n",
      "\n",
      "Processing.836\n",
      "LinearRegression()\n",
      "\n",
      "Processing.837\n",
      "LinearRegression()\n",
      "\n",
      "Processing.838\n",
      "LinearRegression()\n",
      "\n",
      "Processing.839\n",
      "LinearRegression()\n",
      "\n",
      "Processing.840\n",
      "LinearRegression()\n",
      "\n",
      "Processing.841\n",
      "LinearRegression()\n",
      "\n",
      "Processing.842\n",
      "LinearRegression()\n",
      "\n",
      "Processing.843\n",
      "LinearRegression()\n",
      "\n",
      "Processing.844\n",
      "LinearRegression()\n",
      "\n",
      "Processing.845\n",
      "LinearRegression()\n",
      "\n",
      "Processing.846\n",
      "LinearRegression()\n",
      "\n",
      "Processing.847\n",
      "LinearRegression()\n",
      "\n",
      "Processing.848\n",
      "LinearRegression()\n",
      "\n",
      "Processing.849\n",
      "LinearRegression()\n",
      "\n",
      "Processing.850\n",
      "LinearRegression()\n",
      "\n",
      "Processing.851\n",
      "LinearRegression()\n",
      "\n",
      "Processing.852\n",
      "LinearRegression()\n",
      "\n",
      "Processing.853\n",
      "LinearRegression()\n",
      "\n",
      "Processing.854\n",
      "LinearRegression()\n",
      "\n",
      "Processing.855\n",
      "LinearRegression()\n",
      "\n",
      "Processing.856\n",
      "LinearRegression()\n",
      "\n",
      "Processing.857\n",
      "LinearRegression()\n",
      "\n",
      "Processing.858\n",
      "LinearRegression()\n",
      "\n",
      "Processing.859\n",
      "LinearRegression()\n",
      "\n",
      "Processing.860\n",
      "LinearRegression()\n",
      "\n",
      "Processing.861\n",
      "LinearRegression()\n",
      "\n",
      "Processing.862\n",
      "LinearRegression()\n",
      "\n",
      "Processing.863\n",
      "LinearRegression()\n",
      "\n",
      "Processing.864\n",
      "LinearRegression()\n",
      "\n",
      "Processing.865\n",
      "LinearRegression()\n",
      "\n",
      "Processing.866\n",
      "LinearRegression()\n",
      "\n",
      "Processing.867\n",
      "LinearRegression()\n",
      "\n",
      "Processing.868\n",
      "LinearRegression()\n",
      "\n",
      "Processing.869\n",
      "LinearRegression()\n",
      "\n",
      "Processing.870\n",
      "LinearRegression()\n",
      "\n",
      "Processing.871\n",
      "LinearRegression()\n",
      "\n",
      "Processing.872\n",
      "LinearRegression()\n",
      "\n",
      "Processing.873\n",
      "LinearRegression()\n",
      "\n",
      "Processing.874\n",
      "LinearRegression()\n",
      "\n",
      "Processing.875\n",
      "LinearRegression()\n",
      "\n",
      "Processing.876\n",
      "LinearRegression()\n",
      "\n",
      "Processing.877\n",
      "LinearRegression()\n",
      "\n",
      "Processing.878\n",
      "LinearRegression()\n",
      "\n",
      "Processing.879\n",
      "LinearRegression()\n",
      "\n",
      "Processing.880\n",
      "LinearRegression()\n",
      "\n",
      "Processing.881\n",
      "LinearRegression()\n",
      "\n",
      "Processing.882\n",
      "LinearRegression()\n",
      "\n",
      "Processing.883\n",
      "LinearRegression()\n",
      "\n",
      "Processing.884\n",
      "LinearRegression()\n",
      "\n",
      "Processing.885\n",
      "LinearRegression()\n",
      "\n",
      "Processing.886\n",
      "LinearRegression()\n",
      "\n",
      "Processing.887\n",
      "LinearRegression()\n",
      "\n",
      "Processing.888\n",
      "LinearRegression()\n",
      "\n",
      "Processing.889\n",
      "LinearRegression()\n",
      "\n",
      "Processing.890\n",
      "LinearRegression()\n",
      "\n",
      "Processing.891\n",
      "LinearRegression()\n",
      "\n",
      "Processing.892\n",
      "LinearRegression()\n",
      "\n",
      "Processing.893\n",
      "LinearRegression()\n",
      "\n",
      "Processing.894\n",
      "LinearRegression()\n",
      "\n",
      "Processing.895\n",
      "LinearRegression()\n",
      "\n",
      "Processing.896\n",
      "LinearRegression()\n",
      "\n",
      "Processing.897\n",
      "LinearRegression()\n",
      "\n",
      "Processing.898\n",
      "LinearRegression()\n",
      "\n",
      "Processing.899\n"
     ]
    }
   ],
   "source": [
    "resultat = []\n",
    "i=0 \n",
    "for pid_i in pid:\n",
    "  resultat.append(evaluate_pred_reg(l_reg,train_dataset_2[train_dataset_2['pid']==pid_i].iloc[:,Selected_features], y_train[y_train['pid']==pid_i]['target']))\n",
    "  print(i)\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245539,
     "status": "ok",
     "timestamp": 1613225938061,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "qlyFQCmYRhaC",
    "outputId": "29c2a533-404e-4a0d-d11d-f43166811228"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3702027724976742"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(resultat)\n",
    "#list(train_dataset['pid'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xqi8Zb8XRnvk"
   },
   "source": [
    "50 features 0.39\n",
    "25 features 0.37\n",
    "20 features 0.37\n",
    "15 features 0.36\n",
    "10 features 0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICHE78Ypn178"
   },
   "outputs": [],
   "source": [
    "resultat_df = pd.DataFrame(resultat, columns = ['MSE'])\n",
    "resultat_df['pid'] = pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d80Nc6RQWHFB"
   },
   "outputs": [],
   "source": [
    "bad_pid = list(resultat_df[resultat_df['MSE']>0.4]['pid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245525,
     "status": "ok",
     "timestamp": 1613225938066,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "-_Eh50KAcMBn",
    "outputId": "ff5e12a6-67e7-49f5-bf64-07c46b7ee6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdELK99gSdXR"
   },
   "source": [
    "#### 4.4 Run linear predict for each pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245518,
     "status": "ok",
     "timestamp": 1613225938067,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "qQY0n2BbNFzW",
    "outputId": "df27848a-6502-4924-e57b-bc0b6000e453"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311744"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHyCVwU_SmmY"
   },
   "outputs": [],
   "source": [
    "for pid in list(train_dataset['pid'].unique()):  \n",
    "  l_reg_2 = LinearRegression()\n",
    "  l_reg_2.fit(train_dataset_2[train_dataset_2['pid']==pid].iloc[:,Selected_features], y_train[y_train['pid']==pid]['target'])\n",
    "  y_test = pd.DataFrame(l_reg_2.predict(test_dataset_2[test_dataset_2['pid'] == pid].iloc[:,Selected_features]),columns=['target'])\n",
    "  y_test = y_test.set_index(test_dataset_2[test_dataset_2['pid'] == pid].index)\n",
    "  if pid == 360 :\n",
    "    y_final = y_test\n",
    "  else :\n",
    "    y_final = pd.concat([y_final,y_test], axis = 0)\n",
    "y_final = y_final.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-v4ViCETpX9"
   },
   "source": [
    "### Task 5 : XGboost error prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8m1kTwJpUcM"
   },
   "outputs": [],
   "source": [
    "y_train['day'] = train_dataset['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gy7bQOqTops2"
   },
   "outputs": [],
   "source": [
    "train_X_error, test_X_error, train_y_error, test_y_error = train_test_split(train_dataset_2, y_train, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BLFZIYFondX"
   },
   "outputs": [],
   "source": [
    "date_series = test_X_error['day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDXEi3dCiZZw"
   },
   "outputs": [],
   "source": [
    "pid = list(train_X_error['pid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274032,
     "status": "ok",
     "timestamp": 1613225969032,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "hFm45TIWibbw",
    "outputId": "6cf74d24-6c47-4e10-8a43-a674f9179089"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290777,
     "status": "ok",
     "timestamp": 1613225985785,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "1uzYP7mapLdW",
    "outputId": "e1922ec5-0ef1-4be5-a960-fd80ed439103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "420\n",
      "288\n",
      "526\n",
      "584\n",
      "431\n",
      "211\n",
      "128\n",
      "787\n",
      "800\n",
      "115\n",
      "491\n",
      "360\n",
      "591\n",
      "434\n",
      "522\n",
      "164\n",
      "768\n",
      "445\n",
      "320\n",
      "593\n",
      "603\n",
      "667\n",
      "6\n",
      "629\n",
      "781\n",
      "858\n",
      "429\n",
      "862\n",
      "404\n",
      "334\n",
      "465\n",
      "775\n",
      "459\n",
      "387\n",
      "656\n",
      "699\n",
      "126\n",
      "330\n",
      "482\n",
      "821\n",
      "300\n",
      "783\n",
      "275\n",
      "873\n",
      "177\n",
      "72\n",
      "375\n",
      "143\n",
      "241\n",
      "67\n",
      "368\n",
      "351\n",
      "536\n",
      "289\n",
      "344\n",
      "875\n",
      "296\n",
      "762\n",
      "439\n",
      "730\n",
      "118\n",
      "610\n",
      "163\n",
      "601\n",
      "835\n",
      "56\n",
      "333\n",
      "132\n",
      "273\n",
      "356\n",
      "712\n",
      "42\n",
      "23\n",
      "3\n",
      "637\n",
      "337\n",
      "774\n",
      "406\n",
      "525\n",
      "838\n",
      "548\n",
      "841\n",
      "823\n",
      "581\n",
      "19\n",
      "52\n",
      "598\n",
      "469\n",
      "127\n",
      "702\n",
      "283\n",
      "680\n",
      "579\n",
      "569\n",
      "88\n",
      "694\n",
      "11\n",
      "634\n",
      "236\n",
      "716\n",
      "818\n",
      "711\n",
      "641\n",
      "530\n",
      "287\n",
      "870\n",
      "693\n",
      "407\n",
      "811\n",
      "399\n",
      "379\n",
      "502\n",
      "395\n",
      "717\n",
      "578\n",
      "773\n",
      "57\n",
      "473\n",
      "701\n",
      "367\n",
      "252\n",
      "815\n",
      "223\n",
      "695\n",
      "450\n",
      "721\n",
      "291\n",
      "378\n",
      "137\n",
      "709\n",
      "438\n",
      "377\n",
      "276\n",
      "69\n",
      "74\n",
      "604\n",
      "410\n",
      "323\n",
      "87\n",
      "796\n",
      "310\n",
      "597\n",
      "863\n",
      "311\n",
      "322\n",
      "595\n",
      "529\n",
      "107\n",
      "40\n",
      "867\n",
      "644\n",
      "31\n",
      "899\n",
      "883\n",
      "540\n",
      "170\n",
      "845\n",
      "706\n",
      "200\n",
      "85\n",
      "176\n",
      "890\n",
      "263\n",
      "20\n",
      "562\n",
      "192\n",
      "652\n",
      "130\n",
      "561\n",
      "462\n",
      "669\n",
      "51\n",
      "219\n",
      "878\n",
      "585\n",
      "338\n",
      "91\n",
      "369\n",
      "196\n",
      "93\n",
      "519\n",
      "157\n",
      "755\n",
      "664\n",
      "560\n",
      "658\n",
      "346\n",
      "791\n",
      "175\n",
      "227\n",
      "686\n",
      "278\n",
      "99\n",
      "766\n",
      "405\n",
      "847\n",
      "806\n",
      "770\n",
      "8\n",
      "317\n",
      "831\n",
      "454\n",
      "654\n",
      "495\n",
      "757\n",
      "358\n",
      "659\n",
      "626\n",
      "146\n",
      "851\n",
      "778\n",
      "760\n",
      "371\n",
      "734\n",
      "341\n",
      "708\n",
      "17\n",
      "551\n",
      "691\n",
      "138\n",
      "607\n",
      "476\n",
      "398\n",
      "826\n",
      "879\n",
      "233\n",
      "43\n",
      "411\n",
      "190\n",
      "385\n",
      "872\n",
      "259\n",
      "169\n",
      "882\n",
      "214\n",
      "225\n",
      "83\n",
      "160\n",
      "824\n",
      "627\n",
      "340\n",
      "628\n",
      "417\n",
      "537\n",
      "651\n",
      "850\n",
      "180\n",
      "820\n",
      "150\n",
      "231\n",
      "147\n",
      "498\n",
      "141\n",
      "671\n",
      "240\n",
      "545\n",
      "832\n",
      "123\n",
      "457\n",
      "688\n",
      "661\n",
      "490\n",
      "403\n",
      "419\n",
      "27\n",
      "788\n",
      "361\n",
      "253\n",
      "674\n",
      "242\n",
      "884\n",
      "754\n",
      "179\n",
      "359\n",
      "381\n",
      "18\n",
      "247\n",
      "507\n",
      "96\n",
      "864\n",
      "487\n",
      "504\n",
      "442\n",
      "885\n",
      "206\n",
      "14\n",
      "621\n",
      "782\n",
      "345\n",
      "573\n",
      "750\n",
      "113\n",
      "103\n",
      "586\n",
      "298\n",
      "892\n",
      "517\n",
      "765\n",
      "362\n",
      "553\n",
      "501\n",
      "186\n",
      "612\n",
      "165\n",
      "613\n",
      "302\n",
      "638\n",
      "825\n",
      "28\n",
      "515\n",
      "380\n",
      "489\n",
      "763\n",
      "402\n",
      "565\n",
      "350\n",
      "295\n",
      "262\n",
      "400\n",
      "682\n",
      "881\n",
      "166\n",
      "391\n",
      "769\n",
      "66\n",
      "795\n",
      "780\n",
      "308\n",
      "846\n",
      "497\n",
      "139\n",
      "220\n",
      "810\n",
      "767\n",
      "393\n",
      "348\n",
      "700\n",
      "282\n",
      "814\n",
      "22\n",
      "110\n",
      "557\n",
      "704\n",
      "152\n",
      "195\n",
      "784\n",
      "383\n",
      "819\n",
      "347\n",
      "161\n",
      "25\n",
      "218\n",
      "834\n",
      "49\n",
      "234\n",
      "270\n",
      "257\n",
      "264\n",
      "363\n",
      "707\n",
      "723\n",
      "798\n",
      "460\n",
      "527\n",
      "50\n",
      "514\n",
      "564\n",
      "441\n",
      "600\n",
      "853\n",
      "134\n",
      "316\n",
      "58\n",
      "534\n",
      "305\n",
      "676\n",
      "136\n",
      "26\n",
      "327\n",
      "808\n",
      "86\n",
      "249\n",
      "94\n",
      "325\n",
      "472\n",
      "318\n",
      "887\n",
      "772\n",
      "142\n",
      "173\n",
      "437\n",
      "554\n",
      "313\n",
      "418\n",
      "209\n",
      "486\n",
      "32\n",
      "106\n",
      "552\n",
      "500\n",
      "301\n",
      "449\n",
      "622\n",
      "558\n",
      "443\n",
      "45\n",
      "479\n",
      "267\n",
      "421\n",
      "430\n",
      "447\n",
      "422\n",
      "666\n",
      "738\n",
      "599\n",
      "722\n",
      "424\n",
      "494\n",
      "744\n",
      "10\n",
      "620\n",
      "187\n",
      "79\n",
      "743\n",
      "41\n",
      "785\n",
      "412\n",
      "335\n",
      "148\n",
      "92\n",
      "739\n",
      "856\n",
      "81\n",
      "505\n",
      "827\n",
      "435\n",
      "174\n",
      "746\n",
      "98\n",
      "16\n",
      "759\n",
      "566\n",
      "632\n",
      "724\n",
      "523\n",
      "238\n",
      "224\n",
      "29\n",
      "162\n",
      "159\n",
      "511\n",
      "648\n",
      "452\n",
      "374\n",
      "729\n",
      "639\n",
      "886\n",
      "294\n",
      "0\n",
      "269\n",
      "63\n",
      "484\n",
      "172\n",
      "574\n",
      "7\n",
      "254\n",
      "869\n",
      "580\n",
      "655\n",
      "39\n",
      "271\n",
      "321\n",
      "777\n",
      "605\n",
      "328\n",
      "512\n",
      "475\n",
      "349\n",
      "392\n",
      "592\n",
      "880\n",
      "745\n",
      "740\n",
      "657\n",
      "747\n",
      "805\n",
      "188\n",
      "279\n",
      "268\n",
      "828\n",
      "801\n",
      "101\n",
      "102\n",
      "829\n",
      "568\n",
      "408\n",
      "228\n",
      "797\n",
      "839\n",
      "212\n",
      "199\n",
      "266\n",
      "100\n",
      "70\n",
      "854\n",
      "30\n",
      "703\n",
      "590\n",
      "193\n",
      "451\n",
      "842\n",
      "307\n",
      "649\n",
      "73\n",
      "596\n",
      "731\n",
      "154\n",
      "12\n",
      "284\n",
      "61\n",
      "119\n",
      "645\n",
      "384\n",
      "776\n",
      "518\n",
      "36\n",
      "394\n",
      "245\n",
      "633\n",
      "877\n",
      "237\n",
      "567\n",
      "894\n",
      "444\n",
      "893\n",
      "681\n",
      "520\n",
      "105\n",
      "89\n",
      "277\n",
      "616\n",
      "488\n",
      "201\n",
      "9\n",
      "129\n",
      "608\n",
      "898\n",
      "499\n",
      "749\n",
      "844\n",
      "857\n",
      "222\n",
      "896\n",
      "248\n",
      "265\n",
      "104\n",
      "413\n",
      "532\n",
      "668\n",
      "764\n",
      "733\n",
      "736\n",
      "144\n",
      "822\n",
      "389\n",
      "306\n",
      "409\n",
      "319\n",
      "521\n",
      "261\n",
      "292\n",
      "478\n",
      "550\n",
      "315\n",
      "258\n",
      "640\n",
      "232\n",
      "609\n",
      "155\n",
      "202\n",
      "833\n",
      "397\n",
      "376\n",
      "339\n",
      "871\n",
      "215\n",
      "425\n",
      "725\n",
      "650\n",
      "414\n",
      "689\n",
      "354\n",
      "235\n",
      "849\n",
      "386\n",
      "803\n",
      "285\n",
      "503\n",
      "714\n",
      "121\n",
      "582\n",
      "76\n",
      "477\n",
      "888\n",
      "860\n",
      "753\n",
      "226\n",
      "415\n",
      "135\n",
      "35\n",
      "286\n",
      "684\n",
      "21\n",
      "576\n",
      "120\n",
      "726\n",
      "427\n",
      "243\n",
      "891\n",
      "830\n",
      "124\n",
      "448\n",
      "643\n",
      "556\n",
      "531\n",
      "611\n",
      "239\n",
      "464\n",
      "203\n",
      "874\n",
      "817\n",
      "53\n",
      "312\n",
      "538\n",
      "687\n",
      "77\n",
      "326\n",
      "309\n",
      "167\n",
      "697\n",
      "614\n",
      "587\n",
      "416\n",
      "732\n",
      "198\n",
      "727\n",
      "793\n",
      "541\n",
      "365\n",
      "2\n",
      "204\n",
      "653\n",
      "456\n",
      "189\n",
      "647\n",
      "543\n",
      "71\n",
      "470\n",
      "812\n",
      "280\n",
      "453\n",
      "255\n",
      "624\n",
      "549\n",
      "606\n",
      "895\n",
      "112\n",
      "583\n",
      "184\n",
      "4\n",
      "24\n",
      "868\n",
      "304\n",
      "631\n",
      "168\n",
      "794\n",
      "329\n",
      "46\n",
      "843\n",
      "250\n",
      "324\n",
      "563\n",
      "720\n",
      "572\n",
      "836\n",
      "848\n",
      "742\n",
      "122\n",
      "370\n",
      "426\n",
      "314\n",
      "210\n",
      "185\n",
      "471\n",
      "299\n",
      "662\n",
      "577\n",
      "194\n",
      "493\n",
      "432\n",
      "216\n",
      "761\n",
      "771\n",
      "837\n",
      "636\n",
      "272\n",
      "809\n",
      "13\n",
      "756\n",
      "55\n",
      "594\n",
      "840\n",
      "692\n",
      "117\n",
      "15\n",
      "62\n",
      "458\n",
      "90\n",
      "735\n",
      "68\n",
      "618\n",
      "672\n",
      "151\n",
      "33\n",
      "428\n",
      "467\n",
      "665\n",
      "483\n",
      "366\n",
      "646\n",
      "630\n",
      "617\n",
      "388\n",
      "575\n",
      "423\n",
      "510\n",
      "37\n",
      "718\n",
      "182\n",
      "588\n",
      "542\n",
      "373\n",
      "673\n",
      "357\n",
      "485\n",
      "353\n",
      "792\n",
      "84\n",
      "466\n",
      "642\n",
      "807\n",
      "524\n",
      "741\n",
      "713\n",
      "589\n",
      "111\n",
      "571\n",
      "623\n",
      "802\n",
      "181\n",
      "804\n",
      "861\n",
      "131\n",
      "737\n",
      "547\n",
      "461\n",
      "125\n",
      "446\n",
      "116\n",
      "696\n",
      "133\n",
      "65\n",
      "748\n",
      "752\n",
      "690\n",
      "48\n",
      "663\n",
      "343\n",
      "516\n",
      "213\n",
      "660\n",
      "509\n",
      "789\n",
      "401\n",
      "5\n",
      "336\n",
      "390\n",
      "492\n",
      "859\n",
      "244\n",
      "710\n",
      "496\n",
      "440\n",
      "463\n",
      "480\n",
      "59\n",
      "342\n",
      "615\n",
      "506\n",
      "178\n",
      "559\n",
      "191\n",
      "281\n",
      "535\n",
      "382\n",
      "555\n",
      "396\n",
      "331\n",
      "230\n",
      "246\n",
      "786\n",
      "779\n",
      "619\n",
      "274\n",
      "685\n",
      "533\n",
      "546\n",
      "865\n",
      "153\n",
      "75\n",
      "433\n",
      "352\n",
      "156\n",
      "719\n",
      "1\n",
      "97\n",
      "293\n",
      "751\n",
      "171\n",
      "251\n",
      "60\n",
      "372\n",
      "625\n",
      "80\n",
      "635\n",
      "468\n",
      "816\n",
      "889\n",
      "436\n",
      "114\n",
      "602\n",
      "715\n",
      "513\n",
      "364\n",
      "95\n",
      "109\n",
      "679\n",
      "866\n",
      "897\n",
      "705\n",
      "544\n",
      "303\n",
      "876\n",
      "207\n",
      "64\n",
      "728\n",
      "528\n",
      "297\n",
      "813\n",
      "790\n",
      "140\n",
      "145\n",
      "677\n",
      "197\n",
      "474\n",
      "855\n",
      "38\n",
      "355\n",
      "158\n",
      "670\n",
      "481\n",
      "108\n",
      "34\n",
      "260\n",
      "217\n",
      "678\n",
      "78\n",
      "799\n",
      "54\n",
      "221\n",
      "455\n",
      "208\n",
      "44\n",
      "332\n",
      "675\n",
      "698\n",
      "82\n",
      "570\n",
      "149\n",
      "47\n",
      "758\n",
      "290\n",
      "205\n",
      "852\n",
      "539\n",
      "683\n",
      "508\n",
      "229\n",
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "for pid_id in pid:  \n",
    "  print(pid_id)\n",
    "  l_reg_error = LinearRegression()\n",
    "  l_reg_error.fit(train_X_error[train_X_error['pid']==pid_id].iloc[:,Selected_features], train_y_error[train_y_error['pid']==pid_id]['target'])\n",
    "  y_test_error = pd.DataFrame(l_reg_error.predict(test_X_error[test_X_error['pid'] == pid_id].iloc[:,Selected_features]),columns=['target'])\n",
    "  y_test_error = y_test_error.set_index(test_X_error[test_X_error['pid'] == pid_id].index)\n",
    "  if pid_id == pid[0] :\n",
    "      y_final_error = y_test_error\n",
    "  else :\n",
    "      y_final_error = pd.concat([y_final_error,y_test_error], axis = 0)\n",
    "\n",
    "y_final_error = y_final_error.sort_index()\n",
    "test_X_error['predict'] = y_final_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290775,
     "status": "ok",
     "timestamp": 1613225985788,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "y5XMdEKPqMGy",
    "outputId": "a0ea1662-78d0-49a7-ad7a-dd0deddf4c90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3777353663860586"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_squared_error(test_y_error['target'], test_X_error['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeqDf8w_riUu"
   },
   "outputs": [],
   "source": [
    "y_test_error = pd.DataFrame(y_test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290763,
     "status": "ok",
     "timestamp": 1613225985791,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "sBUTmQdFrlZK",
    "outputId": "b4d72fba-5878-46de-b2b7-8fe13e29c016"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_X_error['error']=test_y_error[\"target\"] - test_X_error['predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb4KF-q3rr7R"
   },
   "outputs": [],
   "source": [
    "min_date = date_series.min()\n",
    "max_date = date_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290754,
     "status": "ok",
     "timestamp": 1613225985794,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "BnLNl4nirxCl",
    "outputId": "8146ec0d-fbca-4b82-c6d8-649c3c4fc89d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = list(range(min_date, max_date + 1))\n",
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kis7HLQir6HG"
   },
   "outputs": [],
   "source": [
    "X_error_train = test_X_error[test_X_error['day']<=600].drop(['error'], axis = 1)\n",
    "y_error_train = test_X_error[test_X_error['day']<=600]['error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGGsSFxar6-w"
   },
   "outputs": [],
   "source": [
    "X_error_test = test_X_error[test_X_error['day']>600].drop(['error'], axis = 1)\n",
    "y_error_test = test_X_error[test_X_error['day']>600]['error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rGWnzoVqFJU"
   },
   "outputs": [],
   "source": [
    "Selected_features2 = [125, 497, 496, 506, 495, 126, 127, 371, 249, 494,   0, 499,  63,\n",
    "        17,  11,507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDnHpzWnqs8W"
   },
   "outputs": [],
   "source": [
    "X_error_train['median_target_dummy'] = X_error_train['median_target_dummy'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293144,
     "status": "ok",
     "timestamp": 1613225988216,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "1kzQjrPIstow",
    "outputId": "c0509a23-e129-4b84-eb9b-2c01c96f5bfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99589    -0.236138\n",
       "535740   -1.854041\n",
       "228479    2.750132\n",
       "214338    0.557808\n",
       "242520    0.979978\n",
       "            ...   \n",
       "343544    0.557250\n",
       "646428    0.265962\n",
       "142340   -0.130043\n",
       "143115   -0.377147\n",
       "446301    0.105719\n",
       "Name: error, Length: 342241, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_error.iloc[:,Selected_features2]\n",
    "test_X_error['error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWkCFlZus96p"
   },
   "outputs": [],
   "source": [
    "def learning_rate_010_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_010_decay_power_0995(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_005_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.05\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ag4RiOpArlxl"
   },
   "outputs": [],
   "source": [
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'neg_mean_squared_error', \n",
    "            \"eval_set\" : [(X_error_test.iloc[:,Selected_features2],y_error_test)],\n",
    "            'eval_names': ['valid'],\n",
    "            'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature':'auto'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dinIrINWr3AN"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHg6fKBXun0U"
   },
   "outputs": [],
   "source": [
    "model = LGBMRegressor( random_state=314, n_estimators=1000, device='gpu')\n",
    "tscv = TimeSeriesSplit(n_splits=2)\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=model, \n",
    "    param_distributions=param_test, \n",
    "    n_iter= n_HP_points_to_test,\n",
    "    scoring= 'neg_mean_squared_error',\n",
    "    cv= tscv,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "executionInfo": {
     "elapsed": 293527,
     "status": "error",
     "timestamp": 1613225988625,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "VxQ9GK2wtvqX",
    "outputId": "99de6bd9-41a6-4f48-d46c-9fd4c11ecd05"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ed0585547c05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#gs.fit(X_error_train.iloc[:,Selected_features2], y_error_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best score reached: {} with params: {} '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "#gs.fit(X_error_train.iloc[:,Selected_features2], y_error_train)\n",
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uz2lP7cN9srM"
   },
   "outputs": [],
   "source": [
    "opt_parameters = {'colsample_bytree': 0.7076074093370144, \n",
    "                  'min_child_samples': 105, \n",
    "                  'min_child_weight': 1e-05, \n",
    "                  'num_leaves': 26, \n",
    "                  'reg_alpha': 5, \n",
    "                  'reg_lambda': 5, \n",
    "                  'subsample': 0.7468773130235173} \n",
    "                  \n",
    "LGBM_final = LGBMRegressor(**model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WppbCLawr_Wz"
   },
   "outputs": [],
   "source": [
    "evaluate_pred_reg(LGBM_final, test_X_error.iloc[:,Selected_features2], test_X_error['error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgYtAJWSpSi1"
   },
   "source": [
    "Add error just for bad predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nxIhBtUTITP"
   },
   "outputs": [],
   "source": [
    "test_dataset_2['predict']  = y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ojp-Q2fUQ75"
   },
   "outputs": [],
   "source": [
    "test_dataset_2['median_target_dummy']=test_dataset_2['median_target_dummy'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgAKUXEASnJ9"
   },
   "outputs": [],
   "source": [
    "test_dataset_2['error'] = LGBM_final.predict(test_dataset_2.iloc[:,Selected_features2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsT77BAaXjsO"
   },
   "outputs": [],
   "source": [
    "pid_test = list(test_dataset_2['pid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq5na-SDWzqf"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for pid_id in pid_test:  \n",
    "  print(i)\n",
    "  if pid_id == pid_test[0] :\n",
    "    #if pid_id in bad_pid :\n",
    "     # y_predict_final = test_dataset_2[test_dataset_2['pid'] == pid_id]['predict'] + test_dataset_2[test_dataset_2['pid'] == pid_id]['error']\n",
    "    #else :\n",
    "    y_predict_final= test_dataset_2[test_dataset_2['pid'] == pid_id]['predict']\n",
    "    y_predict_final['index'] = test_dataset_2[test_dataset_2['pid'] == pid_id].index\n",
    "  else :\n",
    "    if pid_id in bad_pid :\n",
    "      y_predict = test_dataset_2[test_dataset_2['pid'] == pid_id]['predict'] \n",
    "      y_predict = y_predict +  test_dataset_2[test_dataset_2['pid'] == pid_id]['error']\n",
    "    else :\n",
    "      y_predict = test_dataset_2[test_dataset_2['pid'] == pid_id]['predict']\n",
    "   \n",
    "    y_predict['index'] = test_dataset_2[test_dataset_2['pid'] == pid_id].index\n",
    "    y_predict_final = pd.concat([y_predict_final,y_predict], axis = 0)\n",
    "  i +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ9Q-T0jTzmZ"
   },
   "source": [
    "### Task 6 : Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1RvnjpQphHo"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6_1g-iRU4Hs"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv_umNSXrlaL"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43Yh-Pf1wE8M"
   },
   "source": [
    "## Step 4 : Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4YnpaE0Lrys"
   },
   "source": [
    "### Task 1 Loading libraries and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK9s9S4aLry2"
   },
   "source": [
    "#### 1.1 Lib and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "executionInfo": {
     "elapsed": 727,
     "status": "ok",
     "timestamp": 1613318511149,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "m6R7B15qLry2"
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "executionInfo": {
     "elapsed": 4122,
     "status": "error",
     "timestamp": 1613318515166,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "iLMDAmmOLry4",
    "outputId": "5f2b77a6-b393-4d18-cff2-9d26aee8c83a"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-226-7519f2973ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure_factory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_subplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/plotly/figure_factory/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moptional_imports\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skimage\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ternary_contour\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_ternary_contour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/plotly/figure_factory/_ternary_contour.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mscipy_interp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_imports\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scipy.interpolate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# -------------------------- Layout ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skimage/measure/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msimple_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompare_mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_nrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare_psnr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_structural_similarity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompare_ssim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_polygon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapproximate_polygon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdivide_polygon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpnpoly\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpoints_in_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_points_in_poly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m from ._moments import (moments, moments_central, moments_coords,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skimage/measure/_polygon.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapproximate_polygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/signal/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbsplines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfilter_design\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfir_filter_design\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/signal/bsplines.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloat_factorial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m __all__ = ['spline_filter', 'bspline', 'gauss_spline', 'cubic', 'quadratic',\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'float_factorial'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import delayed\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score \n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sys\n",
    "from numpy import mean\n",
    "import pickle\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble  import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import preprocessing\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuJ37cT_Lry5"
   },
   "outputs": [],
   "source": [
    "data_dir  = \"/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/dataset/clean_dataset\"\n",
    "data_list = glob.glob(os.path.join(data_dir, '**.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vbJ-f0TLry5"
   },
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"%s/train_dataset.csv\" % data_dir, sep=\",\")\n",
    "test_dataset = pd.read_csv(\"%s/test_dataset.csv\" % data_dir, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "executionInfo": {
     "elapsed": 163730,
     "status": "ok",
     "timestamp": 1613308851607,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "MbR2HaJxjRRe",
    "outputId": "f05677bd-37c6-41c7-b8df-f3d73c88b102"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>ID</th>\n",
       "      <th>pid</th>\n",
       "      <th>abs_ret0</th>\n",
       "      <th>abs_ret1</th>\n",
       "      <th>abs_ret2</th>\n",
       "      <th>abs_ret3</th>\n",
       "      <th>abs_ret4</th>\n",
       "      <th>abs_ret5</th>\n",
       "      <th>abs_ret6</th>\n",
       "      <th>abs_ret7</th>\n",
       "      <th>abs_ret8</th>\n",
       "      <th>abs_ret9</th>\n",
       "      <th>abs_ret10</th>\n",
       "      <th>abs_ret11</th>\n",
       "      <th>abs_ret12</th>\n",
       "      <th>abs_ret13</th>\n",
       "      <th>abs_ret14</th>\n",
       "      <th>abs_ret15</th>\n",
       "      <th>abs_ret16</th>\n",
       "      <th>abs_ret17</th>\n",
       "      <th>abs_ret18</th>\n",
       "      <th>abs_ret19</th>\n",
       "      <th>abs_ret20</th>\n",
       "      <th>abs_ret21</th>\n",
       "      <th>abs_ret22</th>\n",
       "      <th>abs_ret23</th>\n",
       "      <th>abs_ret24</th>\n",
       "      <th>abs_ret25</th>\n",
       "      <th>abs_ret26</th>\n",
       "      <th>abs_ret27</th>\n",
       "      <th>abs_ret28</th>\n",
       "      <th>abs_ret29</th>\n",
       "      <th>abs_ret30</th>\n",
       "      <th>abs_ret31</th>\n",
       "      <th>abs_ret32</th>\n",
       "      <th>abs_ret33</th>\n",
       "      <th>abs_ret34</th>\n",
       "      <th>abs_ret35</th>\n",
       "      <th>abs_ret36</th>\n",
       "      <th>...</th>\n",
       "      <th>median_day_abs_ret36</th>\n",
       "      <th>median_day_abs_ret37</th>\n",
       "      <th>median_day_abs_ret38</th>\n",
       "      <th>median_day_abs_ret39</th>\n",
       "      <th>median_day_abs_ret40</th>\n",
       "      <th>median_day_abs_ret41</th>\n",
       "      <th>median_day_abs_ret42</th>\n",
       "      <th>median_day_abs_ret43</th>\n",
       "      <th>median_day_abs_ret44</th>\n",
       "      <th>median_day_abs_ret45</th>\n",
       "      <th>median_day_abs_ret46</th>\n",
       "      <th>median_day_abs_ret47</th>\n",
       "      <th>median_day_abs_ret48</th>\n",
       "      <th>median_day_abs_ret49</th>\n",
       "      <th>median_day_abs_ret50</th>\n",
       "      <th>median_day_abs_ret51</th>\n",
       "      <th>median_day_abs_ret52</th>\n",
       "      <th>median_day_abs_ret53</th>\n",
       "      <th>median_day_abs_ret54</th>\n",
       "      <th>median_day_abs_ret55</th>\n",
       "      <th>median_day_abs_ret56</th>\n",
       "      <th>median_day_abs_ret57</th>\n",
       "      <th>median_day_abs_ret58</th>\n",
       "      <th>median_day_abs_ret59</th>\n",
       "      <th>median_day_abs_ret60</th>\n",
       "      <th>min_ret</th>\n",
       "      <th>max_ret</th>\n",
       "      <th>std_ret</th>\n",
       "      <th>median_ret</th>\n",
       "      <th>sum_ret</th>\n",
       "      <th>min_vol</th>\n",
       "      <th>max_vol</th>\n",
       "      <th>std_vol</th>\n",
       "      <th>median_vol</th>\n",
       "      <th>median_day_sum_ret</th>\n",
       "      <th>median_day_sum_ret_before</th>\n",
       "      <th>kmeans_cluster_median_day_sum_ret_before</th>\n",
       "      <th>abs_kmeans_cluster_median_day_sum_ret_before</th>\n",
       "      <th>median_target_dummy</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073260</td>\n",
       "      <td>0.073265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036601</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>0.029261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073206</td>\n",
       "      <td>0.032942</td>\n",
       "      <td>0.036609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>0.036643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036630</td>\n",
       "      <td>0.007326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.036627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102399</td>\n",
       "      <td>0.022135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.739680</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.076994</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>1.120654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.403606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>444</td>\n",
       "      <td>203</td>\n",
       "      <td>0.176289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110302</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>0.044307</td>\n",
       "      <td>0.154902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044405</td>\n",
       "      <td>0.088692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>0.022139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066357</td>\n",
       "      <td>0.066372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066386</td>\n",
       "      <td>0.088633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218818</td>\n",
       "      <td>0.047647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.878094</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>0.135543</td>\n",
       "      <td>0.022036</td>\n",
       "      <td>0.009472</td>\n",
       "      <td>1.120654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.193810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>592</td>\n",
       "      <td>398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027337</td>\n",
       "      <td>0.109649</td>\n",
       "      <td>0.027345</td>\n",
       "      <td>0.027255</td>\n",
       "      <td>0.054675</td>\n",
       "      <td>0.054585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027307</td>\n",
       "      <td>0.082079</td>\n",
       "      <td>0.027375</td>\n",
       "      <td>0.054675</td>\n",
       "      <td>0.082147</td>\n",
       "      <td>0.041051</td>\n",
       "      <td>0.054780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027322</td>\n",
       "      <td>0.027360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054750</td>\n",
       "      <td>0.054735</td>\n",
       "      <td>0.027367</td>\n",
       "      <td>0.068428</td>\n",
       "      <td>0.027337</td>\n",
       "      <td>0.013669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109649</td>\n",
       "      <td>0.028894</td>\n",
       "      <td>0.020502</td>\n",
       "      <td>1.492592</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.056237</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>0.013988</td>\n",
       "      <td>1.120654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.387511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>740</td>\n",
       "      <td>258</td>\n",
       "      <td>0.063403</td>\n",
       "      <td>0.031752</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>0.063573</td>\n",
       "      <td>0.006350</td>\n",
       "      <td>0.076316</td>\n",
       "      <td>0.025436</td>\n",
       "      <td>0.076423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050988</td>\n",
       "      <td>0.012767</td>\n",
       "      <td>0.038324</td>\n",
       "      <td>0.012765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012767</td>\n",
       "      <td>0.025569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025507</td>\n",
       "      <td>0.025523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038260</td>\n",
       "      <td>0.025484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012731</td>\n",
       "      <td>0.025458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102119</td>\n",
       "      <td>0.021687</td>\n",
       "      <td>0.012723</td>\n",
       "      <td>1.120654</td>\n",
       "      <td>0.006477</td>\n",
       "      <td>0.038170</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.015730</td>\n",
       "      <td>1.120654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.467485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>888</td>\n",
       "      <td>444</td>\n",
       "      <td>0.063318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042230</td>\n",
       "      <td>0.021155</td>\n",
       "      <td>0.063532</td>\n",
       "      <td>0.106270</td>\n",
       "      <td>0.042535</td>\n",
       "      <td>0.063939</td>\n",
       "      <td>0.063735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021268</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.021259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063816</td>\n",
       "      <td>0.063884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021299</td>\n",
       "      <td>0.021295</td>\n",
       "      <td>0.021254</td>\n",
       "      <td>0.021254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042526</td>\n",
       "      <td>0.021249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021245</td>\n",
       "      <td>0.021263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106270</td>\n",
       "      <td>0.024410</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>1.284760</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.082588</td>\n",
       "      <td>0.013899</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>1.120654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.380318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 509 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   day   ID  ...  median_target_dummy    target\n",
       "0    0  148  ...                    0 -3.403606\n",
       "1    0  444  ...                    0 -2.193810\n",
       "2    0  592  ...                    0 -2.387511\n",
       "3    0  740  ...                    0 -2.467485\n",
       "4    0  888  ...                    0 -2.380318\n",
       "\n",
       "[5 rows x 509 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOB7IePZLry6"
   },
   "source": [
    "#### 1.2 Store ID "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIH9CZPLLry7"
   },
   "source": [
    "We don't need ID in ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntcTFKxFLry8"
   },
   "outputs": [],
   "source": [
    "ID_train=train_dataset['ID']\n",
    "ID_test=test_dataset['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcnRNqwkLry9"
   },
   "outputs": [],
   "source": [
    "train_dataset=train_dataset.drop(\"ID\",axis=1)\n",
    "test_dataset=test_dataset.drop(\"ID\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIOGWllHLrzF"
   },
   "source": [
    "#### 1.4 Store target and pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X17GZa2zLrzF"
   },
   "outputs": [],
   "source": [
    "y_train = train_dataset[['pid','target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGO9vGub2Qbp"
   },
   "source": [
    "### Task 2. LTSM Seq2Seq with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-nw9S8zj2Me"
   },
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXCQzOEZj2Mc"
   },
   "outputs": [],
   "source": [
    "#features = [c for c in train_dataset.columns if ((c !=\"target\") &(c !=\"pid\") & (c !=\"day\") & (c !=\"median_target_dummy\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uegI8-6j2Mf"
   },
   "outputs": [],
   "source": [
    "#scaler = preprocessing.StandardScaler()\n",
    "#train_dataset[features] = scaler.fit_transform(train_dataset[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXtMFhyw9lQe"
   },
   "outputs": [],
   "source": [
    "#test_dataset[features]=scaler.fit_transform(test_dataset[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNzAadkkiEbJ"
   },
   "source": [
    "#### Min Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8t_d84ziEbL"
   },
   "outputs": [],
   "source": [
    "features = [c for c in train_dataset.columns if ((c !=\"target\") & (c !=\"pid\") & (c !=\"day\") & (c !=\"median_target_dummy\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88_Sv9uUiEbL"
   },
   "outputs": [],
   "source": [
    "x_scaler = preprocessing.MinMaxScaler((-1,1))\n",
    "train_dataset[features] = x_scaler.fit_transform(train_dataset[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKASjyuLUWZI"
   },
   "outputs": [],
   "source": [
    "target = [c for c in train_dataset.columns if (c ==\"target\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_A4toj1LT7o7"
   },
   "outputs": [],
   "source": [
    "y_scaler = preprocessing.MinMaxScaler((-1,1))\n",
    "train_dataset['target'] = y_scaler.fit_transform(train_dataset[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LURNVJkLSdtr"
   },
   "outputs": [],
   "source": [
    "#test_dataset['target']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1usbrKkiEbM"
   },
   "outputs": [],
   "source": [
    "test_dataset[features]=x_scaler.fit_transform(test_dataset[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWnxvPc5UlEc"
   },
   "outputs": [],
   "source": [
    "#test_dataset[target]=y_scaler.fit_transform(test_dataset[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRxNdN4w3oe1"
   },
   "source": [
    "##### Pid selection before stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hCloiLa3shV"
   },
   "outputs": [],
   "source": [
    "pid_selection = pd.DataFrame(train_dataset['pid'].value_counts()).sort_values(by=['pid'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1511,
     "status": "ok",
     "timestamp": 1613245620416,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "y1uKy91A39MQ",
    "outputId": "f70c4243-20e1-4d67-ac60-142b335b2a47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "346+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7ZsmJlC3wyq"
   },
   "outputs": [],
   "source": [
    "pid_selection=list(pid_selection[pid_selection['pid']>790].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1613245622858,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "SzDofStR4kwv",
    "outputId": "99bd4ef0-b671-4633-c0ec-8c27a1a0602f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[214,\n",
       " 663,\n",
       " 291,\n",
       " 612,\n",
       " 710,\n",
       " 230,\n",
       " 542,\n",
       " 450,\n",
       " 763,\n",
       " 9,\n",
       " 452,\n",
       " 195,\n",
       " 473,\n",
       " 722,\n",
       " 716,\n",
       " 718,\n",
       " 712,\n",
       " 756,\n",
       " 462,\n",
       " 471,\n",
       " 496,\n",
       " 499,\n",
       " 703,\n",
       " 487,\n",
       " 603,\n",
       " 93,\n",
       " 616,\n",
       " 872,\n",
       " 96,\n",
       " 97,\n",
       " 304,\n",
       " 357,\n",
       " 610,\n",
       " 308,\n",
       " 309,\n",
       " 695,\n",
       " 333,\n",
       " 541,\n",
       " 500,\n",
       " 216,\n",
       " 25,\n",
       " 244,\n",
       " 810,\n",
       " 1,\n",
       " 255,\n",
       " 260,\n",
       " 204,\n",
       " 478,\n",
       " 258,\n",
       " 227,\n",
       " 738,\n",
       " 514,\n",
       " 505,\n",
       " 2,\n",
       " 513,\n",
       " 14,\n",
       " 551,\n",
       " 259,\n",
       " 431,\n",
       " 44,\n",
       " 209,\n",
       " 771,\n",
       " 202,\n",
       " 562,\n",
       " 468,\n",
       " 17,\n",
       " 734,\n",
       " 501,\n",
       " 777,\n",
       " 775,\n",
       " 512,\n",
       " 265,\n",
       " 48,\n",
       " 425,\n",
       " 776,\n",
       " 264,\n",
       " 876,\n",
       " 203,\n",
       " 89,\n",
       " 588,\n",
       " 591,\n",
       " 792,\n",
       " 66,\n",
       " 413,\n",
       " 59,\n",
       " 412,\n",
       " 269,\n",
       " 693,\n",
       " 270,\n",
       " 181,\n",
       " 180,\n",
       " 783,\n",
       " 405,\n",
       " 572,\n",
       " 789,\n",
       " 589,\n",
       " 378,\n",
       " 177,\n",
       " 585,\n",
       " 369,\n",
       " 54,\n",
       " 893,\n",
       " 386,\n",
       " 68,\n",
       " 793,\n",
       " 581,\n",
       " 895,\n",
       " 794,\n",
       " 183,\n",
       " 795,\n",
       " 381,\n",
       " 565,\n",
       " 584,\n",
       " 73,\n",
       " 573,\n",
       " 796,\n",
       " 688,\n",
       " 627,\n",
       " 126,\n",
       " 850,\n",
       " 836,\n",
       " 225,\n",
       " 839,\n",
       " 336,\n",
       " 843,\n",
       " 348,\n",
       " 349,\n",
       " 778,\n",
       " 874,\n",
       " 676,\n",
       " 804,\n",
       " 574,\n",
       " 293,\n",
       " 618,\n",
       " 846,\n",
       " 323,\n",
       " 340,\n",
       " 395,\n",
       " 788,\n",
       " 65,\n",
       " 289,\n",
       " 883,\n",
       " 290,\n",
       " 736,\n",
       " 345,\n",
       " 344,\n",
       " 832,\n",
       " 624,\n",
       " 321,\n",
       " 107,\n",
       " 834,\n",
       " 443,\n",
       " 416,\n",
       " 410,\n",
       " 434,\n",
       " 400,\n",
       " 558,\n",
       " 50,\n",
       " 561,\n",
       " 406,\n",
       " 569,\n",
       " 64,\n",
       " 402,\n",
       " 549,\n",
       " 37,\n",
       " 63,\n",
       " 576,\n",
       " 383,\n",
       " 393,\n",
       " 394,\n",
       " 446,\n",
       " 445,\n",
       " 882,\n",
       " 21,\n",
       " 368,\n",
       " 374,\n",
       " 845,\n",
       " 82,\n",
       " 453,\n",
       " 86,\n",
       " 598,\n",
       " 881,\n",
       " 540,\n",
       " 456,\n",
       " 604,\n",
       " 878,\n",
       " 601,\n",
       " 27,\n",
       " 47,\n",
       " 459,\n",
       " 460,\n",
       " 463,\n",
       " 24,\n",
       " 577,\n",
       " 611,\n",
       " 389,\n",
       " 899,\n",
       " 342,\n",
       " 10,\n",
       " 116,\n",
       " 485,\n",
       " 115,\n",
       " 103,\n",
       " 864,\n",
       " 334,\n",
       " 530,\n",
       " 868,\n",
       " 335,\n",
       " 355,\n",
       " 528,\n",
       " 484,\n",
       " 101,\n",
       " 3,\n",
       " 526,\n",
       " 15,\n",
       " 30,\n",
       " 628,\n",
       " 343,\n",
       " 491,\n",
       " 848,\n",
       " 579,\n",
       " 442,\n",
       " 70,\n",
       " 531,\n",
       " 90,\n",
       " 427,\n",
       " 524,\n",
       " 862,\n",
       " 341,\n",
       " 623,\n",
       " 337,\n",
       " 350,\n",
       " 859,\n",
       " 110,\n",
       " 13,\n",
       " 869,\n",
       " 847,\n",
       " 111,\n",
       " 593,\n",
       " 163,\n",
       " 77,\n",
       " 191,\n",
       " 132,\n",
       " 828,\n",
       " 648,\n",
       " 231,\n",
       " 140,\n",
       " 743,\n",
       " 727,\n",
       " 816,\n",
       " 806,\n",
       " 750,\n",
       " 277,\n",
       " 171,\n",
       " 282,\n",
       " 252,\n",
       " 764,\n",
       " 253,\n",
       " 281,\n",
       " 791,\n",
       " 254,\n",
       " 217,\n",
       " 319,\n",
       " 208,\n",
       " 656,\n",
       " 31,\n",
       " 823,\n",
       " 307,\n",
       " 822,\n",
       " 724,\n",
       " 820,\n",
       " 146,\n",
       " 653,\n",
       " 817,\n",
       " 643,\n",
       " 696,\n",
       " 313,\n",
       " 652,\n",
       " 119,\n",
       " 120,\n",
       " 122,\n",
       " 638,\n",
       " 733,\n",
       " 685,\n",
       " 329,\n",
       " 190,\n",
       " 154,\n",
       " 669,\n",
       " 672,\n",
       " 674,\n",
       " 681,\n",
       " 237,\n",
       " 803,\n",
       " 714,\n",
       " 678,\n",
       " 167,\n",
       " 679,\n",
       " 168,\n",
       " 245,\n",
       " 286,\n",
       " 175,\n",
       " 735,\n",
       " 844,\n",
       " 359,\n",
       " 711,\n",
       " 587,\n",
       " 752,\n",
       " 287,\n",
       " 664,\n",
       " 182,\n",
       " 630,\n",
       " 700,\n",
       " 240,\n",
       " 687,\n",
       " 176,\n",
       " 188,\n",
       " 261,\n",
       " 773,\n",
       " 262,\n",
       " 179,\n",
       " 200,\n",
       " 659,\n",
       " 751,\n",
       " 673,\n",
       " 149,\n",
       " 709,\n",
       " 150,\n",
       " 207,\n",
       " 797,\n",
       " 292,\n",
       " 317,\n",
       " 390,\n",
       " 316,\n",
       " 800,\n",
       " 802,\n",
       " 801,\n",
       " 288,\n",
       " 428,\n",
       " 294,\n",
       " 347,\n",
       " 364,\n",
       " 857,\n",
       " 372,\n",
       " 371,\n",
       " 861,\n",
       " 370,\n",
       " 366,\n",
       " 365,\n",
       " 354,\n",
       " 363,\n",
       " 362,\n",
       " 356,\n",
       " 358,\n",
       " 280,\n",
       " 870,\n",
       " 283,\n",
       " 812,\n",
       " 276,\n",
       " 819,\n",
       " 360,\n",
       " 779,\n",
       " 809,\n",
       " 419,\n",
       " 418,\n",
       " 299,\n",
       " 811,\n",
       " 300,\n",
       " 302,\n",
       " 404,\n",
       " 403,\n",
       " 399,\n",
       " 821,\n",
       " 787,\n",
       " 312,\n",
       " 826,\n",
       " 396,\n",
       " 391,\n",
       " 422,\n",
       " 807,\n",
       " 423,\n",
       " 435,\n",
       " 447,\n",
       " 444,\n",
       " 441,\n",
       " 275,\n",
       " 871,\n",
       " 489,\n",
       " 858,\n",
       " 853,\n",
       " 477,\n",
       " 235,\n",
       " 747,\n",
       " 482,\n",
       " 744,\n",
       " 494,\n",
       " 236,\n",
       " 748,\n",
       " 749,\n",
       " 238,\n",
       " 239,\n",
       " 241,\n",
       " 451,\n",
       " 754,\n",
       " 490,\n",
       " 465,\n",
       " 242,\n",
       " 232,\n",
       " 774,\n",
       " 506,\n",
       " 457,\n",
       " 257,\n",
       " 769,\n",
       " 772,\n",
       " 737,\n",
       " 504,\n",
       " 118,\n",
       " 753,\n",
       " 745,\n",
       " 475,\n",
       " 330,\n",
       " 829,\n",
       " 838,\n",
       " 898,\n",
       " 830,\n",
       " 896,\n",
       " 322,\n",
       " 325,\n",
       " 837,\n",
       " 384,\n",
       " 840,\n",
       " 376,\n",
       " 892,\n",
       " 379,\n",
       " 472,\n",
       " 890,\n",
       " 849,\n",
       " 889,\n",
       " 338,\n",
       " 780,\n",
       " 873,\n",
       " 476,\n",
       " 480,\n",
       " 766,\n",
       " 765,\n",
       " 233,\n",
       " 470,\n",
       " 511,\n",
       " 0,\n",
       " 169,\n",
       " 98,\n",
       " 723,\n",
       " 189,\n",
       " 706,\n",
       " 193,\n",
       " 212,\n",
       " 525,\n",
       " 213,\n",
       " 34,\n",
       " 11,\n",
       " 728,\n",
       " 702,\n",
       " 35,\n",
       " 521,\n",
       " 546,\n",
       " 174,\n",
       " 559,\n",
       " 100,\n",
       " 141,\n",
       " 104,\n",
       " 654,\n",
       " 143,\n",
       " 613,\n",
       " 655,\n",
       " 144,\n",
       " 657,\n",
       " 51,\n",
       " 99,\n",
       " 708,\n",
       " 721,\n",
       " 198,\n",
       " 42,\n",
       " 76,\n",
       " 518,\n",
       " 41,\n",
       " 697,\n",
       " 218,\n",
       " 223,\n",
       " 186,\n",
       " 4,\n",
       " 222,\n",
       " 36,\n",
       " 187,\n",
       " 732,\n",
       " 554,\n",
       " 527,\n",
       " 519,\n",
       " 699,\n",
       " 553,\n",
       " 19,\n",
       " 520,\n",
       " 707,\n",
       " 20,\n",
       " 532,\n",
       " 18,\n",
       " 529,\n",
       " 26,\n",
       " 28,\n",
       " 658,\n",
       " 199,\n",
       " 608,\n",
       " 686,\n",
       " 178,\n",
       " 690,\n",
       " 691,\n",
       " 56,\n",
       " 567,\n",
       " 55,\n",
       " 53,\n",
       " 564,\n",
       " 184,\n",
       " 563,\n",
       " 71,\n",
       " 682,\n",
       " 75,\n",
       " 602,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 600,\n",
       " 87,\n",
       " 164,\n",
       " 85,\n",
       " 165,\n",
       " 84,\n",
       " 166,\n",
       " 594,\n",
       " 95,\n",
       " 81,\n",
       " 62,\n",
       " 156,\n",
       " 173,\n",
       " 113,\n",
       " 605,\n",
       " 662,\n",
       " 651,\n",
       " 650,\n",
       " 621,\n",
       " 647,\n",
       " 631,\n",
       " 632,\n",
       " 123,\n",
       " 635,\n",
       " 636,\n",
       " 125,\n",
       " 637,\n",
       " 634,\n",
       " 666,\n",
       " 69,\n",
       " 128,\n",
       " 641,\n",
       " 622,\n",
       " 644,\n",
       " 135,\n",
       " 92,\n",
       " 155,\n",
       " 72,\n",
       " 578,\n",
       " 639,\n",
       " 785,\n",
       " 273,\n",
       " 566,\n",
       " 784,\n",
       " 455,\n",
       " 29,\n",
       " 689,\n",
       " 420,\n",
       " 438,\n",
       " 39,\n",
       " 40,\n",
       " 552,\n",
       " 297,\n",
       " 782,\n",
       " 545,\n",
       " 705,\n",
       " 33,\n",
       " 694,\n",
       " 32,\n",
       " 172,\n",
       " 479,\n",
       " 461,\n",
       " 284,\n",
       " 606,\n",
       " 715,\n",
       " 808,\n",
       " 250,\n",
       " 704,\n",
       " 49,\n",
       " 805,\n",
       " 157,\n",
       " 397,\n",
       " 486,\n",
       " 305,\n",
       " 58,\n",
       " 192,\n",
       " 421,\n",
       " 661,\n",
       " 814,\n",
       " 570,\n",
       " 152,\n",
       " 488,\n",
       " 301,\n",
       " 153,\n",
       " 407,\n",
       " 483,\n",
       " 57,\n",
       " 16,\n",
       " 409,\n",
       " 660,\n",
       " 306,\n",
       " 61,\n",
       " 246,\n",
       " 392,\n",
       " 827,\n",
       " 243,\n",
       " 314,\n",
       " 825,\n",
       " 824,\n",
       " 142,\n",
       " 60,\n",
       " 755,\n",
       " 713,\n",
       " 523,\n",
       " 818,\n",
       " 560,\n",
       " 398,\n",
       " 22,\n",
       " 831,\n",
       " 106,\n",
       " 741,\n",
       " 74,\n",
       " 586,\n",
       " 492,\n",
       " 607,\n",
       " 717,\n",
       " 206,\n",
       " 134,\n",
       " 509,\n",
       " 897,\n",
       " 133,\n",
       " 318,\n",
       " 507,\n",
       " 385,\n",
       " 620,\n",
       " 130,\n",
       " 320,\n",
       " 129,\n",
       " 609,\n",
       " 640,\n",
       " 583,\n",
       " 522,\n",
       " 327,\n",
       " 127,\n",
       " 835,\n",
       " 324,\n",
       " 221,\n",
       " 740,\n",
       " 582,\n",
       " 108,\n",
       " 720,\n",
       " 109,\n",
       " 543,\n",
       " 8,\n",
       " 502,\n",
       " 353,\n",
       " 731,\n",
       " 842,\n",
       " 220,\n",
       " 315,\n",
       " 498,\n",
       " 137,\n",
       " 701,\n",
       " 43,\n",
       " 45,\n",
       " 46,\n",
       " 617,\n",
       " 629,\n",
       " 798,\n",
       " 6,\n",
       " 592,\n",
       " 5,\n",
       " 339,\n",
       " 729,\n",
       " 885,\n",
       " 375,\n",
       " 517,\n",
       " 215,\n",
       " 351,\n",
       " 114,\n",
       " 619,\n",
       " 614,\n",
       " 888,\n",
       " 377,\n",
       " 266,\n",
       " 649,\n",
       " 626,\n",
       " 667,\n",
       " 124,\n",
       " 680,\n",
       " 170,\n",
       " 248,\n",
       " 272,\n",
       " 851,\n",
       " 361,\n",
       " 454,\n",
       " 884,\n",
       " 516,\n",
       " 38,\n",
       " 580,\n",
       " 886,\n",
       " 877,\n",
       " 83,\n",
       " 112,\n",
       " 426,\n",
       " 79,\n",
       " 730,\n",
       " 568,\n",
       " 234,\n",
       " 615,\n",
       " 887,\n",
       " 387,\n",
       " 224,\n",
       " 105,\n",
       " 719,\n",
       " 761,\n",
       " 94]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pid_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMlb3plUTqnz"
   },
   "source": [
    "#### Function all pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5RneqjSR01L"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNm9OBcjWufr"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4EN4TlZTlNI"
   },
   "outputs": [],
   "source": [
    "###  This function creates a sliding window or sequences of seq_length days and labels_length  days label ####\n",
    "def sliding_windows(data, seq_length,labels_length):\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "\n",
    "    for i in range(len(data)-(seq_length+labels_length)):\n",
    "        _x = data.iloc[i:(i+seq_length),:]\n",
    "        _y = data.iloc[(i+seq_length):(i+seq_length+labels_length),506:507]\n",
    "        _z  = data.iloc[(i+seq_length):(i+seq_length+labels_length),:506]\n",
    "        x.append(np.array(_x))\n",
    "        y.append(np.array(_y))\n",
    "        z.append(np.array(_z))\n",
    "\n",
    "    return x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKkuC8lxWi2c"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.embedding_dim, self.hidden_dim = embedding_dim,  embedding_dim\n",
    "        self.num_layers = 3\n",
    "        self.rnn1 = nn.LSTM(\n",
    "          input_size=n_features,\n",
    "          hidden_size=self.hidden_dim,\n",
    "          num_layers=3,\n",
    "          batch_first=True,\n",
    "          dropout = 0.35\n",
    "        )\n",
    "   \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.reshape((1, self.seq_len, self.n_features))\n",
    "        \n",
    "        h_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_dim).to(device))\n",
    "         \n",
    "        \n",
    "        c_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_dim).to(device))\n",
    "              \n",
    "        x, (hidden, cell) = self.rnn1(x,(h_1, c_1))\n",
    "        \n",
    "        \n",
    "        #return hidden_n.reshape((self.n_features, self.embedding_dim))\n",
    "        return hidden , cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSRN6KNAWmCc"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim=64, n_features=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features =  input_dim, n_features\n",
    "        \n",
    "        self.rnn1 = nn.LSTM(\n",
    "          input_size=n_features,\n",
    "          hidden_size=input_dim,\n",
    "          num_layers=3,\n",
    "          batch_first=True,\n",
    "          dropout = 0.35\n",
    "        )\n",
    "        \n",
    "        \n",
    "      \n",
    "        self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "    def forward(self, x,input_hidden,input_cell):\n",
    "       \n",
    "       \n",
    "        x = x.reshape((1,1,self.n_features ))\n",
    "        #print(\"decode input\",x.size())\n",
    "             \n",
    "\n",
    "        x, (hidden_n, cell_n) = self.rnn1(x,(input_hidden,input_cell))\n",
    "    \n",
    "        x = self.output_layer(x)\n",
    "        return x, hidden_n, cell_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxspo-QKWufp"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64,output_length = 0):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "        self.n_features = n_features\n",
    "        self.output_length = output_length\n",
    "        self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
    "        \n",
    "\n",
    "    def forward(self,x, prev_y,features):\n",
    "        \n",
    "       \n",
    "        hidden,cell = self.encoder(x)\n",
    "         \n",
    "        #Prepare place holder for decoder output\n",
    "        targets_ta = []\n",
    "        #prev_output become the next input to the LSTM cell\n",
    "        dec_input = prev_y\n",
    "        \n",
    "        \n",
    "        \n",
    "       #dec_input = torch.cat([prev_output, curr_features], dim=1) \n",
    "        \n",
    "        #itearate over LSTM - according to the required output days\n",
    "        for out_days in range(self.output_length) :\n",
    "            \n",
    "          \n",
    "            prev_x,prev_hidden,prev_cell = self.decoder(dec_input,hidden,cell)\n",
    "            hidden,cell = prev_hidden,prev_cell\n",
    "            \n",
    "            prev_x = prev_x[:,:,0:1]\n",
    "            #print(\"preve x shape is:\",prev_x.size())\n",
    "            #print(\"features shape is:\",features[out_days+1].size())\n",
    "            \n",
    "            if out_days+1 < self.output_length :\n",
    "                dec_input = torch.cat([prev_x,features[out_days+1].reshape(1,1,506)], dim=2) \n",
    "            \n",
    "            targets_ta.append(prev_x.reshape(1))\n",
    "           \n",
    "            \n",
    "        \n",
    "        \n",
    "        targets = torch.stack(targets_ta)\n",
    "\n",
    "        return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CIBwl0lYDZf"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSqiuuWIY0F6"
   },
   "outputs": [],
   "source": [
    "def train_model(model, TrainX,Trainy,ValidX,Validy, Valid_features, seq_length, n_epochs, train_features, optimizer, criterion, scheduler):\n",
    " \n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "    mb = master_bar(range(1, n_epochs + 1))\n",
    "\n",
    "    for epoch in mb:\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for i in progress_bar(range(TrainX.size()[0]),parent=mb):\n",
    "            seq_inp = TrainX[i,:,:].to(device)\n",
    "            seq_true = Trainy[i,:,:].to(device)\n",
    "            features = train_features[i,:,:].to(device)\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:],features)\n",
    "            \n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in progress_bar(range(ValidX.size()[0]),parent=mb):\n",
    "                seq_inp = ValidX[i,:,:].to(device)\n",
    "                seq_true = Validy[i,:,:].to(device)\n",
    "                features = Valid_features[i,:,:].to(device)\n",
    "        \n",
    "                seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:],features)\n",
    "               \n",
    "                loss = criterion(seq_pred, seq_true)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_n_features.pt')\n",
    "            print(\"saved best model epoch:\",epoch,\"val loss is:\",val_loss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "        scheduler.step(val_loss)\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBzKz4UqRPAe"
   },
   "outputs": [],
   "source": [
    "def seq2seq_global(pid,seq_length,epoch):\n",
    "  test_pid=test_dataset[test_dataset['pid']==pid]\n",
    "  test_pid[\"target\"]=None\n",
    "  train_pid = train_dataset[train_dataset['pid'] == pid]\n",
    "  all_pid=pd.concat([train_pid,test_pid])\n",
    "\n",
    "  print(len (all_pid))\n",
    "  if (len(all_pid)==0):\n",
    "    return ('No value for this pid')\n",
    "  all_pid = all_pid.set_index(['day'])\n",
    "\n",
    "  train_size = int((all_pid.shape[0]-len(test_pid)) * 0.55)\n",
    "  valid_size = (all_pid.shape[0]-len(test_pid))- train_size\n",
    "  print(\"train size is:\",train_size)\n",
    "  print(\"validation size is:\",valid_size)\n",
    "\n",
    "  train_data = all_pid.iloc[0:train_size,:]\n",
    "  valid_data = all_pid.iloc[train_size:train_size+valid_size,:]\n",
    "  print(\"train data shape is:\",train_data.shape)\n",
    "  print(\"validation data shape is:\",valid_data.shape)\n",
    "\n",
    "  labels_length = len(test_pid)\n",
    "\n",
    "  train_X, train_y,train_features = sliding_windows(train_data, seq_length,labels_length)\n",
    "  print(\"train X  has:\", len(train_X) , \"series\")\n",
    "  print(\"train labels  has:\", len(train_y) , \"series\")\n",
    "  valid_X, valid_y,valid_features = sliding_windows(valid_data, seq_length,labels_length)\n",
    "\n",
    "  if (len(valid_X)==0):\n",
    "    return ('no validation dataset for this pid')\n",
    "  print(\"validiation  X  has:\", len(valid_X) , \"series\")\n",
    "  print(\"Validiation  labels  has:\" ,len(valid_y) , \"series\")\n",
    "\n",
    "  trainX = Variable(torch.Tensor(train_X))\n",
    "  trainy = Variable(torch.Tensor(train_y))\n",
    "  train_features = Variable(torch.Tensor(train_features))\n",
    "  validX = Variable(torch.Tensor(valid_X))\n",
    "  validy= Variable(torch.Tensor(valid_y))\n",
    "  valid_features = Variable(torch.Tensor(valid_features))\n",
    "  print (\"trainX shape is:\",trainX.size())\n",
    "  print (\"trainy shape is:\",trainy.size())\n",
    "  print (\"train features  shape is:\",train_features.size())\n",
    "  print (\"validX shape is:\",validX.size())\n",
    "  print (\"validy shape is:\",validy.size())\n",
    "  print (\"valid features  shape is:\",valid_features.size())\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  n_features = trainX.shape[2]\n",
    "  model = Seq2Seq(seq_length, n_features, 512,output_length=len(test_pid))\n",
    "  model = model.to(device)\n",
    "  model\n",
    "  print(model) \n",
    "  model.apply(init_weights)\n",
    "    \n",
    "  #optimizer = torch.optim.RMSprop(model.parameters())\n",
    "  #optimizer = torch.optim.Adam(model.parameters(), lr=4e-3,weight_decay=1e-5)\n",
    "  #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  criterion = torch.nn.MSELoss().to(device) \n",
    "  #lambda1 = lambda epoch: 0.65 ** epoch\n",
    "  #scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "  #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 5e-3, eta_min=1e-8, last_epoch=-1)\n",
    "  #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=10, factor =0.5 ,min_lr=1e-7, eps=1e-08)\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, mode='min', factor=0.7, verbose=True, min_lr=1e-5)\n",
    "  \n",
    "  model, history = train_model(\n",
    "  model = model,\n",
    "  TrainX = trainX, Trainy = trainy,\n",
    "  ValidX= validX, Validy = validy,\n",
    "  Valid_features = valid_features, \n",
    "  seq_length = seq_length,\n",
    "  n_epochs = epoch, \n",
    "  train_features = train_features, \n",
    "  optimizer = optimizer, \n",
    "  criterion = criterion, \n",
    "  scheduler= scheduler   \n",
    "  )\n",
    "\n",
    "  TestX = np.array(all_pid.iloc[-2*len(test_pid):-len(test_pid),:])\n",
    "  Testy = np.array(all_pid.iloc[-len(test_pid):,:])\n",
    "\n",
    "  TestX = Variable(torch.Tensor(TestX))\n",
    "  Testy = Variable(torch.Tensor(Testy))\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    seq_inp = TestX.to(device)\n",
    "    seq_pred = model(TestX[0:seq_length,:].to(device),seq_inp[seq_length-1:seq_length,:],seq_inp[:,:506])\n",
    "    #seq_pred = model(TestX[-seq_length:,:].to(device),seq_inp[seq_length-1:seq_length,:],seq_inp[:,:506])\n",
    "\n",
    "  data_predict = seq_pred.cpu().numpy()\n",
    "  #labels = Testy\n",
    "  #data_predict.flatten()\n",
    "  \n",
    "  #original_data = all_pid.iloc[-len(test_pid):,:]\n",
    "  #final = pd.DataFrame(original_data['target'])\n",
    "  \n",
    "  pred = data_predict.flatten()\n",
    "  pred_df = pd.DataFrame(pred)\n",
    "  pred_df = pred_df.set_index([test_pid.index])\n",
    "  test_pid['target'] = pred_df\n",
    "  \n",
    "  return test_pid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8U-ulJ929F-"
   },
   "source": [
    "##### Run predict all pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoaGThNTsfXO"
   },
   "outputs": [],
   "source": [
    "df_princip = pd.DataFrame([])\n",
    "df_princip = pd.concat([df_princip, df_princip], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1613245655920,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "KzMIz7lgf18a",
    "outputId": "277dd81c-e8db-423f-ff84-ff1b3c99c6cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[214, 663, 291]"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid_selection[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qFesvR81UGCH",
    "outputId": "da2b590d-c230-423e-ce04-ad6b0622ce06"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='48' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      96.00% [48/50 4:50:10<12:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.028080455586314203\n",
      "Epoch 1: train loss 0.019764571494999387 val loss 0.028080455586314203\n",
      "Epoch 2: train loss 0.01706123480661994 val loss 0.029568711668252944\n",
      "saved best model epoch: 3 val loss is: 0.02789255976676941\n",
      "Epoch 3: train loss 0.016911968322736875 val loss 0.02789255976676941\n",
      "saved best model epoch: 4 val loss is: 0.02671697363257408\n",
      "Epoch 4: train loss 0.016716912876637208 val loss 0.02671697363257408\n",
      "saved best model epoch: 5 val loss is: 0.026326731592416764\n",
      "Epoch 5: train loss 0.01660903383578573 val loss 0.026326731592416764\n",
      "saved best model epoch: 6 val loss is: 0.026260706409811974\n",
      "Epoch 6: train loss 0.016468249394425323 val loss 0.026260706409811974\n",
      "saved best model epoch: 7 val loss is: 0.023813247680664062\n",
      "Epoch 7: train loss 0.016216207983060962 val loss 0.023813247680664062\n",
      "saved best model epoch: 8 val loss is: 0.02143314518034458\n",
      "Epoch 8: train loss 0.015836964050928753 val loss 0.02143314518034458\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.011327947489917278\n",
      "Epoch 1: train loss 0.012214347631448791 val loss 0.011327947489917278\n",
      "Epoch 2: train loss 0.00810855800574202 val loss 0.011970473080873489\n",
      "Epoch 3: train loss 0.007856647350958415 val loss 0.01162523590028286\n",
      "Epoch 4: train loss 0.007718614873565024 val loss 0.01171762365847826\n",
      "Epoch 5: train loss 0.007589248801758956 val loss 0.01197731588035822\n",
      "saved best model epoch: 6 val loss is: 0.010991557873785496\n",
      "Epoch 6: train loss 0.007444710864330686 val loss 0.010991557873785496\n",
      "Epoch 7: train loss 0.00750346757316341 val loss 0.011635248921811581\n",
      "Epoch 8: train loss 0.007314317427309496 val loss 0.011668768711388111\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.013277865014970303\n",
      "Epoch 1: train loss 0.015245014014432118 val loss 0.013277865014970303\n",
      "Epoch 2: train loss 0.010211751918264088 val loss 0.013313020020723343\n",
      "Epoch 3: train loss 0.009644249620448266 val loss 0.013592981919646262\n",
      "Epoch 4: train loss 0.009349611654345478 val loss 0.014355033449828625\n",
      "Epoch 5: train loss 0.009122669996161546 val loss 0.013327371887862682\n",
      "Epoch 6: train loss 0.008935393710132866 val loss 0.014136564731597901\n",
      "Epoch 7: train loss 0.008637360934655936 val loss 0.013432040996849536\n",
      "Epoch     7: reducing learning rate of group 0 to 7.0000e-05.\n",
      "Epoch 8: train loss 0.008405601766536989 val loss 0.014005350694060326\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.017718494683504105\n",
      "Epoch 1: train loss 0.022278298779080313 val loss 0.017718494683504105\n",
      "saved best model epoch: 2 val loss is: 0.016651373729109763\n",
      "Epoch 2: train loss 0.018277325412435902 val loss 0.016651373729109763\n",
      "saved best model epoch: 3 val loss is: 0.01570676378905773\n",
      "Epoch 3: train loss 0.0172684896027758 val loss 0.01570676378905773\n",
      "saved best model epoch: 4 val loss is: 0.015470663458108902\n",
      "Epoch 4: train loss 0.016454931902920918 val loss 0.015470663458108902\n",
      "Epoch 5: train loss 0.01571124329763864 val loss 0.019908280670642854\n",
      "Epoch 6: train loss 0.01504922790142397 val loss 0.016607903689146043\n",
      "Epoch 7: train loss 0.014018256815948657 val loss 0.019020943343639372\n",
      "Epoch 8: train loss 0.012982066776159974 val loss 0.018600340560078622\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.03361293524503708\n",
      "Epoch 1: train loss 0.01931382039384473 val loss 0.03361293524503708\n",
      "saved best model epoch: 2 val loss is: 0.02447049580514431\n",
      "Epoch 2: train loss 0.01651423821403157 val loss 0.02447049580514431\n",
      "Epoch 3: train loss 0.015647535733435126 val loss 0.025379291176795958\n",
      "Epoch 4: train loss 0.015109613792793382 val loss 0.03883090317249298\n",
      "saved best model epoch: 5 val loss is: 0.02348432131111622\n",
      "Epoch 5: train loss 0.014564823980132738 val loss 0.02348432131111622\n",
      "Epoch 6: train loss 0.014725842025308382 val loss 0.029959313571453094\n",
      "Epoch 7: train loss 0.014320628152095847 val loss 0.04060457125306129\n",
      "Epoch 8: train loss 0.013472325989000854 val loss 0.035287410765886304\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.019363691098988056\n",
      "Epoch 1: train loss 0.021374206047460258 val loss 0.019363691098988056\n",
      "saved best model epoch: 2 val loss is: 0.018128428608179092\n",
      "Epoch 2: train loss 0.01857131124889276 val loss 0.018128428608179092\n",
      "saved best model epoch: 3 val loss is: 0.01715635322034359\n",
      "Epoch 3: train loss 0.01792759861213615 val loss 0.01715635322034359\n",
      "saved best model epoch: 4 val loss is: 0.017115753144025803\n",
      "Epoch 4: train loss 0.016913928304720355 val loss 0.017115753144025803\n",
      "Epoch 5: train loss 0.01639766915959407 val loss 0.017281072214245796\n",
      "Epoch 6: train loss 0.01567707960325551 val loss 0.017246904782950878\n",
      "Epoch 7: train loss 0.01575755396386586 val loss 0.02024575136601925\n",
      "Epoch 8: train loss 0.01492330465600433 val loss 0.01785749662667513\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.017528698593378068\n",
      "Epoch 1: train loss 0.022876135472740446 val loss 0.017528698593378068\n",
      "Epoch 2: train loss 0.018139864850257124 val loss 0.017664847895503044\n",
      "Epoch 3: train loss 0.01765818115589874 val loss 0.01753946878015995\n",
      "saved best model epoch: 4 val loss is: 0.017473863437771797\n",
      "Epoch 4: train loss 0.01753288596158936 val loss 0.017473863437771797\n",
      "saved best model epoch: 5 val loss is: 0.017458992823958398\n",
      "Epoch 5: train loss 0.017359834341775803 val loss 0.017458992823958398\n",
      "Epoch 6: train loss 0.01707248156890273 val loss 0.01791900098323822\n",
      "Epoch 7: train loss 0.016746964837823595 val loss 0.018676574900746347\n",
      "Epoch 8: train loss 0.01628853477138494 val loss 0.01847469061613083\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.01578306104056537\n",
      "Epoch 1: train loss 0.01567047446158277 val loss 0.01578306104056537\n",
      "Epoch 2: train loss 0.012556606611932617 val loss 0.016721712425351143\n",
      "Epoch 3: train loss 0.012210296107882476 val loss 0.016738916747272015\n",
      "Epoch 4: train loss 0.012141240412840643 val loss 0.016766623593866825\n",
      "Epoch 5: train loss 0.01179402143167085 val loss 0.01748674688860774\n",
      "Epoch 6: train loss 0.0114718713427344 val loss 0.018716204445809126\n",
      "Epoch 7: train loss 0.011191005055534553 val loss 0.021570734214037657\n",
      "Epoch     7: reducing learning rate of group 0 to 7.0000e-05.\n",
      "Epoch 8: train loss 0.010583080934831896 val loss 0.01999108400195837\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.016957843210548162\n",
      "Epoch 1: train loss 0.019175430456827205 val loss 0.016957843210548162\n",
      "Epoch 2: train loss 0.014895406226257244 val loss 0.017006683629006147\n",
      "Epoch 3: train loss 0.014312912934426084 val loss 0.01721080904826522\n",
      "saved best model epoch: 4 val loss is: 0.016811753623187542\n",
      "Epoch 4: train loss 0.014173786458839854 val loss 0.016811753623187542\n",
      "Epoch 5: train loss 0.013746491049876413 val loss 0.01788472244516015\n",
      "Epoch 6: train loss 0.013594412300960127 val loss 0.018143864814192057\n",
      "Epoch 7: train loss 0.013011043128299427 val loss 0.021077302750200033\n",
      "Epoch 8: train loss 0.012505752303783434 val loss 0.019864988047629595\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.03359435126185417\n",
      "Epoch 1: train loss 0.022663147873189075 val loss 0.03359435126185417\n",
      "Epoch 2: train loss 0.020388164703386377 val loss 0.0363938445225358\n",
      "Epoch 3: train loss 0.01996464028685208 val loss 0.033854239620268345\n",
      "Epoch 4: train loss 0.01966727533972407 val loss 0.03582765068858862\n",
      "saved best model epoch: 5 val loss is: 0.029920848086476326\n",
      "Epoch 5: train loss 0.01932297422584281 val loss 0.029920848086476326\n",
      "Epoch 6: train loss 0.019144724269049715 val loss 0.031327840872108936\n",
      "Epoch 7: train loss 0.018781426710536682 val loss 0.032052322290837765\n",
      "Epoch 8: train loss 0.018316775863339383 val loss 0.031925950199365616\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.028208947740495205\n",
      "Epoch 1: train loss 0.02291963339210993 val loss 0.028208947740495205\n",
      "saved best model epoch: 2 val loss is: 0.023678440134972334\n",
      "Epoch 2: train loss 0.016866547302399056 val loss 0.023678440134972334\n",
      "saved best model epoch: 3 val loss is: 0.023154047783464193\n",
      "Epoch 3: train loss 0.016349748169711555 val loss 0.023154047783464193\n",
      "saved best model epoch: 4 val loss is: 0.023025997448712587\n",
      "Epoch 4: train loss 0.01597025729042579 val loss 0.023025997448712587\n",
      "saved best model epoch: 5 val loss is: 0.02211884642019868\n",
      "Epoch 5: train loss 0.015653272585786252 val loss 0.02211884642019868\n",
      "Epoch 6: train loss 0.015489373131688819 val loss 0.022311296314001083\n",
      "Epoch 7: train loss 0.015206953656511852 val loss 0.022514001466333866\n",
      "Epoch 8: train loss 0.014872799930443248 val loss 0.025458417367190123\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.03537100926041603\n",
      "Epoch 1: train loss 0.030064393418381012 val loss 0.03537100926041603\n",
      "saved best model epoch: 2 val loss is: 0.034991996362805367\n",
      "Epoch 2: train loss 0.02686948261885758 val loss 0.034991996362805367\n",
      "Epoch 3: train loss 0.02590120204511177 val loss 0.039479827508330345\n",
      "saved best model epoch: 4 val loss is: 0.02715587243437767\n",
      "Epoch 4: train loss 0.025260020047426224 val loss 0.02715587243437767\n",
      "Epoch 5: train loss 0.024614322751042354 val loss 0.04102988447993994\n",
      "Epoch 6: train loss 0.02300553580364549 val loss 0.03833800554275513\n",
      "Epoch 7: train loss 0.021777798096279066 val loss 0.03011307306587696\n",
      "Epoch 8: train loss 0.021579535139432872 val loss 0.04582468047738075\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.014879291504621505\n",
      "Epoch 1: train loss 0.020002006819205626 val loss 0.014879291504621505\n",
      "Epoch 2: train loss 0.01228581775822455 val loss 0.01639693006873131\n",
      "Epoch 3: train loss 0.012039564372528167 val loss 0.019748421013355257\n",
      "Epoch 4: train loss 0.011679155242052815 val loss 0.020164672285318375\n",
      "Epoch 5: train loss 0.011461204560917048 val loss 0.020652127265930176\n",
      "Epoch 6: train loss 0.011338309085528766 val loss 0.02031138353049755\n",
      "Epoch 7: train loss 0.011020391248166561 val loss 0.021400478109717368\n",
      "Epoch     7: reducing learning rate of group 0 to 7.0000e-05.\n",
      "Epoch 8: train loss 0.010711800672912173 val loss 0.021735168248414993\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.03737213276326656\n",
      "Epoch 1: train loss 0.020545908126486354 val loss 0.03737213276326656\n",
      "saved best model epoch: 2 val loss is: 0.03229719679802656\n",
      "Epoch 2: train loss 0.017555617267288357 val loss 0.03229719679802656\n",
      "saved best model epoch: 3 val loss is: 0.031322211027145386\n",
      "Epoch 3: train loss 0.016891657767525638 val loss 0.031322211027145386\n",
      "saved best model epoch: 4 val loss is: 0.028722341638058424\n",
      "Epoch 4: train loss 0.016349302616015256 val loss 0.028722341638058424\n",
      "Epoch 5: train loss 0.015760431952594994 val loss 0.0324331633746624\n",
      "Epoch 6: train loss 0.015143397634467447 val loss 0.031183375045657158\n",
      "Epoch 7: train loss 0.014571655810956496 val loss 0.0297341151162982\n",
      "saved best model epoch: 8 val loss is: 0.027016229927539825\n",
      "Epoch 8: train loss 0.013613614029284701 val loss 0.027016229927539825\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.02187782572582364\n",
      "Epoch 1: train loss 0.02211770489363067 val loss 0.02187782572582364\n",
      "saved best model epoch: 2 val loss is: 0.018624216318130493\n",
      "Epoch 2: train loss 0.019367091380149484 val loss 0.018624216318130493\n",
      "Epoch 3: train loss 0.01893615785492472 val loss 0.019200393930077553\n",
      "saved best model epoch: 4 val loss is: 0.0183032532222569\n",
      "Epoch 4: train loss 0.018386769860264766 val loss 0.0183032532222569\n",
      "saved best model epoch: 5 val loss is: 0.01807159511372447\n",
      "Epoch 5: train loss 0.01793222008160798 val loss 0.01807159511372447\n",
      "Epoch 6: train loss 0.017627792859292894 val loss 0.0188482403755188\n",
      "Epoch 7: train loss 0.017397026183554924 val loss 0.023190381471067667\n",
      "Epoch 8: train loss 0.016544786573623318 val loss 0.01915559684857726\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.008417773433029652\n",
      "Epoch 1: train loss 0.013052795060156356 val loss 0.008417773433029652\n",
      "Epoch 2: train loss 0.008705360498944563 val loss 0.010032922960817814\n",
      "Epoch 3: train loss 0.007910652962025432 val loss 0.010796316154301166\n",
      "Epoch 4: train loss 0.007330850382069392 val loss 0.009928953275084496\n",
      "Epoch 5: train loss 0.00653320054213206 val loss 0.00984885785728693\n",
      "Epoch 6: train loss 0.005739756072649644 val loss 0.010563935898244382\n",
      "Epoch 7: train loss 0.0051531981721165635 val loss 0.010375549457967282\n",
      "Epoch     7: reducing learning rate of group 0 to 7.0000e-05.\n",
      "Epoch 8: train loss 0.004529053894137698 val loss 0.010331232845783234\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.010966712236404419\n",
      "Epoch 1: train loss 0.013024195241520093 val loss 0.010966712236404419\n",
      "Epoch 2: train loss 0.009970704115749825 val loss 0.012485087849199772\n",
      "Epoch 3: train loss 0.009387591232856115 val loss 0.01384250670671463\n",
      "Epoch 4: train loss 0.008786952028804947 val loss 0.013580685667693614\n",
      "Epoch 5: train loss 0.008546433162077196 val loss 0.011857693083584309\n",
      "Epoch 6: train loss 0.00837054360835325 val loss 0.01354286503046751\n",
      "Epoch 7: train loss 0.008265527913213841 val loss 0.012693470157682896\n",
      "Epoch     7: reducing learning rate of group 0 to 7.0000e-05.\n",
      "Epoch 8: train loss 0.008176606536532441 val loss 0.014306812919676304\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.014959665946662426\n",
      "Epoch 1: train loss 0.021845070822607903 val loss 0.014959665946662426\n",
      "saved best model epoch: 2 val loss is: 0.014542113989591599\n",
      "Epoch 2: train loss 0.01460221926459954 val loss 0.014542113989591599\n",
      "saved best model epoch: 3 val loss is: 0.014184136874973774\n",
      "Epoch 3: train loss 0.014069749264135248 val loss 0.014184136874973774\n",
      "Epoch 4: train loss 0.0135891757506345 val loss 0.014396696910262108\n",
      "Epoch 5: train loss 0.012970597627350972 val loss 0.015514268167316913\n",
      "Epoch 6: train loss 0.012858074180604447 val loss 0.015198434889316558\n",
      "Epoch 7: train loss 0.012175099713550437 val loss 0.016466397792100906\n",
      "Epoch 8: train loss 0.01223073469563609 val loss 0.014763793349266053\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.020190811716020107\n",
      "Epoch 1: train loss 0.022231685670923037 val loss 0.020190811716020107\n",
      "Epoch 2: train loss 0.019295742735266685 val loss 0.020251433365046978\n",
      "Epoch 3: train loss 0.018910494532032186 val loss 0.020892996340990067\n",
      "Epoch 4: train loss 0.018144074401043982 val loss 0.021352517418563366\n",
      "Epoch 5: train loss 0.017045841989926546 val loss 0.02140090800821781\n",
      "Epoch 6: train loss 0.016400209807577622 val loss 0.022557467687875032\n",
      "Epoch 7: train loss 0.01602406778473811 val loss 0.02212440362200141\n",
      "Epoch     7: reducing learning rate of group 0 to 7.0000e-05.\n",
      "Epoch 8: train loss 0.015437888867973563 val loss 0.024813936557620764\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.013164098374545574\n",
      "Epoch 1: train loss 0.01489217880935896 val loss 0.013164098374545574\n",
      "Epoch 2: train loss 0.012754622325744657 val loss 0.014462347328662872\n",
      "Epoch 3: train loss 0.012433396785386972 val loss 0.016141487285494804\n",
      "Epoch 4: train loss 0.012274353720602534 val loss 0.016288101300597192\n",
      "Epoch 5: train loss 0.012099924275562876 val loss 0.01772027388215065\n",
      "Epoch 6: train loss 0.01184494013986772 val loss 0.017316967621445654\n",
      "Epoch 7: train loss 0.011586978089153058 val loss 0.01975127197802067\n",
      "Epoch     7: reducing learning rate of group 0 to 7.0000e-05.\n",
      "Epoch 8: train loss 0.01131754141256568 val loss 0.018372759222984314\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.017112010344862937\n",
      "Epoch 1: train loss 0.02151948382102308 val loss 0.017112010344862937\n",
      "saved best model epoch: 2 val loss is: 0.0166277751326561\n",
      "Epoch 2: train loss 0.014921831155550621 val loss 0.0166277751326561\n",
      "Epoch 3: train loss 0.014666681211175663 val loss 0.01667882949113846\n",
      "Epoch 4: train loss 0.014535534361909543 val loss 0.01667424701154232\n",
      "Epoch 5: train loss 0.014308384752699308 val loss 0.01675509437918663\n",
      "Epoch 6: train loss 0.014046995856222651 val loss 0.017667219042778015\n",
      "Epoch 7: train loss 0.01396703772202489 val loss 0.01668851003050804\n",
      "Epoch 8: train loss 0.013682414899535832 val loss 0.017268473282456398\n",
      "Epoch     8: reducing learning rate of group 0 to 7.0000e-05.\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.015205624746158719\n",
      "Epoch 1: train loss 0.016135292363364296 val loss 0.015205624746158719\n",
      "saved best model epoch: 2 val loss is: 0.015169401420280337\n",
      "Epoch 2: train loss 0.013607606864029384 val loss 0.015169401420280337\n",
      "Epoch 3: train loss 0.012927827144782227 val loss 0.015285322442650795\n",
      "saved best model epoch: 4 val loss is: 0.01469058683142066\n",
      "Epoch 4: train loss 0.012478400100336736 val loss 0.01469058683142066\n",
      "saved best model epoch: 5 val loss is: 0.014537865994498134\n",
      "Epoch 5: train loss 0.012093275695680136 val loss 0.014537865994498134\n",
      "Epoch 6: train loss 0.011671197600662708 val loss 0.01489582797512412\n",
      "saved best model epoch: 7 val loss is: 0.014214539900422096\n",
      "Epoch 7: train loss 0.01113715536712882 val loss 0.014214539900422096\n",
      "Epoch 8: train loss 0.010382379966238177 val loss 0.015344538725912571\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.02098910929635167\n",
      "Epoch 1: train loss 0.02183373187141246 val loss 0.02098910929635167\n",
      "saved best model epoch: 2 val loss is: 0.020979054272174835\n",
      "Epoch 2: train loss 0.018669372805988932 val loss 0.020979054272174835\n",
      "saved best model epoch: 3 val loss is: 0.017973075155168772\n",
      "Epoch 3: train loss 0.018113320334309554 val loss 0.017973075155168772\n",
      "saved best model epoch: 4 val loss is: 0.01739439321681857\n",
      "Epoch 4: train loss 0.017396412587848055 val loss 0.01739439321681857\n",
      "saved best model epoch: 5 val loss is: 0.016439820174127817\n",
      "Epoch 5: train loss 0.01707279433058687 val loss 0.016439820174127817\n",
      "Epoch 6: train loss 0.01650673265467925 val loss 0.018318782094866037\n",
      "Epoch 7: train loss 0.016218577534050108 val loss 0.017679209355264902\n",
      "Epoch 8: train loss 0.01660407993224371 val loss 0.019378353841602802\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.03442043624818325\n",
      "Epoch 1: train loss 0.021191165098045247 val loss 0.03442043624818325\n",
      "saved best model epoch: 2 val loss is: 0.031434847973287106\n",
      "Epoch 2: train loss 0.018877854166799282 val loss 0.031434847973287106\n",
      "saved best model epoch: 3 val loss is: 0.0286637875251472\n",
      "Epoch 3: train loss 0.018387557972357214 val loss 0.0286637875251472\n",
      "saved best model epoch: 4 val loss is: 0.028158247005194426\n",
      "Epoch 4: train loss 0.018098168041422426 val loss 0.028158247005194426\n",
      "Epoch 5: train loss 0.018004386775554663 val loss 0.02838826458901167\n",
      "Epoch 6: train loss 0.017828026719122047 val loss 0.034308554604649544\n",
      "Epoch 7: train loss 0.017714400383004224 val loss 0.030002973042428493\n",
      "Epoch 8: train loss 0.017138237840529667 val loss 0.02885685209184885\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.015227733412757516\n",
      "Epoch 1: train loss 0.018918600898370684 val loss 0.015227733412757516\n",
      "saved best model epoch: 2 val loss is: 0.01513720746152103\n",
      "Epoch 2: train loss 0.015379210608641067 val loss 0.01513720746152103\n",
      "Epoch 3: train loss 0.015104946119329297 val loss 0.015402469784021378\n",
      "Epoch 4: train loss 0.014825314687318113 val loss 0.015212445054203272\n",
      "Epoch 5: train loss 0.014558545998241528 val loss 0.015390918590128422\n",
      "Epoch 6: train loss 0.014323898434010615 val loss 0.015498350374400616\n",
      "Epoch 7: train loss 0.014269261834133103 val loss 0.018121593166142702\n",
      "Epoch 8: train loss 0.013668799380132234 val loss 0.021221890579909086\n",
      "Epoch     8: reducing learning rate of group 0 to 7.0000e-05.\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.013226635754108429\n",
      "Epoch 1: train loss 0.016044948092964757 val loss 0.013226635754108429\n",
      "saved best model epoch: 2 val loss is: 0.012970123440027237\n",
      "Epoch 2: train loss 0.013096151200224119 val loss 0.012970123440027237\n",
      "Epoch 3: train loss 0.012722889275615474 val loss 0.013319585705175996\n",
      "saved best model epoch: 4 val loss is: 0.012775912648066878\n",
      "Epoch 4: train loss 0.012560197623469025 val loss 0.012775912648066878\n",
      "Epoch 5: train loss 0.012282265103366002 val loss 0.012980219675228\n",
      "saved best model epoch: 6 val loss is: 0.012427295092493296\n",
      "Epoch 6: train loss 0.011875061037759465 val loss 0.012427295092493296\n",
      "Epoch 7: train loss 0.011166983575795788 val loss 0.013699022587388754\n",
      "Epoch 8: train loss 0.010854268569992968 val loss 0.015928468201309443\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.028280925005674362\n",
      "Epoch 1: train loss 0.019775247434154153 val loss 0.028280925005674362\n",
      "Epoch 2: train loss 0.01567284884818253 val loss 0.030015609040856362\n",
      "saved best model epoch: 3 val loss is: 0.027744071930646895\n",
      "Epoch 3: train loss 0.015309822202349702 val loss 0.027744071930646895\n",
      "saved best model epoch: 4 val loss is: 0.025708923861384392\n",
      "Epoch 4: train loss 0.014967368195010792 val loss 0.025708923861384392\n",
      "Epoch 5: train loss 0.014803968358873612 val loss 0.029012543708086015\n",
      "Epoch 6: train loss 0.014158132652352964 val loss 0.0329549215734005\n",
      "saved best model epoch: 7 val loss is: 0.023839811235666274\n",
      "Epoch 7: train loss 0.013760681641066358 val loss 0.023839811235666274\n",
      "Epoch 8: train loss 0.013555467672025165 val loss 0.02549779824912548\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.017300904262810946\n",
      "Epoch 1: train loss 0.021283909005213934 val loss 0.017300904262810946\n",
      "saved best model epoch: 2 val loss is: 0.017181545961648226\n",
      "Epoch 2: train loss 0.01619386536080435 val loss 0.017181545961648226\n",
      "Epoch 3: train loss 0.015703756429524308 val loss 0.01739394525066018\n",
      "Epoch 4: train loss 0.015579846137797976 val loss 0.017365333158522844\n",
      "Epoch 5: train loss 0.015354345900466642 val loss 0.01760115148499608\n",
      "Epoch 6: train loss 0.015276684739952346 val loss 0.017653692048043013\n",
      "Epoch 7: train loss 0.01510164814900203 val loss 0.018763678148388863\n",
      "Epoch 8: train loss 0.014948111207011235 val loss 0.017416466027498245\n",
      "Epoch     8: reducing learning rate of group 0 to 7.0000e-05.\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.019285273738205433\n",
      "Epoch 1: train loss 0.016546568070280265 val loss 0.019285273738205433\n",
      "saved best model epoch: 2 val loss is: 0.018670279998332262\n",
      "Epoch 2: train loss 0.012371358248483703 val loss 0.018670279998332262\n",
      "saved best model epoch: 3 val loss is: 0.018648356664925814\n",
      "Epoch 3: train loss 0.012011683770421758 val loss 0.018648356664925814\n",
      "saved best model epoch: 4 val loss is: 0.017541783396154642\n",
      "Epoch 4: train loss 0.011482842625623726 val loss 0.017541783396154642\n",
      "saved best model epoch: 5 val loss is: 0.0171653819270432\n",
      "Epoch 5: train loss 0.011396192575255072 val loss 0.0171653819270432\n",
      "Epoch 6: train loss 0.011005618385072932 val loss 0.018164426553994417\n",
      "Epoch 7: train loss 0.010295962432332068 val loss 0.02018228964880109\n",
      "Epoch 8: train loss 0.008886760588825107 val loss 0.01846550963819027\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.018206078093498945\n",
      "Epoch 1: train loss 0.020126427961401194 val loss 0.018206078093498945\n",
      "saved best model epoch: 2 val loss is: 0.018024377524852753\n",
      "Epoch 2: train loss 0.016365895519055516 val loss 0.018024377524852753\n",
      "Epoch 3: train loss 0.015658448528812593 val loss 0.01807125099003315\n",
      "Epoch 4: train loss 0.014866173065390932 val loss 0.018032348714768887\n",
      "Epoch 5: train loss 0.013914803054228604 val loss 0.01895546680316329\n",
      "Epoch 6: train loss 0.012724222772451768 val loss 0.01840045629069209\n",
      "Epoch 7: train loss 0.011589826320309237 val loss 0.020018348935991526\n",
      "Epoch 8: train loss 0.010181635034730635 val loss 0.020148200914263725\n",
      "Epoch     8: reducing learning rate of group 0 to 7.0000e-05.\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.019210746977478266\n",
      "Epoch 1: train loss 0.018789479778294104 val loss 0.019210746977478266\n",
      "saved best model epoch: 2 val loss is: 0.018433145247399807\n",
      "Epoch 2: train loss 0.014104839908071312 val loss 0.018433145247399807\n",
      "Epoch 3: train loss 0.013502216796918088 val loss 0.02058341819792986\n",
      "Epoch 4: train loss 0.01316139320786818 val loss 0.01920843217521906\n",
      "Epoch 5: train loss 0.012600415498467094 val loss 0.022248287685215473\n",
      "Epoch 6: train loss 0.012116464831114533 val loss 0.018949071411043406\n",
      "Epoch 7: train loss 0.011513305435248887 val loss 0.0227605695836246\n",
      "Epoch 8: train loss 0.010824513655290547 val loss 0.023020644672214985\n",
      "Epoch     8: reducing learning rate of group 0 to 7.0000e-05.\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.014182951068505645\n",
      "Epoch 1: train loss 0.015157732181251049 val loss 0.014182951068505645\n",
      "saved best model epoch: 2 val loss is: 0.014170864131301641\n",
      "Epoch 2: train loss 0.013253443010420684 val loss 0.014170864131301641\n",
      "saved best model epoch: 3 val loss is: 0.014102759072557092\n",
      "Epoch 3: train loss 0.013016681074647301 val loss 0.014102759072557092\n",
      "Epoch 4: train loss 0.012868770229888249 val loss 0.01411436963826418\n",
      "saved best model epoch: 5 val loss is: 0.013993639731779695\n",
      "Epoch 5: train loss 0.012842784400086805 val loss 0.013993639731779695\n",
      "Epoch 6: train loss 0.01283735841378031 val loss 0.014033997198566794\n",
      "Epoch 7: train loss 0.012698653434593993 val loss 0.014030550606548786\n",
      "Epoch 8: train loss 0.012480765771883798 val loss 0.01427493616938591\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.018830459099262953\n",
      "Epoch 1: train loss 0.014067703162331179 val loss 0.018830459099262953\n",
      "Epoch 2: train loss 0.011883765043892774 val loss 0.018914734944701195\n",
      "saved best model epoch: 3 val loss is: 0.01856868714094162\n",
      "Epoch 3: train loss 0.011499060205666416 val loss 0.01856868714094162\n",
      "Epoch 4: train loss 0.01123984513736992 val loss 0.018892429769039154\n",
      "Epoch 5: train loss 0.011030105997370669 val loss 0.018817367497831583\n",
      "saved best model epoch: 6 val loss is: 0.01723636779934168\n",
      "Epoch 6: train loss 0.01070099224123251 val loss 0.01723636779934168\n",
      "Epoch 7: train loss 0.010196247093199965 val loss 0.017323991283774376\n",
      "Epoch 8: train loss 0.010130242018186185 val loss 0.021211490500718355\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.034313428401947024\n",
      "Epoch 1: train loss 0.017842648617391075 val loss 0.034313428401947024\n",
      "saved best model epoch: 2 val loss is: 0.03355739638209343\n",
      "Epoch 2: train loss 0.015239586177769871 val loss 0.03355739638209343\n",
      "saved best model epoch: 3 val loss is: 0.03147742860019207\n",
      "Epoch 3: train loss 0.014864106429740787 val loss 0.03147742860019207\n",
      "saved best model epoch: 4 val loss is: 0.03106333427131176\n",
      "Epoch 4: train loss 0.01467611456644677 val loss 0.03106333427131176\n",
      "Epoch 5: train loss 0.014498403772623056 val loss 0.03191235587000847\n",
      "Epoch 6: train loss 0.014108227011525915 val loss 0.03132810294628143\n",
      "saved best model epoch: 7 val loss is: 0.030810684338212012\n",
      "Epoch 7: train loss 0.01377756605368285 val loss 0.030810684338212012\n",
      "Epoch 8: train loss 0.013790523001391972 val loss 0.03489984646439552\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.02379463752731681\n",
      "Epoch 1: train loss 0.016721529167998267 val loss 0.02379463752731681\n",
      "Epoch 2: train loss 0.013306526074477708 val loss 0.023939634207636118\n",
      "saved best model epoch: 3 val loss is: 0.022545862942934036\n",
      "Epoch 3: train loss 0.012797650522047496 val loss 0.022545862942934036\n",
      "saved best model epoch: 4 val loss is: 0.020886266138404608\n",
      "Epoch 4: train loss 0.012556395426123258 val loss 0.020886266138404608\n",
      "saved best model epoch: 5 val loss is: 0.020839196164160967\n",
      "Epoch 5: train loss 0.01228491671338498 val loss 0.020839196164160967\n",
      "Epoch 6: train loss 0.012056406250739672 val loss 0.020919280126690865\n",
      "Epoch 7: train loss 0.011700195697955338 val loss 0.023905588779598475\n",
      "Epoch 8: train loss 0.011422486785036254 val loss 0.022687969263643026\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.028548785019665956\n",
      "Epoch 1: train loss 0.01918166811880936 val loss 0.028548785019665956\n",
      "saved best model epoch: 2 val loss is: 0.027547269128262997\n",
      "Epoch 2: train loss 0.016320437013384807 val loss 0.027547269128262997\n",
      "saved best model epoch: 3 val loss is: 0.023621768224984407\n",
      "Epoch 3: train loss 0.015461720930165556 val loss 0.023621768224984407\n",
      "saved best model epoch: 4 val loss is: 0.022199569270014763\n",
      "Epoch 4: train loss 0.014599622597536409 val loss 0.022199569270014763\n",
      "saved best model epoch: 5 val loss is: 0.01994157861918211\n",
      "Epoch 5: train loss 0.013857599628348666 val loss 0.01994157861918211\n",
      "saved best model epoch: 6 val loss is: 0.01993605261668563\n",
      "Epoch 6: train loss 0.013460675433996212 val loss 0.01993605261668563\n",
      "Epoch 7: train loss 0.012920259733695582 val loss 0.028154442086815834\n",
      "Epoch 8: train loss 0.012712180782782745 val loss 0.02579958690330386\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.05394638795405626\n",
      "Epoch 1: train loss 0.02217924217861819 val loss 0.05394638795405626\n",
      "saved best model epoch: 2 val loss is: 0.03731164522469044\n",
      "Epoch 2: train loss 0.017024208533476633 val loss 0.03731164522469044\n",
      "Epoch 3: train loss 0.016386669757495444 val loss 0.039628478698432446\n",
      "Epoch 4: train loss 0.016159391086772983 val loss 0.04215957410633564\n",
      "Epoch 5: train loss 0.015610584113971296 val loss 0.04547310620546341\n",
      "Epoch 6: train loss 0.015018225120133665 val loss 0.0421921219676733\n",
      "Epoch 7: train loss 0.013906018034252057 val loss 0.04685465432703495\n",
      "Epoch 8: train loss 0.012762270778058523 val loss 0.05135950446128845\n",
      "Epoch     8: reducing learning rate of group 0 to 7.0000e-05.\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.020092541724443434\n",
      "Epoch 1: train loss 0.03458834228859771 val loss 0.020092541724443434\n",
      "saved best model epoch: 2 val loss is: 0.01966281794011593\n",
      "Epoch 2: train loss 0.028968674995537316 val loss 0.01966281794011593\n",
      "Epoch 3: train loss 0.02847803798725917 val loss 0.019898752495646477\n",
      "Epoch 4: train loss 0.02783358704653524 val loss 0.020083936303853987\n",
      "Epoch 5: train loss 0.02685919643512794 val loss 0.022667815908789635\n",
      "Epoch 6: train loss 0.02578524701918165 val loss 0.02254798971116543\n",
      "Epoch 7: train loss 0.025301491509058645 val loss 0.024956594035029412\n",
      "Epoch 8: train loss 0.023965115437195414 val loss 0.02243170849978924\n",
      "Epoch     8: reducing learning rate of group 0 to 7.0000e-05.\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.018691856414079666\n",
      "Epoch 1: train loss 0.015075799687543795 val loss 0.018691856414079666\n",
      "saved best model epoch: 2 val loss is: 0.016743604838848115\n",
      "Epoch 2: train loss 0.011274311658261078 val loss 0.016743604838848115\n",
      "saved best model epoch: 3 val loss is: 0.015733280405402183\n",
      "Epoch 3: train loss 0.010784541389771871 val loss 0.015733280405402183\n",
      "saved best model epoch: 4 val loss is: 0.01493067853152752\n",
      "Epoch 4: train loss 0.010468072512940992 val loss 0.01493067853152752\n",
      "saved best model epoch: 5 val loss is: 0.01396320629864931\n",
      "Epoch 5: train loss 0.010273774198832967 val loss 0.01396320629864931\n",
      "Epoch 6: train loss 0.010152335478258985 val loss 0.01509640421718359\n",
      "Epoch 7: train loss 0.009939371153623575 val loss 0.015360203571617603\n",
      "Epoch 8: train loss 0.009734672849022206 val loss 0.016934173926711083\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.009812348708510398\n",
      "Epoch 1: train loss 0.011285956860298202 val loss 0.009812348708510398\n",
      "saved best model epoch: 2 val loss is: 0.009745930135250092\n",
      "Epoch 2: train loss 0.008836348430209216 val loss 0.009745930135250092\n",
      "saved best model epoch: 3 val loss is: 0.008233178034424781\n",
      "Epoch 3: train loss 0.008487518688309052 val loss 0.008233178034424781\n",
      "Epoch 4: train loss 0.008354479280699576 val loss 0.008608040399849415\n",
      "Epoch 5: train loss 0.008203677733295731 val loss 0.009136944264173507\n",
      "Epoch 6: train loss 0.008047335454085399 val loss 0.01005605049431324\n",
      "Epoch 7: train loss 0.00795249206878777 val loss 0.009007071517407894\n",
      "Epoch 8: train loss 0.007779829442456719 val loss 0.008799588494002818\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.046714216470718384\n",
      "Epoch 1: train loss 0.025826233106532266 val loss 0.046714216470718384\n",
      "saved best model epoch: 2 val loss is: 0.036852990835905076\n",
      "Epoch 2: train loss 0.017339234961019384 val loss 0.036852990835905076\n",
      "Epoch 3: train loss 0.016987837496257964 val loss 0.03941105902194977\n",
      "Epoch 4: train loss 0.016534364349874004 val loss 0.038830258697271344\n",
      "Epoch 5: train loss 0.016321501849840086 val loss 0.04223173260688782\n",
      "Epoch 6: train loss 0.015760072229784868 val loss 0.04350303635001183\n",
      "saved best model epoch: 7 val loss is: 0.031138669326901437\n",
      "Epoch 7: train loss 0.015607024698207775 val loss 0.031138669326901437\n",
      "Epoch 8: train loss 0.015360589615911954 val loss 0.04178188741207123\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.015667536482214927\n",
      "Epoch 1: train loss 0.019867362498882272 val loss 0.015667536482214927\n",
      "Epoch 2: train loss 0.016448528018026126 val loss 0.016121906042099\n",
      "Epoch 3: train loss 0.015962623469975023 val loss 0.01637333519756794\n",
      "Epoch 4: train loss 0.015212009771771375 val loss 0.01812833398580551\n",
      "Epoch 5: train loss 0.01442949089132959 val loss 0.017379732429981233\n",
      "Epoch 6: train loss 0.01342326432599553 val loss 0.018800373747944833\n",
      "saved best model epoch: 7 val loss is: 0.015595637075603009\n",
      "Epoch 7: train loss 0.012723049825234782 val loss 0.015595637075603009\n",
      "Epoch 8: train loss 0.011465448320710234 val loss 0.017078741267323495\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.020668099448084832\n",
      "Epoch 1: train loss 0.02967769932001829 val loss 0.020668099448084832\n",
      "Epoch 2: train loss 0.02613047111247267 val loss 0.02074289359152317\n",
      "saved best model epoch: 3 val loss is: 0.017906352505087854\n",
      "Epoch 3: train loss 0.02546164056374913 val loss 0.017906352505087854\n",
      "Epoch 4: train loss 0.025093235141996826 val loss 0.01892692781984806\n",
      "Epoch 5: train loss 0.024894387949080693 val loss 0.018244649469852447\n",
      "Epoch 6: train loss 0.024683797865041664 val loss 0.01858431287109852\n",
      "Epoch 7: train loss 0.02448495506264624 val loss 0.019112270697951315\n",
      "Epoch 8: train loss 0.024250379852240996 val loss 0.019647125527262686\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.015214960556477308\n",
      "Epoch 1: train loss 0.023879382961306227 val loss 0.015214960556477308\n",
      "Epoch 2: train loss 0.021978633276310312 val loss 0.015262369066476822\n",
      "Epoch 3: train loss 0.021634442776621104 val loss 0.01572700683027506\n",
      "Epoch 4: train loss 0.021202881876603668 val loss 0.015354686183854938\n",
      "saved best model epoch: 5 val loss is: 0.014800063567236066\n",
      "Epoch 5: train loss 0.02075151594198612 val loss 0.014800063567236066\n",
      "Epoch 6: train loss 0.0203720742544855 val loss 0.014824989484623075\n",
      "Epoch 7: train loss 0.019777210743491907 val loss 0.014900928363204002\n",
      "saved best model epoch: 8 val loss is: 0.014570496045053005\n",
      "Epoch 8: train loss 0.019429599148142768 val loss 0.014570496045053005\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.027693807613104582\n",
      "Epoch 1: train loss 0.023926184748310642 val loss 0.027693807613104582\n",
      "saved best model epoch: 2 val loss is: 0.026748798321932554\n",
      "Epoch 2: train loss 0.019899560183466197 val loss 0.026748798321932554\n",
      "Epoch 3: train loss 0.019001476332006686 val loss 0.028921309392899275\n",
      "saved best model epoch: 4 val loss is: 0.02486493205651641\n",
      "Epoch 4: train loss 0.01856664794845035 val loss 0.02486493205651641\n",
      "Epoch 5: train loss 0.018266919798341143 val loss 0.028284423518925905\n",
      "Epoch 6: train loss 0.01773496434451586 val loss 0.025459649972617626\n",
      "Epoch 7: train loss 0.017064981050340527 val loss 0.02514745807275176\n",
      "Epoch 8: train loss 0.017074198136667173 val loss 0.026253627613186836\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.022796295955777167\n",
      "Epoch 1: train loss 0.02261271919789059 val loss 0.022796295955777167\n",
      "saved best model epoch: 2 val loss is: 0.021967212110757826\n",
      "Epoch 2: train loss 0.018470588073666607 val loss 0.021967212110757826\n",
      "Epoch 3: train loss 0.017743334218504884 val loss 0.02328215353190899\n",
      "saved best model epoch: 4 val loss is: 0.019774146005511285\n",
      "Epoch 4: train loss 0.016960191595855923 val loss 0.019774146005511285\n",
      "Epoch 5: train loss 0.01626850844227842 val loss 0.019993208721280097\n",
      "Epoch 6: train loss 0.015780294535770303 val loss 0.02285667508840561\n",
      "Epoch 7: train loss 0.01506876507552252 val loss 0.02358766607940197\n",
      "saved best model epoch: 8 val loss is: 0.019446101784706116\n",
      "Epoch 8: train loss 0.014813025304604144 val loss 0.019446101784706116\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.014570785872638225\n",
      "Epoch 1: train loss 0.023787150691662515 val loss 0.014570785872638225\n",
      "saved best model epoch: 2 val loss is: 0.013694306090474128\n",
      "Epoch 2: train loss 0.021030711564457134 val loss 0.013694306090474128\n",
      "Epoch 3: train loss 0.01997078371988166 val loss 0.01402215901762247\n",
      "Epoch 4: train loss 0.01928362755903176 val loss 0.013727952912449836\n",
      "Epoch 5: train loss 0.01871166302866879 val loss 0.01389888245612383\n",
      "Epoch 6: train loss 0.018077501761061803 val loss 0.014110727608203888\n",
      "Epoch 7: train loss 0.017437264256711518 val loss 0.014011814445257186\n",
      "Epoch 8: train loss 0.01718548558918493 val loss 0.014346869476139546\n",
      "Epoch     8: reducing learning rate of group 0 to 7.0000e-05.\n",
      "1148\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 83 series\n",
      "train labels  has: 83 series\n",
      "validiation  X  has: 4 series\n",
      "Validiation  labels  has: 4 series\n",
      "trainX shape is: torch.Size([83, 10, 507])\n",
      "trainy shape is: torch.Size([83, 347, 1])\n",
      "train features  shape is: torch.Size([83, 347, 506])\n",
      "validX shape is: torch.Size([4, 10, 507])\n",
      "validy shape is: torch.Size([4, 347, 1])\n",
      "valid features  shape is: torch.Size([4, 347, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved best model epoch: 1 val loss is: 0.016903340816497803\n",
      "Epoch 1: train loss 0.015456319057259214 val loss 0.016903340816497803\n",
      "saved best model epoch: 2 val loss is: 0.016595954075455666\n",
      "Epoch 2: train loss 0.012360795131738645 val loss 0.016595954075455666\n",
      "Epoch 3: train loss 0.011992699556411749 val loss 0.017002688720822334\n",
      "saved best model epoch: 4 val loss is: 0.016476061660796404\n",
      "Epoch 4: train loss 0.011878999028669065 val loss 0.016476061660796404\n",
      "Epoch 5: train loss 0.011669660660337252 val loss 0.01649800967425108\n",
      "Epoch 6: train loss 0.011226874114160078 val loss 0.01774146594107151\n",
      "Epoch 7: train loss 0.010646454922585603 val loss 0.018164494074881077\n",
      "Epoch 8: train loss 0.009747146247291422 val loss 0.016854883637279272\n",
      "1147\n",
      "train size is: 440\n",
      "validation size is: 361\n",
      "train data shape is: (440, 507)\n",
      "validation data shape is: (361, 507)\n",
      "train X  has: 84 series\n",
      "train labels  has: 84 series\n",
      "validiation  X  has: 5 series\n",
      "Validiation  labels  has: 5 series\n",
      "trainX shape is: torch.Size([84, 10, 507])\n",
      "trainy shape is: torch.Size([84, 346, 1])\n",
      "train features  shape is: torch.Size([84, 346, 506])\n",
      "validX shape is: torch.Size([5, 10, 507])\n",
      "validy shape is: torch.Size([5, 346, 1])\n",
      "valid features  shape is: torch.Size([5, 346, 506])\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(507, 512, num_layers=3, batch_first=True, dropout=0.35)\n",
      "    (output_layer): Linear(in_features=512, out_features=507, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/8 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='80' class='' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      95.24% [80/84 00:40<00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_princip = pd.DataFrame([])\n",
    "for pid in progress_bar(pid_selection[250:300]):\n",
    "   df_aux = seq2seq_global(pid = pid, seq_length = 10, epoch = 8)\n",
    "   df_princip = pd.concat([df_princip, df_aux], axis = 0)\n",
    "\n",
    "df_princip.to_csv('/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/prediction/lstm_pid/250_300.csv', sep=',', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQ9GzamJ24NX"
   },
   "outputs": [],
   "source": [
    "df_princip = pd.DataFrame([])\n",
    "for pid in progress_bar(list(test_dataset['pid'].unique())[100:200]):\n",
    "   df_aux = seq2seq_global(pid = pid, seq_length = 10, epoch = 5)\n",
    "   df_princip = pd.concat([df_princip, df_aux], axis = 0)\n",
    "\n",
    "df_princip.to_csv('princip_200.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uA9PCk0i3qmB"
   },
   "outputs": [],
   "source": [
    "df_princip = pd.DataFrame([])\n",
    "for pid in progress_bar(list(test_dataset['pid'].unique())[200:300]):\n",
    "   df_aux = seq2seq_global(pid = pid, seq_length = 10, epoch = 5)\n",
    "   df_princip = pd.concat([df_princip, df_aux], axis = 0)\n",
    "\n",
    "df_princip.to_csv('princip_300.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44vN_Qkq315H"
   },
   "outputs": [],
   "source": [
    "df_princip = pd.DataFrame([])\n",
    "for pid in progress_bar(list(test_dataset['pid'].unique())[300:400]):\n",
    "   df_aux = seq2seq_global(pid = pid, seq_length = 10, epoch = 5)\n",
    "   df_princip = pd.concat([df_princip, df_aux], axis = 0)\n",
    "\n",
    "df_princip.to_csv('princip_400.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4gRE_2q3DTn"
   },
   "source": [
    "##### Get csv pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBXuKz8mjAQS"
   },
   "outputs": [],
   "source": [
    "lstm_0_50 = pd.read_csv(\"/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/prediction/lstm_pid/0_50.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BpS9PjDuJJg"
   },
   "outputs": [],
   "source": [
    "lstm_50_100 = pd.read_csv(\"/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/prediction/lstm_pid/50_100.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZGdO-hkuH4z"
   },
   "outputs": [],
   "source": [
    "lstm_100_150 = pd.read_csv(\"/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/prediction/lstm_pid/100_150.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEooVdKYuYUv"
   },
   "outputs": [],
   "source": [
    "lstm_150_200 = pd.read_csv(\"/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/prediction/lstm_pid/150_200.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqEaievmucd1"
   },
   "outputs": [],
   "source": [
    "lstm_200_250 = pd.read_csv(\"/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/prediction/lstm_pid/200_250.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aNADpLOufz7"
   },
   "outputs": [],
   "source": [
    "def prep_bf_stacking(lstm_prep): \n",
    "  lstm_prep = lstm_prep.set_index([lstm_prep['Unnamed: 0']])\n",
    "  lstm_prep = lstm_prep.drop('Unnamed: 0',axis = 1)\n",
    "  lstm_prep.index.name=\"\"\n",
    "  return lstm_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yexTYJpfvA7f"
   },
   "outputs": [],
   "source": [
    "lstm_0_50 = prep_bf_stacking(lstm_0_50)\n",
    "lstm_50_100 = prep_bf_stacking(lstm_50_100)\n",
    "lstm_100_150 = prep_bf_stacking(lstm_100_150)\n",
    "lstm_150_200 = prep_bf_stacking(lstm_150_200)\n",
    "lstm_200_250 = prep_bf_stacking(lstm_200_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WABOc3lTvfvH"
   },
   "outputs": [],
   "source": [
    "lstm_total = pd.concat([lstm_0_50, lstm_50_100, lstm_100_150, lstm_150_200, lstm_200_250], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDnFO2p1xGur"
   },
   "outputs": [],
   "source": [
    "lstm_total[target] = y_scaler.inverse_transform(lstm_total[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06Ab07tcvyPU"
   },
   "outputs": [],
   "source": [
    "lstm_total = lstm_total[['pid', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1613310522012,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "wxp5OxuGzL-A",
    "outputId": "2046c091-0d11-4c28-dbf6-c63725f60d4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6157601871487626"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_total['target'][60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "executionInfo": {
     "elapsed": 753,
     "status": "ok",
     "timestamp": 1613310546844,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "kjiwgwFqzUTR",
    "outputId": "75f17209-bafb-4c5e-d530-ef6db335fa40"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>pid</th>\n",
       "      <th>abs_ret0</th>\n",
       "      <th>abs_ret1</th>\n",
       "      <th>abs_ret2</th>\n",
       "      <th>abs_ret3</th>\n",
       "      <th>abs_ret4</th>\n",
       "      <th>abs_ret5</th>\n",
       "      <th>abs_ret6</th>\n",
       "      <th>abs_ret7</th>\n",
       "      <th>abs_ret8</th>\n",
       "      <th>abs_ret9</th>\n",
       "      <th>abs_ret10</th>\n",
       "      <th>abs_ret11</th>\n",
       "      <th>abs_ret12</th>\n",
       "      <th>abs_ret13</th>\n",
       "      <th>abs_ret14</th>\n",
       "      <th>abs_ret15</th>\n",
       "      <th>abs_ret16</th>\n",
       "      <th>abs_ret17</th>\n",
       "      <th>abs_ret18</th>\n",
       "      <th>abs_ret19</th>\n",
       "      <th>abs_ret20</th>\n",
       "      <th>abs_ret21</th>\n",
       "      <th>abs_ret22</th>\n",
       "      <th>abs_ret23</th>\n",
       "      <th>abs_ret24</th>\n",
       "      <th>abs_ret25</th>\n",
       "      <th>abs_ret26</th>\n",
       "      <th>abs_ret27</th>\n",
       "      <th>abs_ret28</th>\n",
       "      <th>abs_ret29</th>\n",
       "      <th>abs_ret30</th>\n",
       "      <th>abs_ret31</th>\n",
       "      <th>abs_ret32</th>\n",
       "      <th>abs_ret33</th>\n",
       "      <th>abs_ret34</th>\n",
       "      <th>abs_ret35</th>\n",
       "      <th>abs_ret36</th>\n",
       "      <th>abs_ret37</th>\n",
       "      <th>...</th>\n",
       "      <th>median_day_abs_ret36</th>\n",
       "      <th>median_day_abs_ret37</th>\n",
       "      <th>median_day_abs_ret38</th>\n",
       "      <th>median_day_abs_ret39</th>\n",
       "      <th>median_day_abs_ret40</th>\n",
       "      <th>median_day_abs_ret41</th>\n",
       "      <th>median_day_abs_ret42</th>\n",
       "      <th>median_day_abs_ret43</th>\n",
       "      <th>median_day_abs_ret44</th>\n",
       "      <th>median_day_abs_ret45</th>\n",
       "      <th>median_day_abs_ret46</th>\n",
       "      <th>median_day_abs_ret47</th>\n",
       "      <th>median_day_abs_ret48</th>\n",
       "      <th>median_day_abs_ret49</th>\n",
       "      <th>median_day_abs_ret50</th>\n",
       "      <th>median_day_abs_ret51</th>\n",
       "      <th>median_day_abs_ret52</th>\n",
       "      <th>median_day_abs_ret53</th>\n",
       "      <th>median_day_abs_ret54</th>\n",
       "      <th>median_day_abs_ret55</th>\n",
       "      <th>median_day_abs_ret56</th>\n",
       "      <th>median_day_abs_ret57</th>\n",
       "      <th>median_day_abs_ret58</th>\n",
       "      <th>median_day_abs_ret59</th>\n",
       "      <th>median_day_abs_ret60</th>\n",
       "      <th>min_ret</th>\n",
       "      <th>max_ret</th>\n",
       "      <th>std_ret</th>\n",
       "      <th>median_ret</th>\n",
       "      <th>sum_ret</th>\n",
       "      <th>min_vol</th>\n",
       "      <th>max_vol</th>\n",
       "      <th>std_vol</th>\n",
       "      <th>median_vol</th>\n",
       "      <th>median_day_sum_ret</th>\n",
       "      <th>median_day_sum_ret_before</th>\n",
       "      <th>kmeans_cluster_median_day_sum_ret_before</th>\n",
       "      <th>abs_kmeans_cluster_median_day_sum_ret_before</th>\n",
       "      <th>median_target_dummy</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>805</td>\n",
       "      <td>214</td>\n",
       "      <td>-0.998792</td>\n",
       "      <td>-0.996975</td>\n",
       "      <td>-0.994605</td>\n",
       "      <td>-0.967703</td>\n",
       "      <td>-0.982166</td>\n",
       "      <td>-0.985294</td>\n",
       "      <td>-0.996117</td>\n",
       "      <td>-0.975179</td>\n",
       "      <td>-0.987705</td>\n",
       "      <td>-0.989708</td>\n",
       "      <td>-0.994561</td>\n",
       "      <td>-0.997324</td>\n",
       "      <td>-0.985755</td>\n",
       "      <td>-0.993519</td>\n",
       "      <td>-0.990352</td>\n",
       "      <td>-0.985058</td>\n",
       "      <td>-0.975604</td>\n",
       "      <td>-0.946162</td>\n",
       "      <td>-0.987141</td>\n",
       "      <td>-0.983203</td>\n",
       "      <td>-0.997235</td>\n",
       "      <td>-0.944920</td>\n",
       "      <td>-0.975361</td>\n",
       "      <td>-0.966539</td>\n",
       "      <td>-0.961316</td>\n",
       "      <td>-0.979002</td>\n",
       "      <td>-0.993838</td>\n",
       "      <td>-0.994253</td>\n",
       "      <td>-0.989112</td>\n",
       "      <td>-0.970188</td>\n",
       "      <td>-0.994048</td>\n",
       "      <td>-0.955722</td>\n",
       "      <td>-0.997139</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.982392</td>\n",
       "      <td>-0.981662</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.918722</td>\n",
       "      <td>-0.916961</td>\n",
       "      <td>-0.841969</td>\n",
       "      <td>-0.775322</td>\n",
       "      <td>-0.891495</td>\n",
       "      <td>-0.898596</td>\n",
       "      <td>-0.948592</td>\n",
       "      <td>-0.869917</td>\n",
       "      <td>-0.888285</td>\n",
       "      <td>-0.893857</td>\n",
       "      <td>-0.957270</td>\n",
       "      <td>-0.897335</td>\n",
       "      <td>-0.909165</td>\n",
       "      <td>-0.911040</td>\n",
       "      <td>-0.913363</td>\n",
       "      <td>-0.880827</td>\n",
       "      <td>-0.869772</td>\n",
       "      <td>-0.921078</td>\n",
       "      <td>-0.976912</td>\n",
       "      <td>-0.900332</td>\n",
       "      <td>-0.951813</td>\n",
       "      <td>-0.921976</td>\n",
       "      <td>-0.872759</td>\n",
       "      <td>-0.960537</td>\n",
       "      <td>-0.922098</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.996068</td>\n",
       "      <td>-0.994955</td>\n",
       "      <td>-0.929561</td>\n",
       "      <td>-0.969322</td>\n",
       "      <td>-0.916057</td>\n",
       "      <td>-0.837442</td>\n",
       "      <td>-0.931418</td>\n",
       "      <td>-0.902111</td>\n",
       "      <td>-0.827446</td>\n",
       "      <td>-0.609281</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.615760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>806</td>\n",
       "      <td>214</td>\n",
       "      <td>-0.999396</td>\n",
       "      <td>-0.998791</td>\n",
       "      <td>-0.994623</td>\n",
       "      <td>-0.975238</td>\n",
       "      <td>-0.989876</td>\n",
       "      <td>-0.976661</td>\n",
       "      <td>-0.988469</td>\n",
       "      <td>-0.971904</td>\n",
       "      <td>-0.969477</td>\n",
       "      <td>-0.993173</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.993346</td>\n",
       "      <td>-0.981117</td>\n",
       "      <td>-0.990335</td>\n",
       "      <td>-0.985612</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.990304</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.961704</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.994495</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.980398</td>\n",
       "      <td>-0.993334</td>\n",
       "      <td>-0.987179</td>\n",
       "      <td>-0.989568</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.989176</td>\n",
       "      <td>-0.970358</td>\n",
       "      <td>-0.964492</td>\n",
       "      <td>-0.980442</td>\n",
       "      <td>-0.994307</td>\n",
       "      <td>-0.958707</td>\n",
       "      <td>-0.982511</td>\n",
       "      <td>-0.996356</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.996933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.935412</td>\n",
       "      <td>-0.959510</td>\n",
       "      <td>-0.886201</td>\n",
       "      <td>-0.912009</td>\n",
       "      <td>-0.919540</td>\n",
       "      <td>-0.914398</td>\n",
       "      <td>-0.560111</td>\n",
       "      <td>-0.871354</td>\n",
       "      <td>-0.925036</td>\n",
       "      <td>-0.914469</td>\n",
       "      <td>-0.909200</td>\n",
       "      <td>-0.919728</td>\n",
       "      <td>-0.920893</td>\n",
       "      <td>-0.955698</td>\n",
       "      <td>-0.913621</td>\n",
       "      <td>-0.937541</td>\n",
       "      <td>-0.904180</td>\n",
       "      <td>-0.951593</td>\n",
       "      <td>-0.967610</td>\n",
       "      <td>-0.928309</td>\n",
       "      <td>-0.918630</td>\n",
       "      <td>-0.931308</td>\n",
       "      <td>-0.939301</td>\n",
       "      <td>-0.921071</td>\n",
       "      <td>-0.926814</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.996688</td>\n",
       "      <td>-0.995560</td>\n",
       "      <td>-0.943922</td>\n",
       "      <td>-0.975497</td>\n",
       "      <td>-0.766372</td>\n",
       "      <td>-0.789268</td>\n",
       "      <td>-0.903389</td>\n",
       "      <td>-0.926190</td>\n",
       "      <td>-0.871451</td>\n",
       "      <td>-0.456159</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.751287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>807</td>\n",
       "      <td>214</td>\n",
       "      <td>-0.998491</td>\n",
       "      <td>-0.998188</td>\n",
       "      <td>-0.974930</td>\n",
       "      <td>-0.965436</td>\n",
       "      <td>-0.992405</td>\n",
       "      <td>-0.994162</td>\n",
       "      <td>-0.992296</td>\n",
       "      <td>-0.975391</td>\n",
       "      <td>-0.963428</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.989187</td>\n",
       "      <td>-0.998671</td>\n",
       "      <td>-0.990561</td>\n",
       "      <td>-0.996777</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.985136</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.976179</td>\n",
       "      <td>-0.993612</td>\n",
       "      <td>-0.966602</td>\n",
       "      <td>-0.997244</td>\n",
       "      <td>-0.969505</td>\n",
       "      <td>-0.985277</td>\n",
       "      <td>-0.973327</td>\n",
       "      <td>-0.993593</td>\n",
       "      <td>-0.979149</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.985723</td>\n",
       "      <td>-0.994588</td>\n",
       "      <td>-0.975313</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.985309</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.988727</td>\n",
       "      <td>-0.994157</td>\n",
       "      <td>-0.985387</td>\n",
       "      <td>-0.987831</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.904738</td>\n",
       "      <td>-0.926459</td>\n",
       "      <td>-0.855547</td>\n",
       "      <td>-0.865862</td>\n",
       "      <td>-0.746405</td>\n",
       "      <td>-0.900262</td>\n",
       "      <td>-0.941300</td>\n",
       "      <td>-0.913384</td>\n",
       "      <td>-0.341424</td>\n",
       "      <td>-0.767287</td>\n",
       "      <td>-0.853354</td>\n",
       "      <td>-0.760114</td>\n",
       "      <td>-0.845250</td>\n",
       "      <td>-0.942385</td>\n",
       "      <td>-0.904648</td>\n",
       "      <td>-0.928085</td>\n",
       "      <td>-0.887867</td>\n",
       "      <td>-0.835214</td>\n",
       "      <td>-0.950566</td>\n",
       "      <td>-0.878652</td>\n",
       "      <td>-0.880590</td>\n",
       "      <td>-0.614363</td>\n",
       "      <td>-0.674310</td>\n",
       "      <td>-0.897351</td>\n",
       "      <td>-0.818738</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.995782</td>\n",
       "      <td>-0.994614</td>\n",
       "      <td>-0.915877</td>\n",
       "      <td>-0.971216</td>\n",
       "      <td>-0.812191</td>\n",
       "      <td>-0.761734</td>\n",
       "      <td>-0.910984</td>\n",
       "      <td>-0.914299</td>\n",
       "      <td>-0.872113</td>\n",
       "      <td>-0.361213</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.937945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>808</td>\n",
       "      <td>214</td>\n",
       "      <td>-0.996371</td>\n",
       "      <td>-0.999395</td>\n",
       "      <td>-0.978450</td>\n",
       "      <td>-0.970281</td>\n",
       "      <td>-0.982292</td>\n",
       "      <td>-0.988317</td>\n",
       "      <td>-0.953737</td>\n",
       "      <td>-0.989417</td>\n",
       "      <td>-0.993872</td>\n",
       "      <td>-0.986314</td>\n",
       "      <td>-0.994569</td>\n",
       "      <td>-0.997328</td>\n",
       "      <td>-0.914630</td>\n",
       "      <td>-0.987080</td>\n",
       "      <td>-0.927790</td>\n",
       "      <td>-0.977594</td>\n",
       "      <td>-0.985365</td>\n",
       "      <td>-0.976075</td>\n",
       "      <td>-0.922936</td>\n",
       "      <td>-0.987440</td>\n",
       "      <td>-0.994476</td>\n",
       "      <td>-0.951106</td>\n",
       "      <td>-0.960638</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.942036</td>\n",
       "      <td>-0.994755</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.985639</td>\n",
       "      <td>-0.991832</td>\n",
       "      <td>-0.995035</td>\n",
       "      <td>-0.964278</td>\n",
       "      <td>-0.975409</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.996220</td>\n",
       "      <td>-0.982361</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.995924</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.846363</td>\n",
       "      <td>-0.894526</td>\n",
       "      <td>-0.854136</td>\n",
       "      <td>-0.859500</td>\n",
       "      <td>-0.673064</td>\n",
       "      <td>-0.887111</td>\n",
       "      <td>-0.714475</td>\n",
       "      <td>-0.759479</td>\n",
       "      <td>-0.717495</td>\n",
       "      <td>-0.765939</td>\n",
       "      <td>-0.860156</td>\n",
       "      <td>-0.890289</td>\n",
       "      <td>-0.865347</td>\n",
       "      <td>-0.819580</td>\n",
       "      <td>-0.872172</td>\n",
       "      <td>-0.859830</td>\n",
       "      <td>-0.863936</td>\n",
       "      <td>-0.879732</td>\n",
       "      <td>-0.905963</td>\n",
       "      <td>-0.946556</td>\n",
       "      <td>-0.864930</td>\n",
       "      <td>-0.577143</td>\n",
       "      <td>-0.845183</td>\n",
       "      <td>-0.833443</td>\n",
       "      <td>-0.849790</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.994554</td>\n",
       "      <td>-0.993190</td>\n",
       "      <td>-0.915140</td>\n",
       "      <td>-0.960635</td>\n",
       "      <td>-0.439156</td>\n",
       "      <td>-0.860797</td>\n",
       "      <td>-0.936142</td>\n",
       "      <td>-0.914892</td>\n",
       "      <td>-0.661516</td>\n",
       "      <td>0.127179</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.067138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>809</td>\n",
       "      <td>214</td>\n",
       "      <td>-0.997556</td>\n",
       "      <td>-0.996324</td>\n",
       "      <td>-0.994536</td>\n",
       "      <td>-0.989934</td>\n",
       "      <td>-0.979377</td>\n",
       "      <td>-0.952498</td>\n",
       "      <td>-0.980380</td>\n",
       "      <td>-0.971361</td>\n",
       "      <td>-0.972002</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.977932</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.985550</td>\n",
       "      <td>-0.993425</td>\n",
       "      <td>-0.980403</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.980176</td>\n",
       "      <td>-0.987844</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.991473</td>\n",
       "      <td>-0.994376</td>\n",
       "      <td>-0.950179</td>\n",
       "      <td>-0.979994</td>\n",
       "      <td>-0.979623</td>\n",
       "      <td>-0.980420</td>\n",
       "      <td>-0.989383</td>\n",
       "      <td>-0.981293</td>\n",
       "      <td>-0.994183</td>\n",
       "      <td>-0.997247</td>\n",
       "      <td>-0.994978</td>\n",
       "      <td>-0.951848</td>\n",
       "      <td>-0.990039</td>\n",
       "      <td>-0.991298</td>\n",
       "      <td>-0.984696</td>\n",
       "      <td>-0.994046</td>\n",
       "      <td>-0.985108</td>\n",
       "      <td>-0.991745</td>\n",
       "      <td>-0.993739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.867292</td>\n",
       "      <td>-0.906663</td>\n",
       "      <td>-0.676915</td>\n",
       "      <td>-0.690599</td>\n",
       "      <td>-0.907964</td>\n",
       "      <td>-0.862460</td>\n",
       "      <td>-0.739261</td>\n",
       "      <td>-0.893753</td>\n",
       "      <td>-0.887520</td>\n",
       "      <td>-0.876114</td>\n",
       "      <td>-0.873154</td>\n",
       "      <td>-0.880359</td>\n",
       "      <td>-0.895330</td>\n",
       "      <td>-0.669400</td>\n",
       "      <td>-0.865957</td>\n",
       "      <td>-0.886505</td>\n",
       "      <td>-0.919783</td>\n",
       "      <td>-0.773840</td>\n",
       "      <td>-0.968083</td>\n",
       "      <td>-0.911080</td>\n",
       "      <td>-0.931815</td>\n",
       "      <td>-0.951224</td>\n",
       "      <td>-0.933929</td>\n",
       "      <td>-0.902397</td>\n",
       "      <td>-0.904556</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.995101</td>\n",
       "      <td>-0.994709</td>\n",
       "      <td>-0.942801</td>\n",
       "      <td>-0.970560</td>\n",
       "      <td>-0.645645</td>\n",
       "      <td>-0.459052</td>\n",
       "      <td>-0.815742</td>\n",
       "      <td>-0.930379</td>\n",
       "      <td>-0.749276</td>\n",
       "      <td>-0.521098</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.118526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308303</th>\n",
       "      <td>1148</td>\n",
       "      <td>816</td>\n",
       "      <td>-0.988902</td>\n",
       "      <td>-0.996943</td>\n",
       "      <td>-0.898575</td>\n",
       "      <td>-0.744733</td>\n",
       "      <td>-0.869373</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.980317</td>\n",
       "      <td>-0.826938</td>\n",
       "      <td>-0.901061</td>\n",
       "      <td>-0.907459</td>\n",
       "      <td>-0.963142</td>\n",
       "      <td>-0.997740</td>\n",
       "      <td>-0.935811</td>\n",
       "      <td>-0.920509</td>\n",
       "      <td>-0.828483</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.941814</td>\n",
       "      <td>-0.959284</td>\n",
       "      <td>-0.934377</td>\n",
       "      <td>-0.957050</td>\n",
       "      <td>-0.981149</td>\n",
       "      <td>-0.927059</td>\n",
       "      <td>-0.974819</td>\n",
       "      <td>-0.960097</td>\n",
       "      <td>-0.967040</td>\n",
       "      <td>-0.919622</td>\n",
       "      <td>-0.884404</td>\n",
       "      <td>-0.941048</td>\n",
       "      <td>-0.962668</td>\n",
       "      <td>-0.957346</td>\n",
       "      <td>-0.969275</td>\n",
       "      <td>-0.923740</td>\n",
       "      <td>-0.990106</td>\n",
       "      <td>-0.986962</td>\n",
       "      <td>-0.979730</td>\n",
       "      <td>-0.949286</td>\n",
       "      <td>-0.971920</td>\n",
       "      <td>-0.989368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.462782</td>\n",
       "      <td>-0.434240</td>\n",
       "      <td>-0.254473</td>\n",
       "      <td>0.134350</td>\n",
       "      <td>0.023775</td>\n",
       "      <td>0.403296</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.245930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.719844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.176973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.968925</td>\n",
       "      <td>-0.971523</td>\n",
       "      <td>-0.831285</td>\n",
       "      <td>-0.882056</td>\n",
       "      <td>-0.998104</td>\n",
       "      <td>-0.785168</td>\n",
       "      <td>-0.734404</td>\n",
       "      <td>-0.775849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.275295</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.715138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309203</th>\n",
       "      <td>1149</td>\n",
       "      <td>816</td>\n",
       "      <td>-0.974261</td>\n",
       "      <td>-0.983288</td>\n",
       "      <td>-0.936148</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.905785</td>\n",
       "      <td>-0.918508</td>\n",
       "      <td>-0.912293</td>\n",
       "      <td>-0.929086</td>\n",
       "      <td>-0.970475</td>\n",
       "      <td>-0.945829</td>\n",
       "      <td>-0.895029</td>\n",
       "      <td>-0.969583</td>\n",
       "      <td>-0.995835</td>\n",
       "      <td>-0.957328</td>\n",
       "      <td>-0.974606</td>\n",
       "      <td>-0.816049</td>\n",
       "      <td>-0.923142</td>\n",
       "      <td>-0.747968</td>\n",
       "      <td>-0.920594</td>\n",
       "      <td>-0.977802</td>\n",
       "      <td>-0.995125</td>\n",
       "      <td>-0.989207</td>\n",
       "      <td>-0.956597</td>\n",
       "      <td>-0.988216</td>\n",
       "      <td>-0.926387</td>\n",
       "      <td>-0.917029</td>\n",
       "      <td>-0.848602</td>\n",
       "      <td>-0.924642</td>\n",
       "      <td>-0.971575</td>\n",
       "      <td>-0.948062</td>\n",
       "      <td>-0.875618</td>\n",
       "      <td>-0.845273</td>\n",
       "      <td>-0.995023</td>\n",
       "      <td>-0.888747</td>\n",
       "      <td>-0.959131</td>\n",
       "      <td>-0.955346</td>\n",
       "      <td>-0.943267</td>\n",
       "      <td>-0.973121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.344313</td>\n",
       "      <td>-0.335965</td>\n",
       "      <td>-0.508588</td>\n",
       "      <td>0.007597</td>\n",
       "      <td>0.129695</td>\n",
       "      <td>-0.707337</td>\n",
       "      <td>-0.149776</td>\n",
       "      <td>-0.636016</td>\n",
       "      <td>-0.182767</td>\n",
       "      <td>-0.346883</td>\n",
       "      <td>-0.518401</td>\n",
       "      <td>-0.569716</td>\n",
       "      <td>-0.682240</td>\n",
       "      <td>-0.448331</td>\n",
       "      <td>-0.638369</td>\n",
       "      <td>-0.069430</td>\n",
       "      <td>0.277347</td>\n",
       "      <td>-0.572872</td>\n",
       "      <td>-0.863817</td>\n",
       "      <td>-0.741330</td>\n",
       "      <td>-0.701464</td>\n",
       "      <td>-0.234399</td>\n",
       "      <td>-0.659514</td>\n",
       "      <td>-0.791788</td>\n",
       "      <td>-0.046810</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.974261</td>\n",
       "      <td>-0.975985</td>\n",
       "      <td>-0.653955</td>\n",
       "      <td>-0.850259</td>\n",
       "      <td>-0.977982</td>\n",
       "      <td>-0.794262</td>\n",
       "      <td>-0.904196</td>\n",
       "      <td>-0.927531</td>\n",
       "      <td>0.182952</td>\n",
       "      <td>-0.934862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.706263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310103</th>\n",
       "      <td>1150</td>\n",
       "      <td>816</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.962225</td>\n",
       "      <td>-0.877061</td>\n",
       "      <td>-0.970017</td>\n",
       "      <td>-0.912505</td>\n",
       "      <td>-0.949280</td>\n",
       "      <td>-0.963168</td>\n",
       "      <td>-0.852739</td>\n",
       "      <td>-0.962545</td>\n",
       "      <td>-0.946479</td>\n",
       "      <td>-0.773129</td>\n",
       "      <td>-0.992982</td>\n",
       "      <td>-0.962669</td>\n",
       "      <td>-0.988676</td>\n",
       "      <td>-0.974712</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.923243</td>\n",
       "      <td>-0.916298</td>\n",
       "      <td>-0.820077</td>\n",
       "      <td>-0.963102</td>\n",
       "      <td>-0.922042</td>\n",
       "      <td>-0.860397</td>\n",
       "      <td>-0.905341</td>\n",
       "      <td>-0.903812</td>\n",
       "      <td>-0.944164</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.872156</td>\n",
       "      <td>-0.980067</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.995691</td>\n",
       "      <td>-0.943163</td>\n",
       "      <td>-0.923081</td>\n",
       "      <td>-0.970135</td>\n",
       "      <td>-0.980300</td>\n",
       "      <td>-0.867209</td>\n",
       "      <td>-0.853127</td>\n",
       "      <td>-0.950097</td>\n",
       "      <td>-0.959487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.640014</td>\n",
       "      <td>-0.487649</td>\n",
       "      <td>-0.158274</td>\n",
       "      <td>-0.099397</td>\n",
       "      <td>-0.006392</td>\n",
       "      <td>-0.759825</td>\n",
       "      <td>-0.643929</td>\n",
       "      <td>-0.479827</td>\n",
       "      <td>-0.282993</td>\n",
       "      <td>-0.552543</td>\n",
       "      <td>-0.719045</td>\n",
       "      <td>-0.095346</td>\n",
       "      <td>-0.224267</td>\n",
       "      <td>-0.578415</td>\n",
       "      <td>-0.778971</td>\n",
       "      <td>0.522835</td>\n",
       "      <td>-0.303654</td>\n",
       "      <td>-0.338124</td>\n",
       "      <td>-0.900506</td>\n",
       "      <td>-0.194498</td>\n",
       "      <td>-0.693373</td>\n",
       "      <td>-0.540350</td>\n",
       "      <td>-0.573285</td>\n",
       "      <td>-0.612770</td>\n",
       "      <td>-0.428338</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.962284</td>\n",
       "      <td>-0.968009</td>\n",
       "      <td>-0.618345</td>\n",
       "      <td>-0.848266</td>\n",
       "      <td>-0.997417</td>\n",
       "      <td>-0.860894</td>\n",
       "      <td>-0.914206</td>\n",
       "      <td>-0.926418</td>\n",
       "      <td>0.286611</td>\n",
       "      <td>-0.254908</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.699045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311003</th>\n",
       "      <td>1151</td>\n",
       "      <td>816</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.989725</td>\n",
       "      <td>-0.966860</td>\n",
       "      <td>-0.937718</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.980414</td>\n",
       "      <td>-0.954833</td>\n",
       "      <td>-0.881935</td>\n",
       "      <td>-0.923530</td>\n",
       "      <td>-0.994284</td>\n",
       "      <td>-0.936455</td>\n",
       "      <td>-0.995528</td>\n",
       "      <td>-0.976178</td>\n",
       "      <td>-0.891467</td>\n",
       "      <td>-0.951403</td>\n",
       "      <td>-0.849723</td>\n",
       "      <td>-0.991842</td>\n",
       "      <td>-0.889888</td>\n",
       "      <td>-0.903029</td>\n",
       "      <td>-0.936487</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.958897</td>\n",
       "      <td>-0.933933</td>\n",
       "      <td>-0.971957</td>\n",
       "      <td>-0.989193</td>\n",
       "      <td>-0.929688</td>\n",
       "      <td>-0.866102</td>\n",
       "      <td>-0.918066</td>\n",
       "      <td>-0.977082</td>\n",
       "      <td>-0.974945</td>\n",
       "      <td>-0.849686</td>\n",
       "      <td>-0.966996</td>\n",
       "      <td>-0.995200</td>\n",
       "      <td>-0.980992</td>\n",
       "      <td>-0.950732</td>\n",
       "      <td>-0.975391</td>\n",
       "      <td>-0.986336</td>\n",
       "      <td>-0.989634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.747407</td>\n",
       "      <td>-0.569196</td>\n",
       "      <td>-0.692772</td>\n",
       "      <td>0.004465</td>\n",
       "      <td>-0.587183</td>\n",
       "      <td>0.011805</td>\n",
       "      <td>-0.685823</td>\n",
       "      <td>-0.427228</td>\n",
       "      <td>-0.658543</td>\n",
       "      <td>-0.352214</td>\n",
       "      <td>-0.075219</td>\n",
       "      <td>-0.718618</td>\n",
       "      <td>-0.550562</td>\n",
       "      <td>-0.655667</td>\n",
       "      <td>-0.772794</td>\n",
       "      <td>0.079369</td>\n",
       "      <td>-0.670612</td>\n",
       "      <td>-0.761274</td>\n",
       "      <td>-0.711509</td>\n",
       "      <td>-0.157695</td>\n",
       "      <td>-0.358010</td>\n",
       "      <td>-0.704198</td>\n",
       "      <td>-0.642796</td>\n",
       "      <td>-0.755170</td>\n",
       "      <td>-0.411631</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.989741</td>\n",
       "      <td>-0.984299</td>\n",
       "      <td>-0.762756</td>\n",
       "      <td>-0.898068</td>\n",
       "      <td>-0.998067</td>\n",
       "      <td>-0.869935</td>\n",
       "      <td>-0.921197</td>\n",
       "      <td>-0.927111</td>\n",
       "      <td>-0.026124</td>\n",
       "      <td>-0.659225</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.693075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311612</th>\n",
       "      <td>1128</td>\n",
       "      <td>816</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.967490</td>\n",
       "      <td>-0.910143</td>\n",
       "      <td>-0.862053</td>\n",
       "      <td>-0.885492</td>\n",
       "      <td>-0.907785</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.922057</td>\n",
       "      <td>-0.984578</td>\n",
       "      <td>-0.930665</td>\n",
       "      <td>-0.971915</td>\n",
       "      <td>-0.992883</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.906380</td>\n",
       "      <td>-0.932928</td>\n",
       "      <td>-0.920089</td>\n",
       "      <td>-0.964439</td>\n",
       "      <td>-0.990466</td>\n",
       "      <td>-0.931466</td>\n",
       "      <td>-0.995902</td>\n",
       "      <td>-0.900181</td>\n",
       "      <td>-0.912654</td>\n",
       "      <td>-0.985099</td>\n",
       "      <td>-0.990438</td>\n",
       "      <td>-0.984446</td>\n",
       "      <td>-0.972597</td>\n",
       "      <td>-0.987197</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.882033</td>\n",
       "      <td>-0.982252</td>\n",
       "      <td>-0.955964</td>\n",
       "      <td>-0.987162</td>\n",
       "      <td>-0.943578</td>\n",
       "      <td>-0.982466</td>\n",
       "      <td>-0.972597</td>\n",
       "      <td>-0.981747</td>\n",
       "      <td>-0.997695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.701248</td>\n",
       "      <td>-0.594348</td>\n",
       "      <td>-0.628593</td>\n",
       "      <td>0.012691</td>\n",
       "      <td>0.334055</td>\n",
       "      <td>-0.294596</td>\n",
       "      <td>-0.004128</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.006637</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>-0.034039</td>\n",
       "      <td>0.003856</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>-0.006055</td>\n",
       "      <td>-0.002882</td>\n",
       "      <td>-0.007120</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>-0.011290</td>\n",
       "      <td>-0.629587</td>\n",
       "      <td>-0.007951</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>-0.135828</td>\n",
       "      <td>-0.001658</td>\n",
       "      <td>-0.408540</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.983592</td>\n",
       "      <td>-0.982456</td>\n",
       "      <td>-0.830703</td>\n",
       "      <td>-0.913254</td>\n",
       "      <td>-0.997266</td>\n",
       "      <td>-0.811982</td>\n",
       "      <td>-0.808236</td>\n",
       "      <td>-0.807458</td>\n",
       "      <td>0.081386</td>\n",
       "      <td>0.194020</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.684866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86549 rows × 508 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         day  pid  ...  median_target_dummy    target\n",
       "                   ...                               \n",
       "60       805  214  ...                    0 -1.615760\n",
       "960      806  214  ...                    0 -1.751287\n",
       "1860     807  214  ...                    0 -1.937945\n",
       "2760     808  214  ...                    0 -2.067138\n",
       "3659     809  214  ...                    0 -2.118526\n",
       "...      ...  ...  ...                  ...       ...\n",
       "308303  1148  816  ...                    0 -1.715138\n",
       "309203  1149  816  ...                    0 -1.706263\n",
       "310103  1150  816  ...                    0 -1.699045\n",
       "311003  1151  816  ...                    0 -1.693075\n",
       "311612  1128  816  ...                    0 -1.684866\n",
       "\n",
       "[86549 rows x 508 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-Dts2NwwiR9"
   },
   "outputs": [],
   "source": [
    "pred_lstm = pd.DataFrame(lstm_total['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1613310408874,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "R93nYb3UyPzu",
    "outputId": "27643cc0-89ae-40a0-f18e-829f1761caea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030344121"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lstm['target'][60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsYesOQtrpDV"
   },
   "outputs": [],
   "source": [
    "pred_edg = pd.read_csv('/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/prediction/predictions_ed_8.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQ3E24gdwnw6"
   },
   "outputs": [],
   "source": [
    "for index in pred_edg.index: \n",
    "  if index in lstm_total.index: \n",
    "    pred_edg['target'][index] = lstm_total['target'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1brYx5FRz5so"
   },
   "outputs": [],
   "source": [
    "pred_edg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97AdcaBMwcod"
   },
   "outputs": [],
   "source": [
    "pred_edg.to_csv('/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/prediction/predictions_stacking.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNyTPyRQ1cfO"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oImMsXV1OwV"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBtaC5G75NSA"
   },
   "outputs": [],
   "source": [
    "#test_pid = pd.DataFrame([])\n",
    "#for pid in list(test_dataset['pid'].unique())[0:100]:\n",
    "#  test_pid_aux = test_dataset[test_dataset['pid']==pid]\n",
    "#  test_pid = pd.concat([test_pid, test_pid_aux], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7NRCvx69tIV"
   },
   "outputs": [],
   "source": [
    "#for pred in pred_edg.index:\n",
    "#  if np.isnan(pred_edg['new'][pred]):\n",
    "#    pred_edg['new'][pred]=pred_edg['target'][pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1613238860869,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "_YRRaGu2fSSZ",
    "outputId": "fecbfe33-e0f7-4b62-a826-c4b4d60da979"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8727121296695675"
      ]
     },
     "execution_count": 238,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean_squared_error(differe[(differe['pid']==360) | (differe['pid']==203)][\"target\"],differe[(differe['pid']==360) | (differe['pid']==203)][\"new\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF7AMNDBtbXM"
   },
   "source": [
    "#### Details for one pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksYe3V8ykDL-"
   },
   "source": [
    "##### Set seed and retrieve train pid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3WqbGpU9cH8"
   },
   "outputs": [],
   "source": [
    "test_360=test_dataset[test_dataset['pid']==10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23149,
     "status": "ok",
     "timestamp": 1613227549804,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "K7Fqm4ae-vlu",
    "outputId": "c98c806f-56ce-4c99-dbe7-5228a728070a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346, 507)"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_360.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCtOCRQQ-NjX"
   },
   "outputs": [],
   "source": [
    "test_360[\"target\"]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jdyKtQH2Z6s"
   },
   "outputs": [],
   "source": [
    "train_360 = train_dataset[train_dataset['pid'] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eu99HZfd9yBE"
   },
   "outputs": [],
   "source": [
    "all_360=pd.concat([train_360,test_360])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTu_X4pT26mw"
   },
   "outputs": [],
   "source": [
    "all_360 = all_360.set_index(['day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23166,
     "status": "ok",
     "timestamp": 1613227549837,
     "user": {
      "displayName": "AI Master",
      "photoUrl": "",
      "userId": "16245283951919033553"
     },
     "user_tz": -60
    },
    "id": "ZoV3P63T3iyu",
    "outputId": "a6248142-4653-43c3-edbe-245413463515"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1147"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_360.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExRXL429RUCc"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxxjeElo3x3C"
   },
   "source": [
    "##### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22e7XAic3Xj_"
   },
   "outputs": [],
   "source": [
    "train_size = int((all_360.shape[0]-346) * 0.55)\n",
    "valid_size = (all_360.shape[0]-346)- train_size\n",
    "print(\"train size is:\",train_size)\n",
    "print(\"validation size is:\",valid_size)\n",
    "train_data = all_360.iloc[0:train_size,:]\n",
    "valid_data = all_360.iloc[train_size:train_size+valid_size,:]\n",
    "print(\"train data shape is:\",train_data.shape)\n",
    "print(\"validation data shape is:\",valid_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyCBRV_A3-_7"
   },
   "source": [
    "Multi-Dimensional Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElFWVg2d4FR8"
   },
   "outputs": [],
   "source": [
    "###  This function creates a sliding window or sequences of seq_length days and labels_length  days label ####\n",
    "def sliding_windows(data, seq_length,labels_length):\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "\n",
    "    for i in range(len(data)-(seq_length+labels_length)):\n",
    "        _x = data.iloc[i:(i+seq_length),:]\n",
    "        _y = data.iloc[(i+seq_length):(i+seq_length+labels_length),506:507]\n",
    "        _z  = data.iloc[(i+seq_length):(i+seq_length+labels_length),:506]\n",
    "        x.append(np.array(_x))\n",
    "        y.append(np.array(_y))\n",
    "        z.append(np.array(_z))\n",
    "\n",
    "    return x,y,z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLRgnmVL4LJD"
   },
   "source": [
    "We will use a sliding window of 10 days Our target(labels) is 100 days\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqyPn6Je4HOE"
   },
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "labels_length = 346\n",
    "train_X, train_y,train_features = sliding_windows(train_data, seq_length,labels_length)\n",
    "print(\"train X  has:\", len(train_X) , \"series\")\n",
    "print(\"train labels  has:\", len(train_y) , \"series\")\n",
    "valid_X, valid_y,valid_features = sliding_windows(valid_data, seq_length,labels_length)\n",
    "print(\"validiation  X  has:\", len(valid_X) , \"series\")\n",
    "print(\"Validiation  labels  has:\" ,len(valid_y) , \"series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11f2nElRkmqZ"
   },
   "source": [
    "##### Plot Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7uVvaedkp__"
   },
   "source": [
    "To verify the concept Let's plot\n",
    "\n",
    "    The original TS (take the length of the sliding window + the label size - 90 days + 28 days)\n",
    "    The first array of trainX - which is the first sequence\n",
    "    The first array of trainy , which is the first label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehntroJPh55-"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,17))\n",
    "fig, axs =plt.subplots(3,figsize=(12,9))\n",
    "\n",
    "axs[0].plot(train_data[0:seq_length+labels_length])\n",
    "axs[0].title.set_text('Original Time Series')\n",
    "axs[0].set_xlim(0,seq_length+labels_length)\n",
    "axs[0].set_ylim(-1,1)\n",
    "axs[1].plot(train_X[0].flatten(),color=\"red\")\n",
    "axs[1].title.set_text('Train Data')\n",
    "axs[1].set_xlim(0,seq_length+labels_length)\n",
    "axs[1].set_ylim(-1,1)\n",
    "axs[2].plot(np.pad(train_y[0].flatten(),seq_length),color='black')\n",
    "axs[2].title.set_text('Labels Data')\n",
    "axs[2].set_xlim(0,seq_length+labels_length)\n",
    "axs[2].set_ylim(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZfU7OfK4_1o"
   },
   "outputs": [],
   "source": [
    "train_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVHDJ9by5C60"
   },
   "outputs": [],
   "source": [
    "train_y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQF4BQhF5HUc"
   },
   "outputs": [],
   "source": [
    "train_features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlgdUJsi5Oyh"
   },
   "source": [
    "Convert to Pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NsJa7vl5WUO"
   },
   "outputs": [],
   "source": [
    "trainX = Variable(torch.Tensor(train_X))\n",
    "trainy = Variable(torch.Tensor(train_y))\n",
    "train_features = Variable(torch.Tensor(train_features))\n",
    "validX = Variable(torch.Tensor(valid_X))\n",
    "validy= Variable(torch.Tensor(valid_y))\n",
    "valid_features = Variable(torch.Tensor(valid_features))\n",
    "\n",
    "\n",
    "print (\"trainX shape is:\",trainX.size())\n",
    "print (\"trainy shape is:\",trainy.size())\n",
    "print (\"train features  shape is:\",train_features.size())\n",
    "print (\"validX shape is:\",validX.size())\n",
    "print (\"validy shape is:\",validy.size())\n",
    "print (\"valid features  shape is:\",valid_features.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECcIzcnu6RF2"
   },
   "source": [
    "Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bub7hYlG6XFb"
   },
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_m3B0d56YY6"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.embedding_dim, self.hidden_dim = embedding_dim,  embedding_dim\n",
    "        self.num_layers = 3\n",
    "        self.rnn1 = nn.LSTM(\n",
    "          input_size=n_features,\n",
    "          hidden_size=self.hidden_dim,\n",
    "          num_layers=3,\n",
    "          batch_first=True,\n",
    "          dropout = 0.35\n",
    "        )\n",
    "   \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.reshape((1, self.seq_len, self.n_features))\n",
    "        \n",
    "        h_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_dim).to(device))\n",
    "         \n",
    "        \n",
    "        c_1 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_dim).to(device))\n",
    "              \n",
    "        x, (hidden, cell) = self.rnn1(x,(h_1, c_1))\n",
    "        \n",
    "        \n",
    "        #return hidden_n.reshape((self.n_features, self.embedding_dim))\n",
    "        return hidden , cell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOJ1RXCU6oMV"
   },
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVV6pJCD6qPd"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim=64, n_features=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.input_dim = seq_len, input_dim\n",
    "        self.hidden_dim, self.n_features =  input_dim, n_features\n",
    "        \n",
    "        self.rnn1 = nn.LSTM(\n",
    "          input_size=n_features,\n",
    "          hidden_size=input_dim,\n",
    "          num_layers=3,\n",
    "          batch_first=True,\n",
    "          dropout = 0.35\n",
    "        )\n",
    "        \n",
    "        \n",
    "      \n",
    "        self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "    def forward(self, x,input_hidden,input_cell):\n",
    "       \n",
    "       \n",
    "        x = x.reshape((1,1,self.n_features ))\n",
    "        #print(\"decode input\",x.size())\n",
    "             \n",
    "\n",
    "        x, (hidden_n, cell_n) = self.rnn1(x,(input_hidden,input_cell))\n",
    "    \n",
    "        x = self.output_layer(x)\n",
    "        return x, hidden_n, cell_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4nVEwN_6yUQ"
   },
   "source": [
    "Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNu6SK5H60xd"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64,output_length = 346):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "        self.n_features = n_features\n",
    "        self.output_length = output_length\n",
    "        self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
    "        \n",
    "\n",
    "    def forward(self,x, prev_y,features):\n",
    "        \n",
    "       \n",
    "        hidden,cell = self.encoder(x)\n",
    "         \n",
    "        #Prepare place holder for decoder output\n",
    "        targets_ta = []\n",
    "        #prev_output become the next input to the LSTM cell\n",
    "        dec_input = prev_y\n",
    "        \n",
    "        \n",
    "        \n",
    "       #dec_input = torch.cat([prev_output, curr_features], dim=1) \n",
    "        \n",
    "        #itearate over LSTM - according to the required output days\n",
    "        for out_days in range(self.output_length) :\n",
    "            \n",
    "          \n",
    "            prev_x,prev_hidden,prev_cell = self.decoder(dec_input,hidden,cell)\n",
    "            hidden,cell = prev_hidden,prev_cell\n",
    "            \n",
    "            prev_x = prev_x[:,:,0:1]\n",
    "            #print(\"preve x shape is:\",prev_x.size())\n",
    "            #print(\"features shape is:\",features[out_days+1].size())\n",
    "            \n",
    "            if out_days+1 < self.output_length :\n",
    "                dec_input = torch.cat([prev_x,features[out_days+1].reshape(1,1,506)], dim=2) \n",
    "            \n",
    "            targets_ta.append(prev_x.reshape(1))\n",
    "           \n",
    "            \n",
    "        \n",
    "        \n",
    "        targets = torch.stack(targets_ta)\n",
    "\n",
    "        return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9IJ0PrH7mPA"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxDOXhL_7WYY"
   },
   "outputs": [],
   "source": [
    "n_features = trainX.shape[2]\n",
    "model = Seq2Seq(seq_length, n_features, 512)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYJktgqV8H3D"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iea6L7aG8J0_"
   },
   "outputs": [],
   "source": [
    "def train_model(model, TrainX,Trainy,ValidX,Validy,seq_length, n_epochs):\n",
    "  \n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "    mb = master_bar(range(1, n_epochs + 1))\n",
    "\n",
    "    for epoch in mb:\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for i in progress_bar(range(TrainX.size()[0]),parent=mb):\n",
    "            seq_inp = TrainX[i,:,:].to(device)\n",
    "            seq_true = Trainy[i,:,:].to(device)\n",
    "            features = train_features[i,:,:].to(device)\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:],features)\n",
    "            \n",
    "            \n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in progress_bar(range(validX.size()[0]),parent=mb):\n",
    "                seq_inp = ValidX[i,:,:].to(device)\n",
    "                seq_true = Validy[i,:,:].to(device)\n",
    "                features = valid_features[i,:,:].to(device)\n",
    "        \n",
    "                seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:],features)\n",
    "               \n",
    "\n",
    "                loss = criterion(seq_pred, seq_true)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_n_features.pt')\n",
    "            print(\"saved best model epoch:\",epoch,\"val loss is:\",val_loss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "        scheduler.step(val_loss)\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7cywtuW8W4w"
   },
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-2,weight_decay=1e-5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = torch.nn.MSELoss().to(device) \n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 5e-3, eta_min=1e-8, last_epoch=-1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=10, factor =0.5 ,min_lr=1e-7, eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53Ld0YmX8hkF"
   },
   "outputs": [],
   "source": [
    "model, history = train_model(\n",
    "  model,\n",
    "  trainX,trainy,\n",
    "  validX,validy,\n",
    "  seq_length,\n",
    "  n_epochs=1, #Train for few epochs as illustration, \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyorLmKP-nvh"
   },
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4Fctzn_3LaG"
   },
   "outputs": [],
   "source": [
    "all_360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YB7GVeYC-mMb"
   },
   "outputs": [],
   "source": [
    "TestX = np.array(all_360.iloc[-692:-346,:])\n",
    "Testy = np.array(all_360.iloc[-346:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPxF0BlPBaXF"
   },
   "outputs": [],
   "source": [
    "TestX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HC1_-QFA1pb"
   },
   "outputs": [],
   "source": [
    "TestX = Variable(torch.Tensor(TestX))\n",
    "Testy = Variable(torch.Tensor(Testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oDKgc4yBOx6"
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bAmYKZzF_3R"
   },
   "outputs": [],
   "source": [
    "train_360.iloc[:,:507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ET2C8Sf8F0zs"
   },
   "outputs": [],
   "source": [
    "seq_inp[:,:507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6MDXebMxjMq"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  for i in progress_bar(range(validX.size()[0]),parent=mb):\n",
    "    seq_inp = ValidX[i,:,:].to(device)\n",
    "    seq_true = Validy[i,:,:].to(device)\n",
    "    features = valid_features[i,:,:].to(device)\n",
    "    seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:],features)\n",
    "    loss = criterion(seq_pred, seq_true)\n",
    "    val_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVkBgqff57uw"
   },
   "outputs": [],
   "source": [
    "validX[0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxjFs_-gxu4M"
   },
   "outputs": [],
   "source": [
    "TestX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6_IcSwOBBHr"
   },
   "outputs": [],
   "source": [
    "#model.eval()\n",
    "with torch.no_grad():\n",
    "    seq_inp = TestX.to(device)\n",
    "    seq_pred = model(TestX[0:10,:].to(device),seq_inp[seq_length-1:seq_length,:],seq_inp[:,:506])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhdY11nnIS87"
   },
   "outputs": [],
   "source": [
    "seq_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-19ZXNoInxw"
   },
   "outputs": [],
   "source": [
    "len(seq_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5bnHxEmIWPi"
   },
   "outputs": [],
   "source": [
    "data_predict = seq_pred.cpu().numpy()\n",
    "labels = Testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DZAsR_VJqmm"
   },
   "outputs": [],
   "source": [
    "data_predict.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfD5y9NYLWEq"
   },
   "outputs": [],
   "source": [
    "len(data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naID1RGVLro-"
   },
   "outputs": [],
   "source": [
    "original_data = all_360.iloc[-346:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8lA6ZMzL0Ly"
   },
   "outputs": [],
   "source": [
    "final = pd.DataFrame(original_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H4JFc79QHMB"
   },
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UX1EvZaMH09"
   },
   "outputs": [],
   "source": [
    "pred = data_predict.flatten()\n",
    "pred_df = pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIMgvAcXNCDv"
   },
   "outputs": [],
   "source": [
    "pred_df = pred_df.set_index([final.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVW6MlJxQMIn"
   },
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SkLcSdc3NMla"
   },
   "outputs": [],
   "source": [
    "final['pred'] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlFIIpt4QRfr"
   },
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTqUd-2INTi9"
   },
   "outputs": [],
   "source": [
    "mean_squared_error(final['target'], final['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kRKIMT72YzW"
   },
   "source": [
    "# Saving our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 713258,
     "status": "ok",
     "timestamp": 1613311504451,
     "user": {
      "displayName": "Antoine SETTELEN",
      "photoUrl": "",
      "userId": "08467678322216905282"
     },
     "user_tz": -60
    },
    "id": "zUXXnClG0CxF",
    "outputId": "4529855d-f2a7-4811-afa2-6ca8880e83cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook /content/drive/MyDrive/U4_Prediction_stock_auction_volumes/notebook/New_model.ipynb to slides\n",
      "[NbConvertApp] Writing 1321361 bytes to /content/drive/MyDrive/U4_Prediction_stock_auction_volumes/notebook/New_model.slides.html\n",
      "[NbConvertApp] Redirecting reveal.js requests to https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0\n",
      "Serving your slides at http://127.0.0.1:8000/New_model.slides.html\n",
      "Use Control-C to stop this server\n",
      "[NbConvertApp] WARNING | No web browser found: could not locate runnable browser.\n",
      "\n",
      "InterruptedTraceback (most recent call last):\n",
      "  File \"/usr/local/bin/jupyter-nbconvert\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/jupyter_core/application.py\", line 267, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/nbconvertapp.py\", line 338, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/nbconvertapp.py\", line 508, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/nbconvertapp.py\", line 481, in convert_single_notebook\n",
      "    self.postprocess_single_notebook(write_results)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/nbconvertapp.py\", line 453, in postprocess_single_notebook\n",
      "    self.postprocessor(write_results)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/postprocessors/base.py\", line 28, in __call__\n",
      "    self.postprocess(input)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nbconvert/postprocessors/serve.py\", line 106, in postprocess\n",
      "    print(\"\\nInterrupted\")\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert '/content/drive/MyDrive/U4_Prediction_stock_auction_volumes/notebook/New_model.ipynb' --to slides --post serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook C:\\Users\\swp\\Documents\\_Perso\\Cours\\M2\\U4.Artificial_Intelligence\\artificial_intelligence_for_finance\\Git\\U4_Prediction_stock_auction_volumes\\notebook\\New_model.ipynb to html_toc\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\Scripts\\jupyter-nbconvert-script.py\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jupyter_core\\application.py\", line 254, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 350, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 524, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 489, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(notebook_filename, resources, input_buffer=input_buffer)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 418, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(notebook_filename, resources=resources)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 181, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 199, in from_file\n",
      "    return self.from_notebook_node(nbformat.read(file_stream, as_version=4), resources=resources, **kw)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\html.py\", line 119, in from_notebook_node\n",
      "    return super().from_notebook_node(nb, resources, **kw)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\templateexporter.py\", line 384, in from_notebook_node\n",
      "    output = self.template.render(nb=nb_copy, resources=resources)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\templateexporter.py\", line 148, in template\n",
      "    self._template_cached = self._load_template()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nbconvert\\exporters\\templateexporter.py\", line 355, in _load_template\n",
      "    return self.environment.get_template(template_file)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jinja2\\environment.py\", line 883, in get_template\n",
      "    return self._load_template(name, self.make_globals(globals))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jinja2\\environment.py\", line 857, in _load_template\n",
      "    template = self.loader.load(self, name, globals)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\jinja2\\loaders.py\", line 429, in load\n",
      "    raise TemplateNotFound(name)\n",
      "jinja2.exceptions.TemplateNotFound: toc2\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert \"C:\\Users\\swp\\Documents\\_Perso\\Cours\\M2\\U4.Artificial_Intelligence\\artificial_intelligence_for_finance\\Git\\U4_Prediction_stock_auction_volumes\\notebook\\New_model.ipynb\" --to html_toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'reveal.js'...\n",
      "error: pathspec '3.5.0' did not match any file(s) known to git\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hakimel/reveal.js.git\n",
    "!cd reveal.js\n",
    "!git checkout 3.5.0\n",
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swp\\Documents\\_Perso\\Cours\\M2\\U4.Artificial_Intelligence\\artificial_intelligence_for_finance\\Git\\U4_Prediction_stock_auction_volumes\\notebook\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook C:\\Users\\swp\\Documents\\_Perso\\Cours\\M2\\U4.Artificial_Intelligence\\artificial_intelligence_for_finance\\Git\\U4_Prediction_stock_auction_volumes\\notebook\\New_model.ipynb to slides\n",
      "[NbConvertApp] Writing 1764638 bytes to C:\\Users\\swp\\Documents\\_Perso\\Cours\\M2\\U4.Artificial_Intelligence\\artificial_intelligence_for_finance\\Git\\U4_Prediction_stock_auction_volumes\\notebook\\New_model.slides.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert C:\\Users\\swp\\Documents\\_Perso\\Cours\\M2\\U4.Artificial_Intelligence\\artificial_intelligence_for_finance\\Git\\U4_Prediction_stock_auction_volumes\\notebook\\New_model.ipynb --to slides --reveal-prefix reveal.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Diaporama",
  "colab": {
   "collapsed_sections": [
    "0NIS3_ohwfgA",
    "CF9hc-4Ewkyl",
    "GSs7HPElw-wm",
    "V9GJj8XixYvy",
    "8l-nL7bJzLpt",
    "xRT3abbJx2P0",
    "lgPOjq-5xbjP",
    "gKTNkDc6y7EV",
    "ymSG0JoOUDMV",
    "qb2mUfK0kAgR",
    "SXrLUHY7kP8j",
    "svWFeAjylRFy",
    "8fncxRb-lamq",
    "49UHaA5Rlpbr",
    "pqJaORGOmUud",
    "z76QIbMDLvlo",
    "6iufg2O3muyg",
    "4Q_aLRAMoIKU",
    "mFGIU1lfoCV3",
    "spi7aOlDosCG",
    "fizeLcQBovfW",
    "S6z0scE2qDju",
    "iJiP30TxqriS",
    "hdTYhMcfrDGo",
    "GQI2qmNstDdJ",
    "zfSO6Q9QtTkK",
    "S8yyEs2mtoO8",
    "-zXWyHE4tsG7",
    "IhXnGuaBt3xJ",
    "GCRXN9rOflmb",
    "PPAq97y20Ah6",
    "h-iJlMgn0iGO",
    "I8yQGuvI1V8Z",
    "18OynhQ21aCw",
    "ohRJjzjnlGuo",
    "8n8ggmEQ2AOO",
    "ew0ZSUap2e9I",
    "O7F1NfZO232s",
    "QrGElDDK3acZ",
    "tOOBSmVT3r6B",
    "xd4jBHYyJ_C9",
    "-ylTcogq4xku",
    "GG-S7bZN7rNJ",
    "wPzwO5XRA0J1",
    "VGP2soPGA-mN",
    "yk8Ezaulr1Ha",
    "Yo1FLeOOFjvZ",
    "V366KVyLFt_L",
    "lqeS9p2SGUpA",
    "D5Xk8nM0wBMz",
    "KupPXhblIKau",
    "uQoJjg-VLzxg",
    "wK7SHHB7L4ZI",
    "YKeAeB-yMMK3",
    "-WIuT5-eMhCQ",
    "-LfHamUlMraP",
    "gyCM_K3lNPvc",
    "z0_bGYM5NfXo",
    "Uo8-rxP72ff8",
    "0yp36yqWOCip",
    "Q4ateMWsOFJ6",
    "e4VIWG-2OInI",
    "f8NJfN-AOWHM",
    "BRyFKafRe2qK",
    "XpmlksUpPY2p",
    "1BBEJ9fG5bV5",
    "vwd9NnTk5-p0",
    "zbI2xkzx58KO",
    "A2i-zDGgPyll",
    "8bZ95YQU8h7x",
    "z1tJTU3UQJvW",
    "VYzfBIDjQz0S",
    "cHKIniFhRHEq",
    "Aj2zwkn3RWEX",
    "JdELK99gSdXR",
    "q-v4ViCETpX9",
    "VQ9Q-T0jTzmZ",
    "n4YnpaE0Lrys",
    "sK9s9S4aLry2",
    "eOB7IePZLry6",
    "iIOGWllHLrzF",
    "V-nw9S8zj2Me",
    "KNzAadkkiEbJ",
    "eRxNdN4w3oe1",
    "Q8U-ulJ929F-",
    "qF7AMNDBtbXM",
    "ksYe3V8ykDL-",
    "jxxjeElo3x3C"
   ],
   "machine_shape": "hm",
   "name": "New_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
